{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Initialisation\n",
    "\n",
    "Run this cell to initialise the neccesary variables in your notebook. This will also allow some extensions and permit the autoreload function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     26
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    \n",
    "    from os import pardir, getcwd\n",
    "    from os.path import join, abspath, normpath, basename\n",
    "    from termcolor import colored\n",
    "    \n",
    "    rootDirectory = abspath(join(getcwd(), pardir))\n",
    "    dataDirectory = join(rootDirectory, 'data')\n",
    "    modelDirectory = join(rootDirectory, 'models')\n",
    "    currentSensorsh = ('https://raw.githubusercontent.com/fablabbcn/smartcitizen-kit-20/dev/lib/Sensors/Sensors.h')\n",
    "    \n",
    "    print (('Paths initialisation -> ') + colored('OK', 'green'))\n",
    "    \n",
    "    # `do not disturb` mode\n",
    "    import warnings                                  \n",
    "    warnings.filterwarnings('ignore')\n",
    "    print (('Removing warnings -> ') + colored('OK', 'green'))\n",
    "    \n",
    "    ## Create a button that hides cells\n",
    "    from IPython.display import HTML, display, clear_output, Markdown\n",
    "    from ipywidgets import interact\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    from src.data.recording_class import recordings\n",
    "    from src.data.test_utils import getSensorNames\n",
    "    currentSensorNames = getSensorNames(currentSensorsh, join(dataDirectory, 'interim'))\n",
    "    print (('Sensor names -> ') + colored('OK', 'green'))\n",
    "    \n",
    "    import matplotlib.pyplot as plot\n",
    "    plot.style.use('seaborn')\n",
    "    %matplotlib inline\n",
    "    \n",
    "    # Initialise recordings\n",
    "    records = recordings()\n",
    "    print (('Init recordings -> ') + colored('OK', 'green'))\n",
    "except:\n",
    "    \n",
    "    print (('Notebook initialisation -> ') + colored('NOK', 'red'))\n",
    "else:\n",
    "    print (('Notebook initialisation -> ') + colored('OK', 'green'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Data Import\n",
    "\n",
    "Import test from local test database or smartcitizen API:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order of slots\n",
    "- Check if there was a reference and convert it's units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from src.data.test_utils import getTests\n",
    "from src.data.api_utils import getKitID\n",
    "import re\n",
    "\n",
    "out_load = widgets.Output()\n",
    "\n",
    "selectedTest = tuple()\n",
    "def selectTests(Test):\n",
    "    global selectedTest\n",
    "    selectedTest = list(Test)\n",
    "    selectedTestBases = list()\n",
    "    selectedTestBases.append('')\n",
    "    for test in selectedTest:\n",
    "        selectedTestBases.append(basename(normpath(test)))\n",
    "    name_drop_api.options = selectedTestBases\n",
    "            \n",
    "def loadButton(b):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        \n",
    "        target_raster_csv = raster_text_csv.value + raster_drop_csv.value\n",
    "        if na_drop_csv.value != 'None':\n",
    "            na_drop_action = na_drop_csv.value\n",
    "            na_dropage = True\n",
    "        else:\n",
    "            na_dropage = False\n",
    "            na_drop_action = ''\n",
    "        for testCSV in selectedTest:\n",
    "            testName = basename(normpath(testCSV))\n",
    "            records.add_recording_CSV(testName, testCSV, currentSensorNames, target_raster_csv, dataDirectory, na_dropage, na_drop_action, load_processed_csv)\n",
    "        \n",
    "        \n",
    "        if kitList_api.value != '':\n",
    "            target_raster_api = raster_text_api.value + raster_drop_api.value\n",
    "            devices = kitList_api.value.strip('').split(',')\n",
    "            devicesCorrected = list()\n",
    "            \n",
    "            for device in devices: \n",
    "                device = re.sub(' ', '', device)\n",
    "                devicesCorrected.append(device)\n",
    "            \n",
    "            if name_drop_api.value == '':\n",
    "                testName = testName_api.value\n",
    "                print (testName)\n",
    "\n",
    "            else:\n",
    "                testName = name_drop_api.value\n",
    "            \n",
    "            if testName != '':\n",
    "                if na_drop_api.value != 'None':\n",
    "                    na_drop_action = na_drop_api.value\n",
    "                    na_dropage = True\n",
    "                else:\n",
    "                    na_dropage = False\n",
    "                    na_drop_action = ''\n",
    "                records.add_recording_API(testName, devicesCorrected, currentSensorNames, start_date_widget_api.value, end_date_widget_api.value, \\\n",
    "                                          target_raster_api, dataDirectory)\n",
    "       \n",
    "        unload_drop.options = records.readings.keys()\n",
    "        \n",
    "def clearButton(b):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        records.clear_recordings()\n",
    "        unload_drop.options = records.readings.keys()\n",
    "        \n",
    "def clearAPI(b):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        kitList_api.value = ''\n",
    "        testName_api.value = ''\n",
    "        \n",
    "def reload_list(b):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        global tests\n",
    "        tests = getTests(dataDirectory)\n",
    "\n",
    "def clearSelButton(b):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        records.del_recording(unload_drop.value)\n",
    "        unload_drop.options = records.readings.keys()\n",
    "        if len(records.readings.keys()) > 0:\n",
    "            print ('Current recordings list:')\n",
    "            for reading in records.readings.keys():\n",
    "                print ('\\t',reading)\n",
    "        else:\n",
    "            print ('Current recordings list is empty')\n",
    "        \n",
    "## API\n",
    "banner_api = widgets.HTML('<h3>Import API Tests</h3>')\n",
    "kitList_api = widgets.Text(description = 'Kit List')\n",
    "testName_api = widgets.Text(description = 'Test Name')\n",
    "\n",
    "name_drop_api = widgets.Dropdown(options = selectedTest,\n",
    "                                  description = 'Merge with CSV',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "raster_text_api = widgets.Text(description = 'Target Raster',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "\n",
    "raster_drop_api = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "na_drop_api = widgets.Dropdown(options = ['None', 'fill', 'drop'],\n",
    "                                  value = 'fill',\n",
    "                                  description = 'Process na',\n",
    "                                  layout = widgets.Layout(width='200px'))\n",
    "\n",
    "clearAPIB = widgets.Button(description = 'Clear')\n",
    "clearAPIB.on_click(clearAPI)\n",
    "\n",
    "start_date_widget_api = widgets.DatePicker(description='Start Date')\n",
    "end_date_widget_api = widgets.DatePicker(description='End Date')\n",
    "dateBox_api = widgets.VBox([start_date_widget_api, end_date_widget_api, clearAPIB])\n",
    "\n",
    "raster_box_api = widgets.HBox([raster_text_api, raster_drop_api, na_drop_api])\n",
    "namebox_api = widgets.HBox([testName_api, name_drop_api])\n",
    "Hbox_api = widgets.VBox([kitList_api, namebox_api])\n",
    "apiBox = widgets.VBox([banner_api, Hbox_api, raster_box_api, dateBox_api])\n",
    "        \n",
    "## CSV\n",
    "reload_list_button = widgets.Button(description='Reload List')\n",
    "reload_list_button.on_click(reload_list)\n",
    "\n",
    "display(widgets.HTML('<h2>Test load</h2>'))\n",
    "display(widgets.HBox([widgets.HTML('<h3>Import Local Tests</h3>')]))\n",
    "\n",
    "tests = getTests(dataDirectory)\n",
    "\n",
    "interact(selectTests,\n",
    "         Test = widgets.SelectMultiple(options=tests, \n",
    "                           selected_labels = selectedTest, \n",
    "                           layout=widgets.Layout(width='600px')))\n",
    "\n",
    "raster_text_csv = widgets.Text(description = 'Target Raster',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "\n",
    "raster_drop_csv = widgets.Dropdown(options = ['H', 'Min', 'S', 'ms'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "na_drop_csv = widgets.Dropdown(options = ['None', 'fill', 'drop'],\n",
    "                                  value = 'fill',\n",
    "                                  description = 'Process na',\n",
    "                                  layout = widgets.Layout(width='200px'))\n",
    "\n",
    "load_processed_csv = widgets.Checkbox(value=True, \n",
    "                                  description='Try to load processed CSV') \n",
    "\n",
    "raster_box_csv = widgets.HBox([raster_text_csv, raster_drop_csv, na_drop_csv, load_processed_csv])\n",
    "\n",
    "# buttonBox_csv = widgets.HBox([loadB_csv, resetB_csv])\n",
    "csvBox = widgets.VBox([raster_box_csv])#, buttonBox_csv])\n",
    "\n",
    "loadB = widgets.Button(description='Load')\n",
    "loadB.on_click(loadButton)\n",
    "\n",
    "resetB = widgets.Button(description='Clear All')\n",
    "resetB.on_click(clearButton)\n",
    "\n",
    "unload_drop = widgets.Dropdown(options = records.readings.keys(),\n",
    "                                  layout = widgets.Layout(width='200px'))\n",
    "\n",
    "resetOneB = widgets.Button(description='Clear Selected')\n",
    "resetOneB.on_click(clearSelButton)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB, resetB, unload_drop, resetOneB])\n",
    "# Display everything\n",
    "display(csvBox)\n",
    "display(apiBox)\n",
    "display(widgets.HTML('<hr>'))\n",
    "display(buttonBox)\n",
    "display(out_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe cleaning: anomaly detection\n",
    "\n",
    "Inspired by the code of Dmitriy Sergeev at https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3.\n",
    "\n",
    "### XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from src.data.signal_utils import mean_absolute_percentage_error, timeseries_train_test_split, \\\n",
    "                         plotModelResults, prepareDataFrame\n",
    "\n",
    "test_name = '2018-06_INT_OUTDOOR_TESTS_GRAN_VIA'\n",
    "device_name = 'BOARD1'\n",
    "\n",
    "## Ignore columns\n",
    "irrelevantColumns = ['BATT', 'BATT_CHG_RATE', 'LIGHT', 'CO_MICS_THEAT', 'NO2_MICS_THEAT']\n",
    "frequency = '1Min'\n",
    "\n",
    "# Resample data\n",
    "data = prepareDataFrame(records.readings[test_name]['devices'][device_name]['data'], \n",
    "                 frequency, irrelevantColumns, _plotModelAnom = True, \n",
    "                 _scaleAnom = 1.9, _methodAnom = 'before-after-avg')\n",
    "\n",
    "# Make a copy to a 'CLEAN' keyword\n",
    "records.readings[test_name]['devices'][device_name + '_CLEAN'] = dict()\n",
    "\n",
    "# Put everything except data inside\n",
    "for key in records.readings[test_name]['devices'][device_name].keys():\n",
    "    \n",
    "    if 'data' not in key:\n",
    "        records.readings[test_name]['devices'][device_name + '_CLEAN'][key] = records.readings[test_name]['devices'][device_name][key]\n",
    "\n",
    "# Put data inside\n",
    "records.readings[test_name]['devices'][device_name + '_CLEAN']['data'] = data\n",
    "records.readings[test_name]['ready_to_model'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     8,
     12,
     17
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "import os\n",
    "out_export = widgets.Output()\n",
    "\n",
    "selected = []\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "selected_export=tuple()\n",
    "def selectedDevices_export(Source):\n",
    "    global selected_export\n",
    "    selected_export = list(Source)\n",
    "    \n",
    "def show_device_export(Source):\n",
    "    _devices_select_export.options = [s for s in list(records.readings[_test_export.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def exportWrapper(b):\n",
    "    with out_export:\n",
    "        for i in range(len(selected_export)):\n",
    "            b.f = selected_export[i]\n",
    "            exportDir = exportPath.value\n",
    "            if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "            savePath = os.path.join(exportDir, b.f)\n",
    "            device_export = b.f\n",
    "            test_export = _test_export.value\n",
    "            exportFile(savePath, test_export, device_export)\n",
    "            \n",
    "            \n",
    "            if copyToFolder:\n",
    "                year = test_export[0:4]\n",
    "                month = test_export[5:7]\n",
    "                exportDir_test = os.path.join(dataDirectory, 'processed', year, month, test_export, 'processed')\n",
    "                if not os.path.exists(exportDir_test): os.mkdir(exportDir_test)\n",
    "                savePath_test = os.path.join(exportDir_test, b.f)\n",
    "                \n",
    "                exportFile(savePath_test, test_export, device_export)\n",
    "\n",
    "def exportFile(savePath, test_export, device_export):\n",
    "\n",
    "    if not os.path.exists(savePath):\n",
    "        records.readings[test_export]['devices'][device_export]['data'].to_csv(savePath + '.csv', sep=\",\")\n",
    "        display(widgets.HTML('File saved to: ' + savePath + '.csv'))\n",
    "    else:\n",
    "        display(widgets.HTML('File Already exists!'))\n",
    "    \n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "\n",
    "_test_export = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_devices_select_export = widgets.SelectMultiple(options = records.readings[_test_export.value]['devices'].keys(),\n",
    "                                                layout=widgets.Layout(width='600px'))\n",
    "\n",
    "_test_export_drop = widgets.interactive(show_device_export, \n",
    "                                Source=_test_export, \n",
    "                                layout=layout)\n",
    "\n",
    "display(widgets.HTML('<h2>Files Export</h2>'))\n",
    "\n",
    "selectBox = widgets.VBox([_test_export_drop])\n",
    "display(selectBox)\n",
    "\n",
    "_devices_select_export_drop = interact(selectedDevices_export,\n",
    "                                        Source = _devices_select_export)\n",
    "\n",
    "exportPath = widgets.Text(description = 'Path ', \n",
    "                          value = join(dataDirectory, 'export'),\n",
    "                          layout=widgets.Layout(width='600px'))\n",
    "\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportWrapper)\n",
    "\n",
    "copyToFolder = widgets.Checkbox(value=True, \n",
    "                                  description='Copy to test folder') \n",
    "\n",
    "\n",
    "exportBox = widgets.HBox([exportPath, copyToFolder, eb])\n",
    "_BOX=widgets.VBox([exportBox])\n",
    "display(_BOX)\n",
    "display(out_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     28,
     40,
     55
    ]
   },
   "outputs": [],
   "source": [
    "from src.models.formula_utils import *\n",
    "import pandas as pd\n",
    "\n",
    "out_calc = widgets.Output()\n",
    "\n",
    "def show_device_list(Source):\n",
    "    _devices_select.options = [s for s in list(records.readings[_test.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = records.readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = records.readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(records.readings[_test.value]['devices'][s]['data'].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    with out_calc:\n",
    "        clear_output()\n",
    "        A = _Aterm.value\n",
    "        B = _Bterm.value\n",
    "        C = _Cterm.value\n",
    "        D = _Dterm.value\n",
    "        Name = _formulaName.value\n",
    "        for s in list(selected):\n",
    "            result = functionFormula(s,A,B,C,D,records.readings)\n",
    "            records.readings[_test.value]['devices'][s]['data'][Name] = result\n",
    "            records.readings[_test.value]['ready_to_model'] = False\n",
    "            print (\"Formula {} Added in test {}, device {}\".format(Name, _test.value, s))\n",
    "    \n",
    "def functionFormula(s, Aname, Bname, Cname, Dname, _readings):\n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(_readings[_test.value]['devices'][s]['data'].loc[:,(Aname,)],\\\n",
    "                                           _readings[_test.value]['devices'][s]['data'].loc[:,(Bname,)],\\\n",
    "                                           left_index=True, right_index=True), \\\n",
    "                                  _readings[_test.value]['devices'][s]['data'].loc[:,(Cname,)], \\\n",
    "                                  left_index=True, right_index=True),\\\n",
    "                         _readings[_test.value]['devices'][s]['data'].loc[:,(Dname,)],\\\n",
    "                         left_index=True, right_index=True)\n",
    "    \n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "        \n",
    "selected=tuple()\n",
    "def selectedDevices(Source):\n",
    "    global selected\n",
    "    selected = list(Source)\n",
    "    commonChannels(selected)\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='350px')\n",
    "_test = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "_selectBox = widgets.VBox([_test_drop])\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "\n",
    "display(widgets.HTML('<h3>Calculator</h3>'))\n",
    "display(widgets.HTML('<h4>Select the Files for your formulas to apply</h4>'))\n",
    "display(_selectBox)\n",
    "_devices_select = widgets.SelectMultiple(options = records.readings[_test.value]['devices'].keys(),\n",
    "                                         layout=widgets.Layout(width='700px'))\n",
    "_devices_select_drop = interact(selectedDevices,\n",
    "                                 Source = _devices_select)\n",
    "\n",
    "_test_drop = widgets.interactive(show_device_list, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "display(widgets.HTML('<h4>Select terms</h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)\n",
    "\n",
    "display(out_calc)\n",
    "\n",
    "## Vapour equilibrium: B is temperature in degC, assumed 1013mbar\n",
    "# (1.0007 + 3.46*1e-6*1013)*6.1121*np.exp(17.502*B/(240.97+B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     28,
     34,
     40,
     107,
     166,
     201,
     209,
     214,
     221,
     223
    ]
   },
   "outputs": [],
   "source": [
    "# --\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "out_tsplot = widgets.Output()\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "\n",
    "def show_devices(Source):\n",
    "    _device.options = [s for s in list(records.readings[Source]['devices'].keys())]\n",
    "    _device.source = Source\n",
    "    \n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(records.readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = records.readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = records.readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    with out_tsplot:\n",
    "        clear_output()\n",
    "        del toshow[:]\n",
    "        del axisshow[:]\n",
    "        \n",
    "def clear_sensor(b):\n",
    "    with out_tsplot:\n",
    "        clear_output()\n",
    "        d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "        print('Removing', d[0] + ' - ' + d[1] + ' - ' + d[2])\n",
    "        if d in toshow:\n",
    "            toshow.remove(d)\n",
    "\n",
    "def add_sensor(b):\n",
    "    with out_tsplot:\n",
    "        clear_output()\n",
    "        d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "        \n",
    "        if d not in toshow: \n",
    "            toshow.append(d)\n",
    "            axisshow.append(_axis_drop.value)\n",
    "            \n",
    "        plot_data = records.readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "        list_data_primary = []\n",
    "        list_data_secondary = []\n",
    "        list_data_terciary = []\n",
    "        \n",
    "        if b.slice_time:\n",
    "            plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "            plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "        \n",
    "        if len(toshow) > 1:\n",
    "            for i in range(1, len(toshow)):\n",
    "                plot_data = pd.merge(plot_data, records.readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "    \n",
    "        print ('-------------------------------------')\n",
    "        print (' Medias:\\n')\n",
    "        meanTable = []\n",
    "        for d in toshow:\n",
    "            myMean = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "            meanTable.append(myMean)   \n",
    "        res = plot_data.mean()\n",
    "        for i in range(len(meanTable)): print (meanTable[i] + '%.2f' % (res[i]))\n",
    "        print ('-------------------------------------')\n",
    "        \n",
    "        print ('-------------------------------------')\n",
    "        print (' Std Deviation:\\n')\n",
    "        stdTable = []\n",
    "        for d in toshow:\n",
    "            myStd = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "            stdTable.append(myStd)   \n",
    "        std = plot_data.std()\n",
    "        for i in range(len(stdTable)): print (stdTable[i] + '%.2f' % (std[i]))\n",
    "        print ('-------------------------------------')\n",
    "    \n",
    "        # Change columns naming\n",
    "        changed = []\n",
    "        for i in range(len(plot_data.columns)):\n",
    "            changed.append(toshow[i][0] + ' - '+ toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "            print(plot_data.columns[i], 'added to the plot list')\n",
    "        plot_data.columns = changed\n",
    "        \n",
    "        subplot_rows = 0\n",
    "        if len(toshow) > 0:\n",
    "            for i in range(len(toshow)):\n",
    "                if axisshow[i]=='1': \n",
    "                    list_data_primary.append(str(changed[i]))\n",
    "                    subplot_rows = max(subplot_rows,1)\n",
    "                if axisshow[i]=='2': \n",
    "                    list_data_secondary.append(str(changed[i]))\n",
    "                    subplot_rows = max(subplot_rows,2)\n",
    "                if axisshow[i]=='3': \n",
    "                    list_data_terciary.append(str(changed[i]))\n",
    "                    subplot_rows = max(subplot_rows,3)\n",
    "              \n",
    "        \n",
    "        if _matplotly.value == 'Plotly':\n",
    "            global fig1\n",
    "            fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "            \n",
    "            for i in range(len(list_data_primary)):\n",
    "                fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "        \n",
    "            for i in range(len(list_data_secondary)):\n",
    "                fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "            \n",
    "            for i in range(len(list_data_terciary)):\n",
    "                fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "        \n",
    "            if setLimits: \n",
    "                fig1['layout'].update(height = 800,\n",
    "                                    legend=dict(x=-.1, y=1.2) ,\n",
    "                                    xaxis=dict(title='Time'))\n",
    "                                  \n",
    "            else:\n",
    "                fig1['layout'].update(height = 800,\n",
    "                                      legend=dict(x=-.1, y=1.2),\n",
    "                                      xaxis=dict(title='Time'))\n",
    "                                   \n",
    "        elif _matplotly.value == 'Matplotlib':\n",
    "            global fig\n",
    "            fig, axes = plt.subplots(subplot_rows, 1, figsize=(15,10))\n",
    "            # Four axes, returned as a 2-d array\n",
    "            \n",
    "            if subplot_rows == 1:\n",
    "                for i in range(len(list_data_primary)):\n",
    "                    axes.plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                    axes.legend(loc='best')\n",
    "    \n",
    "            else:\n",
    "                for i in range(len(list_data_primary)):\n",
    "                    axes[0].plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                    axes[0].legend(loc='best')\n",
    "                    axes[0].grid(visible = True)\n",
    "    \n",
    "                for i in range(len(list_data_secondary)):\n",
    "                    axes[1].plot(plot_data.index, plot_data[list_data_secondary[i]], label =  list_data_secondary[i])\n",
    "                    axes[1].legend(loc='best')\n",
    "                    axes[1].grid(visible = True)\n",
    "    \n",
    "                for i in range(len(list_data_terciary)):\n",
    "                    axes[2].plot(plot_data.index, plot_data[list_data_terciary[i]], label =  list_data_terciary[i])\n",
    "                    axes[2].legend(loc='best')\n",
    "                    axes[2].grid(visible = True)\n",
    "    \n",
    "            plt.xlabel('Date') \n",
    "            plt.grid(visible = True)\n",
    "            plt.show()\n",
    "            \n",
    "def plot_cb_ts(b):\n",
    "    with out_tsplot:\n",
    "        print ('hello')\n",
    "        clear_output()\n",
    "        if _matplotly.value == 'Plotly':\n",
    "            ply.offline.iplot(fig1)\n",
    "            \n",
    "def reset_time(b):\n",
    "    _min_date.value = records.readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = records.readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_clear = widgets.Button(description='Clear', layout=widgets.Layout(width='120px'))\n",
    "_b_clear.on_click(clear_sensor)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "_b_plot = widgets.Button(description='Show Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_plot.on_click(plot_cb_ts)\n",
    "\n",
    "# Axis dropdown\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Synchronise Checkbox\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, \n",
    "                                     description='Synchronise X axis', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "\n",
    "_matplotly = widgets.RadioButtons(\n",
    "    options=['Matplotlib', 'Plotly'], value='Matplotlib',\n",
    "    description='Plot Type',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_add_box = widgets.HBox([_b_add , _b_clear, _b_reset_all, _b_plot])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _axis_drop, _add_box])\n",
    "_plot_type_box = widgets.VBox([_matplotly])\n",
    "\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time, _synchroniseXaxis])\n",
    "_root_box = widgets.VBox([_matplotly, _time_box, _device_box, _sensor_box])\n",
    "display(_root_box)\n",
    "\n",
    "display(out_tsplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Back2Back Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     32,
     113
    ]
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Plots\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "sns.set(color_codes=True)\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "\n",
    "out_bbplot = widgets.Output()\n",
    "\n",
    "def show_devices(Source):\n",
    "    A_device.options = [s for s in list(records.readings[Source]['devices'].keys())]\n",
    "    A_device.source = Source\n",
    "    B_device.options = [s for s in list(records.readings[Source]['devices'].keys())]\n",
    "    B_device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensor_drop.options = [s for s in list(records.readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    A_sensor_drop.source = Source\n",
    "    minCropDate.value = records.readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = records.readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensor_drop.options = [s for s in list(records.readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    B_sensor_drop.source = Source\n",
    "    minCropDate.value = records.readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = records.readings[_test.value]['devices'][Source]['data'].index.max()._short_repr    \n",
    "\n",
    "def redraw(b):\n",
    "    with out_bbplot:\n",
    "        cropTime = cropTimeCheck.value\n",
    "        doubleAxis = doubleAxisCheck.value\n",
    "        min_date = minCropDate.value\n",
    "        max_date = maxCropDate.value\n",
    "        mergedData = pd.merge(records.readings[_test.value]['devices'][A_device.value]['data'].loc[:,(A_sensor_drop.value,)], \n",
    "                              records.readings[_test.value]['devices'][B_device.value]['data'].loc[:,(B_sensor_drop.value,)], \n",
    "                              left_index=True, right_index=True, suffixes=('_' + A_sensor_drop.value, '_' + B_sensor_drop.value))\n",
    "        clear_output()\n",
    "        \n",
    "        if cropTime:\n",
    "            mergedData = mergedData[mergedData.index > min_date]\n",
    "            mergedData = mergedData[mergedData.index < max_date]\n",
    "            \n",
    "        #jointplot\n",
    "        df = pd.DataFrame()\n",
    "        A = A_sensor_drop.value + '-' + A_device.value\n",
    "        B = B_sensor_drop.value + '-' + B_device.value\n",
    "        df[A] = mergedData.iloc[:,0]\n",
    "        df[B] = mergedData.iloc[:,1]\n",
    "        \n",
    "        sns.set(font_scale=1.3)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        # sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", height=12, scatter_kws={\"s\": 80})\n",
    "        \n",
    "        print (\"data from \" + str(df.index.min()) + \" to \" + str(df.index.max()))                      \n",
    "        pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "        rmse = sqrt(mean_squared_error(df[A].fillna(0), df[B].fillna(0)))\n",
    "        \n",
    "        print ('Pearson correlation coefficient: ' + str(pearsonCorr))\n",
    "        print ('Coefficient of determination R²: ' + str(pearsonCorr*pearsonCorr))\n",
    "        print ('RMSE: ' + str(rmse))\n",
    "        print ('')\n",
    "    \n",
    "        if cropTime: \n",
    "            \n",
    "            if (doubleAxis):\n",
    "                layout = go.Layout(\n",
    "                    #legend=dict(x=-.1, y=1.2), \n",
    "                    xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                    yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                    yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "                )\n",
    "            else:\n",
    "                layout = go.Layout(\n",
    "                    #legend=dict(x=-.1, y=1.2), \n",
    "                    xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                    yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            if (doubleAxis):\n",
    "                layout = go.Layout(\n",
    "                    #legend=dict(x=-.1, y=1.2), \n",
    "                    xaxis=dict(title='Time'), \n",
    "                    yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                    yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "                )\n",
    "            else:\n",
    "                layout = go.Layout(\n",
    "                    #legend=dict(x=-.1, y=1.2), \n",
    "                    xaxis=dict(title='Time'), \n",
    "                    yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                )\n",
    "        difference = differenceCheck.value\n",
    "        \n",
    "        if not difference:\n",
    "            trace0 = go.Scatter(x=df[A].index, y=df[A], name = A ,line = dict(color='rgb(0,97,255)'))\n",
    "            \n",
    "            if (doubleAxis):\n",
    "                trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "            else:\n",
    "                trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "            data = [trace0, trace1]\n",
    "        else:\n",
    "            trace0 = go.Scatter(x=df[A].index, y=df[A] - df[B], name = A + ' - ' + B, line = dict(color='rgb(0,97,255)'))\n",
    "            data = [trace0]\n",
    "        \n",
    "        figure = go.Figure(data=data, layout=layout)\n",
    "        ply.offline.iplot(figure)\n",
    "        display(HTML('<hr>'))\n",
    "        plot.show()\n",
    "        \n",
    "        # Nice copy figure\n",
    "        \n",
    "        fig = plot.figure(figsize = (20,6))\n",
    "        gs = gridspec.GridSpec(1, 3, figure=fig)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, :-1])\n",
    "        ax2 = fig.add_subplot(gs[0, -1])\n",
    "        # identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "    \n",
    "        ax1.plot(df[A].index, df[A],'g', label = A, linewidth = 1, alpha = 0.9)\n",
    "        ax1.plot(df[B].index, df[B], 'k', label = B, linewidth = 1, alpha = 0.7)\n",
    "        ax1.axis('tight')\n",
    "        ax1.set_title('Time Series Plot for {}'.format(text_title.value))\n",
    "        ax1.grid(True)\n",
    "        ax1.legend(loc=\"best\")\n",
    "        ax1.set_xlabel('Date (-)')\n",
    "        ax1.set_ylabel(text_axis.value)\n",
    "        \n",
    "        \n",
    "        ax2.plot(df[B], df[A], 'go', label = A, linewidth = 1,  alpha = 0.3)\n",
    "        ax2.plot(df[B], df[B], 'k', label =  '1:1 Line', linewidth = 0.4, alpha = 0.3)\n",
    "        ax2.axis('tight')\n",
    "        ax2.set_title('Scatter Plot for {}'.format(text_title.value))\n",
    "        ax2.grid(True)\n",
    "        ax2.legend(loc=\"best\")\n",
    "        ax2.set_xlabel(text_axis.value)\n",
    "        ax2.set_ylabel(text_axis.value)\n",
    "        \n",
    "        plot.show()\n",
    "        \n",
    "if len(records.readings) < 1: \n",
    "    with out_bbplot:\n",
    "        print (\"Please load some data first...\")\n",
    "else:\n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Draw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    # Test dropdown\n",
    "    _test = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout = widgets.Layout(width='350px'),\n",
    "                        description = 'Test')\n",
    "    \n",
    "    _test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test,)\n",
    "    \n",
    "    # Device dropdown\n",
    "    A_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device 1')\n",
    "    \n",
    "    A_device_drop = widgets.interactive(show_sensors_A, \n",
    "                                    Source=A_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    B_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device 2')\n",
    "    \n",
    "    B_device_drop = widgets.interactive(show_sensors_B, \n",
    "                                    Source=B_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    A_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel 1')\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    B_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel 2')\n",
    "    \n",
    "    # Synchronise Checkbox\n",
    "    differenceCheck = widgets.Checkbox(value=False, \n",
    "                                 description='Time Series Difference', \n",
    "                                 disabled=False, \n",
    "                                 layout=widgets.Layout(width='300px'))\n",
    "    \n",
    "    draw_box = widgets.HBox([doubleAxisCheck, differenceCheck])\n",
    "    test_box = widgets.HBox([_test_drop], layout = widgets.Layout(width='400px'))\n",
    "    device_box = widgets.HBox([A_device, B_device])\n",
    "    sensor_box = widgets.HBox([A_sensor_drop, B_sensor_drop])\n",
    "    crop_box = widgets.HBox([minCropDate, maxCropDate, cropTimeCheck])\n",
    "    button_box = widgets.VBox([b_redraw])\n",
    "    text_axis = widgets.Text(description='Text axis', layout=layout)\n",
    "    text_title = widgets.Text(description='Text title', layout=layout)\n",
    "    root_box = widgets.VBox([draw_box, test_box, device_box, sensor_box, crop_box, button_box, text_axis, text_title])\n",
    "        \n",
    "    display(root_box)\n",
    "    display(out_bbplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Scatter plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "# Matplotlib\n",
    "out_smplot = widgets.Output()\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_devices_sp(Source):\n",
    "    _device.options = [s for s in list(records.readings[Source]['devices'].keys())]\n",
    "    _device.source = Source\n",
    "    \n",
    "def show_sensors_sp(Source):\n",
    "    _sensor_drop.options = [s for s in list(records.readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = records.readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = records.readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all_sp(b):\n",
    "    with out_smplot:\n",
    "        clear_output()\n",
    "        del toshow[:]\n",
    "        del axisshow[:]\n",
    "        \n",
    "def clear_sensor_sp(b):\n",
    "    with out_smplot:\n",
    "        clear_output()\n",
    "        d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "        print('Removing', d[0] + ' - ' + d[1] + ' - ' + d[2])\n",
    "        if d in toshow:\n",
    "            toshow.remove(d)\n",
    "        \n",
    "        \n",
    "def add_sensor_sp(b):\n",
    "    dimensions = list()\n",
    "    with out_smplot:\n",
    "        clear_output()\n",
    "        d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "        \n",
    "        if d not in toshow: \n",
    "            toshow.append(d)\n",
    "            \n",
    "        plot_data = records.readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "\n",
    "        if b.slice_time:\n",
    "            plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "            plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "        \n",
    "        if len(toshow) > 1:\n",
    "            for i in range(1, len(toshow)):\n",
    "                plot_data = pd.merge(plot_data, records.readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "    \n",
    "        # Change columns naming\n",
    "        changed = []\n",
    "        for i in range(len(plot_data.columns)):\n",
    "            changed.append(toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "        plot_data.columns = changed\n",
    "        \n",
    "        subplot_rows = 0\n",
    "        \n",
    "        for column in plot_data.columns:\n",
    "            dimensions.append(dict(label=column, values=plot_data[column]))\n",
    "            \n",
    "            print(column, 'added to the plot list')\n",
    "            # xaxis.append(dict(axisd))\n",
    "        \n",
    "        traced = go.Splom(dimensions=dimensions,\n",
    "                  marker=dict(color='rgb(0,30,230, 0.35)',\n",
    "                              size=5,\n",
    "                              #colorscale=pl_colorscaled,\n",
    "                              line=dict(width=0.5,\n",
    "                                        color='rgb(230,230,230, 0.30)') ),\n",
    "                  #text=textd,\n",
    "                  diagonal=dict(visible=False))\n",
    "\n",
    "\n",
    "        title = \"\"\n",
    "    \n",
    "        layout = go.Layout(title=title,\n",
    "                   dragmode='select',\n",
    "                   width=1000,\n",
    "                   height=1000,\n",
    "                   autosize=False,\n",
    "                   hovermode='closest',\n",
    "                   plot_bgcolor='rgba(240,240,240, 0.95)')\n",
    "        global fig\n",
    "        fig = dict(data=[traced], layout=layout)\n",
    "\n",
    "def plot_cb_sp(b):\n",
    "    with out_smplot:\n",
    "        ply.offline.iplot(fig)\n",
    "    \n",
    "def reset_time_sp(b):\n",
    "    _min_date.value = records.readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = records.readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices_sp, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors_sp, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor_sp)\n",
    "_b_add.slice_time = False\n",
    "_b_clear = widgets.Button(description='Clear', layout=widgets.Layout(width='120px'))\n",
    "_b_clear.on_click(clear_sensor_sp)\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all_sp)\n",
    "_plot_b = widgets.Button(description='Plot', layout=widgets.Layout(width='120px'))\n",
    "_plot_b.on_click(plot_cb_sp)\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor_sp)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time_sp)\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _b_add, _b_clear, _b_reset_all])\n",
    "\n",
    "_plot_box = widgets.HBox([_plot_b])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_time_box, _device_box, _sensor_box, _plot_box])\n",
    "display(_root_box)\n",
    "\n",
    "display(out_smplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Plots (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'STATION_CASE'\n",
    "test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "\n",
    "min_date = '2018-08-03 00:00:00'\n",
    "max_date = '2018-09-17 00:00:00'\n",
    "\n",
    "# Assign by time-frames\n",
    "freq_time = 2\n",
    "\n",
    "# Select labels\n",
    "list_channels = ['EXT_PM_1', 'EXT_PM_25', 'EXT_PM_10', 'TEMP']\n",
    "\n",
    "## -----------\n",
    "dataframePlot = pd.DataFrame()\n",
    "dataframePlot = readings[test]['devices'][device]['data'].copy()\n",
    "dataframePlot = dataframePlot[dataframePlot.index>min_date]\n",
    "dataframePlot = dataframePlot[dataframePlot.index<max_date]\n",
    "\n",
    "if freq_time == 6:\n",
    "    labels = ['Morning','Afternoon','Evening', 'Night']\n",
    "    len_labels = 4\n",
    "elif freq_time == 12:\n",
    "    labels = ['Morning', 'Evening']\n",
    "    len_labels = 2\n",
    "else:\n",
    "    labels = [str(i) for i in np.arange(0, 24, freq_time)]\n",
    "    len_labels = freq_time * len(labels)\n",
    "    \n",
    "vector_time = np.arange(0, 25, freq_time)\n",
    "\n",
    "dataframePlot = dataframePlot.assign(session=pd.cut(dataframePlot.index.hour,\n",
    "                                            vector_time,\n",
    "                                            labels=labels))\n",
    "# Group them by session\n",
    "df_se = dataframePlot.groupby(['session']).mean()\n",
    "df_se = df_se[list_channels]\n",
    "\n",
    "# Calculate average\n",
    "df_se_avg = df_se.mean(axis = 0)\n",
    "\n",
    "display(df_se)\n",
    "\n",
    "# Add additional columns\n",
    "append_rel = '_AVG_REL'\n",
    "\n",
    "list_all = []\n",
    "list_all.append('session')\n",
    "for column in list_channels:\n",
    "    dataframePlot[column + append_rel] = dataframePlot[column]/df_se_avg[column]\n",
    "    list_all.append(column)\n",
    "    list_all.append(column + append_rel)\n",
    "\n",
    "## Full dataframe\n",
    "dataframePlot = dataframePlot[list_all]\n",
    "dataframePlot.dropna()\n",
    "\n",
    "## Dataframe by session\n",
    "dataframePlot_avg = dataframePlot.groupby(['session']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16
    ]
   },
   "outputs": [],
   "source": [
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "channel =  'EXT_PM_25'\n",
    "colorscale = [[0, '#edf8fb'], [.3, '#b3cde3'],  [.6, '#8856a7'],  [1, '#810f7c']]\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=dataframePlot[channel],\n",
    "        x=dataframePlot.index.date,\n",
    "        y=dataframePlot['session'],\n",
    "        colorscale=colorscale,\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Daily Pollutant',\n",
    "    xaxis = dict(ticks=''),\n",
    "    yaxis = dict(ticks='' , categoryarray=labels, autorange = 'reversed')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "\n",
    "channel_1 =  'EXT_PM_1' # Goes on the left side (x1)\n",
    "channel_2 =  'TEMP' # Goes on the right side (x2)\n",
    "relative = False\n",
    "\n",
    "if relative:\n",
    "    channel_1 = channel_1 + append_rel\n",
    "    channel_2 = channel_2 + append_rel\n",
    "    limits = ([0.5, 1.5], [0.5, 1.5])\n",
    "    marks = ()\n",
    "else:\n",
    "    limits = ([0, 20], [15, 40]) # x1, x2\n",
    "    marks = ([10, 'x'], [20, 'x'], [300, 'x2'], [400, 'x2'])\n",
    "\n",
    "dict_shapes = [{'type': 'line', \n",
    "                      'x0': mark[0],\n",
    "                      'y0': -1,\n",
    "                      'x1': mark[0],\n",
    "                      'y1': len_labels+1,\n",
    "                      'xref': mark[1],\n",
    "                      'line': {\n",
    "                          'color': 'rgba(5, 0, 0, 0.8)',\n",
    "                          'width': 1,\n",
    "                          'dash': 'dot'\n",
    "                      }}\n",
    "               for mark in marks]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_1],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x',\n",
    "            yaxis = 'y',\n",
    "            name = channel_1,\n",
    "            marker = dict(color = 'rgba(58, 78, 255, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 78, 255, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_2],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x2',\n",
    "            yaxis='y',\n",
    "            name = channel_2,\n",
    "            marker = dict(color = 'rgba(58, 71, 80, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 71, 80, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(title='Daily average measurement (freq = {}h)'.format(freq_time),\n",
    "                   xaxis = dict(domain=[0, 0.5], autorange='reversed', title = channel_1), # range=(limits[0][0], limits[0][1])),\n",
    "                   xaxis2 = dict(domain=[0.5, 1], title = channel_2), # range=(limits[1][0], limits[1][1])),\n",
    "                   yaxis = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   yaxis2 = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   bargap = 0)#,\n",
    "                   #shapes = dict_shapes)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# AlphaSense Baseline Calibration\n",
    "\n",
    "These functions are used to create the alphasense pollutant correction based on Working, Auxiliary and calibration data provided by alphasense. Run the 1.1.1.1 AlphaSense Sensors calibration data cell to load in the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     60
    ]
   },
   "outputs": [],
   "source": [
    "from src.models.pollutant_cal_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "min_delta = 30\n",
    "max_delta = 60\n",
    "ad_append = 'AD_BASE' + str(min_delta) + '-' + str(max_delta)\n",
    "deltas_co = np.arange(10,10,1)\n",
    "deltas_no2 = np.arange(min_delta,min_delta+max_delta,1)\n",
    "deltas_ox = np.arange(min_delta,min_delta+max_delta,1)\n",
    "\n",
    "out_alpha = widgets.Output()\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    with out_alpha:\n",
    "        clear_output()\n",
    "        for testAD in selectedTestsAD:\n",
    "            # Look for a reference\n",
    "            for reading in records.readings[testAD]['devices']:\n",
    "                if 'is_reference' in records.readings[testAD]['devices'][reading]:\n",
    "                    print ('Reference found')\n",
    "                    refAvail = True\n",
    "                    dataframeRef = records.readings[testAD]['devices'][reading]['data']\n",
    "                    break\n",
    "                else:\n",
    "                    refAvail = False\n",
    "                    dataframeRef = ''\n",
    "    \n",
    "            for kit in records.readings[testAD]['devices']:\n",
    "                if 'alphasense' in records.readings[testAD]['devices'][kit]:\n",
    "                    \n",
    "                    sensorID = records.readings[testAD]['devices'][kit]['alphasense']\n",
    "                    sensorID_CO = records.readings[testAD]['devices'][kit]['alphasense']['CO']\n",
    "                    sensorID_NO2 = records.readings[testAD]['devices'][kit]['alphasense']['NO2']\n",
    "                    sensorID_OX = records.readings[testAD]['devices'][kit]['alphasense']['O3']\n",
    "                    sensorSlots = records.readings[testAD]['devices'][kit]['alphasense']['slots']\n",
    "                                  \n",
    "                    sensorID = (['CO', sensorID_CO, 'classic', 'single_aux', sensorSlots.index('CO')+1, deltas_co], \n",
    "                                ['NO2', sensorID_NO2, 'baseline', 'single_temp', sensorSlots.index('NO2')+1, deltas_no2], \n",
    "                                ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1, deltas_ox])\n",
    "                                        \n",
    "                    # Calculate correction\n",
    "                    records.readings[testAD]['devices'][kit]['alphasense']['model_stats'] = dict()\n",
    "                    records.readings[testAD]['ready_to_model'] = False\n",
    "                    records.readings[testAD]['devices'][kit]['data'], records.readings[testAD]['devices'][kit]['alphasense']['model_stats'][ad_append] = calculatePollutantsAlpha(\n",
    "                            _dataframe = records.readings[testAD]['devices'][kit]['data'], \n",
    "                            _pollutantTuples = sensorID,\n",
    "                            _append = ad_append,\n",
    "                            _refAvail = refAvail, \n",
    "                            _dataframeRef = dataframeRef, \n",
    "                            _overlapHours = overlapHours, \n",
    "                            _type_regress = 'best', \n",
    "                            _filterExpSmoothing = filterExpSmoothing, \n",
    "                            _trydecomp = checkBoxDecomp.value,\n",
    "                            _plotsInter = checkBoxPlotsIn.value, \n",
    "                            _plotResult = checkBoxPlotsResult.value,\n",
    "                            _verbose = checkBoxVerb.value, \n",
    "                            _printStats = checkBoxStats.value,\n",
    "                            _calibrationDataPath = join(dataDirectory, 'interim/CalibrationData/'), \n",
    "                            _currentSensorNames = currentSensorNames)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "testAlphaSense = list()\n",
    "for test in records.readings:\n",
    "    for kit in records.readings[test]['devices']:\n",
    "        if 'alphasense' in records.readings[test]['devices'][kit] and test not in testAlphaSense:\n",
    "            testAlphaSense.append(test)\n",
    "\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline',\n",
    "                                     layout=widgets.Layout(width='200px'))\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "# Synchronise Checkbox\n",
    "\n",
    "checkBoxDecomp = widgets.Checkbox(value=False, \n",
    "                                  description='Decomp')\n",
    "checkBoxPlotsIn = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Inter')     \n",
    "checkBoxVerb = widgets.Checkbox(value=False, \n",
    "                                  description='Verbose') \n",
    "checkBoxPlotsResult = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Results') \n",
    "checkBoxStats = widgets.Checkbox(value=True, \n",
    "                                  description='Print Stats') \n",
    "\n",
    "Box = widgets.VBox([calculateCorrection, checkBoxDecomp, checkBoxPlotsIn, checkBoxVerb, checkBoxPlotsResult, checkBoxStats])\n",
    "display(Box)\n",
    "display(out_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Correction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame()\n",
    "MAX_WE = 150\n",
    "df['working_example'] = np.arange(0,MAX_WE,0.1)\n",
    "WORKING_ZERO = 21.4/6.36\n",
    "SENSITIVITY = 347/6.36\n",
    "\n",
    "def sensitivity_alphadelta (row):\n",
    "    return max(0,(SENSITIVITY * (row-WORKING_ZERO))-1/np.power(row-WORKING_ZERO,2))\n",
    "    # return -1/np.power(row-WORKING_ZERO,2)\n",
    "\n",
    "df['test'] = df.apply(lambda row: sensitivity_alphadelta(row['working_example']) if row['working_example'] > WORKING_ZERO else 0, axis = 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(df.working_example, df.test, 'ko', alpha = 0.7)\n",
    "#plt.plot([WORKING_ZERO,WORKING_ZERO], [-100, 100])\n",
    "plt.xlabel('WE (mV)')\n",
    "plt.ylabel('Pollutant (ppm)')\n",
    "plt.title('Sensitivity Plot (S = {} // Zero = {})'.format(SENSITIVITY, WORKING_ZERO))\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([-10,400])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore results\n",
    "# test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "test = 'STATION'\n",
    "# device = 'STATION CHIMNEY'\n",
    "device = '4748'\n",
    "dataframe = readings[test]['devices'][device]['alphasense']['model_stats'][ad_append]\n",
    "dataframe['CO'] = dataframe['CO'].iloc[1:,:]\n",
    "dataframe['NO2'] = dataframe['NO2'].iloc[1:,:]\n",
    "\n",
    "\n",
    "fig, (ax, ax2)= plt.subplots(nrows = 2, figsize= (15,10))\n",
    "ax.bar(dataframe['CO'].index, dataframe['CO']['pollutant_avg'], label='CO')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=70 )\n",
    "ax2.bar(dataframe['NO2'].index,dataframe['NO2']['pollutant_avg'], label='NO2')\n",
    "#ax.plot(dataframe['O3'].index,dataframe['O3']['pollutant_avg'],'ro', label='O3')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=70 )\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.title('Average pollutant concentration')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MICS Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "from test_utils import ref_append\n",
    "%matplotlib inline\n",
    "mics_append = 'MICS_BASE_CALC'\n",
    "\n",
    "selectedTestsMICS = tuple()\n",
    "def selectTestMICS(x):\n",
    "    global selectedTestsMICS\n",
    "    selectedTestsMICS = list(x)\n",
    "    \n",
    "def calculateCorrectionMICS(b):\n",
    "    clear_output()\n",
    "    for testMICS in selectedTestsMICS:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testMICS]['devices']:\n",
    "            # If there is reference, use it\n",
    "            if 'is_reference' in readings[testMICS]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data']\n",
    "                break\n",
    "            # If not, at least use alphasense data\n",
    "            elif 'alphasense' in readings[testMICS]['devices'][reading]:\n",
    "                refAvail = True\n",
    "                \n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data'].loc[:,['CO_' + ad_append, 'NO2_' + ad_append, 'O3_' + ad_append ]]\n",
    "                # Rename to be a reference\n",
    "                for name in dataframeRef.columns:\n",
    "                    namesub = re.sub(ad_append, ref_append, name)\n",
    "                    dataframeRef.rename(columns={name: namesub}, inplace=True)\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in deviceMICS:\n",
    "            if 'mics' in readings[testMICS]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testMICS]['devices'][kit]['mics']               \n",
    "                sensorID = (['CO', sensorID, 'baseline', 'single_temp'], \n",
    "                            ['NO2', sensorID, 'baseline', 'single_temp'])\n",
    "            \n",
    "            # Temporary until better understanding\n",
    "            else:\n",
    "                sensorID = (['CO', 1, 'baseline', 'single_temp'], \n",
    "                            ['NO2', 1, 'baseline', 'single_temp'])\n",
    "                \n",
    "            # Calculate correction\n",
    "            readings[testMICS]['devices'][kit]['data'], CorrParams = calculatePollutantsMICS(\n",
    "                        _dataframe = readings[testMICS]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = mics_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltasMICS,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have measured the mics\n",
    "testMICS = list()\n",
    "deviceMICS = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        columnsTest = readings[test]['devices'][kit]['data'].columns\n",
    "        if ('CO_MICS_RAW' in columnsTest or 'NO2_MICS_RAW' in columnsTest):\n",
    "            if test not in testMICS:\n",
    "                testMICS.append(test)\n",
    "            if kit not in deviceMICS:\n",
    "                deviceMICS.append(kit)\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing MICS to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestMICS,\n",
    "         x = widgets.SelectMultiple(options=testMICS, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsMICS, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionMICS)\n",
    "deltasMICS = np.arange(1,100,1)\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Model Process\n",
    "\n",
    "Inspired by the example of \"Jakob Aungiers, Altum Intelligence ltd\" at https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Utils\n",
    "from src.models.linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from numpy import concatenate\n",
    "\n",
    "# ML Utils\n",
    "from src.models.ml_utils import prep_dataframe_ML, fit_model_ML, predict_ML, get_inverse_transform_ML\n",
    "\n",
    "# Metrics \n",
    "from src.data.signal_utils import metrics\n",
    "import json\n",
    "\n",
    "## Input\n",
    "##--------------\n",
    "configs = json.load(open('./spotcheck_models/models.json', 'r'))\n",
    "test_model = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "device_name = 'STATION_CASE'\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "##--------------\n",
    "\n",
    "for config in configs.keys():\n",
    "    \n",
    "    # Retrieve models\n",
    "    model_name = config\n",
    "    print model_name\n",
    "\n",
    "    if model_name not in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "        model_type = configs[config]['type']\n",
    "        features = configs[config]['features']\n",
    "        \n",
    "        tuple_features = [(k, v) for k, v in features.iteritems()]\n",
    "        print tuple_features\n",
    "        \n",
    "        \n",
    "        list_features = list()\n",
    "        for item in tuple_features: \n",
    "            if item[0] == 'REF':\n",
    "                a = tuple_features.index(item)\n",
    "                list_features.insert(0,item[1] + '_' + device_name)\n",
    "                reference_name = item[1] + '_' + device_name\n",
    "            else:\n",
    "                list_features.append(item[1] + '_' + device_name)\n",
    "                \n",
    "        tuple_features[0], tuple_features[a] = tuple_features[a], tuple_features[0]\n",
    "        \n",
    "        tuple_features_combined = [(item[0], item[1]  + '_' + device_name) for item in tuple_features]\n",
    "        \n",
    "        dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].loc[:,list_features]\n",
    "        dataframeModel = dataframeModel.dropna()\n",
    "        \n",
    "        dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "        dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "        \n",
    "        print '\\t{}'. format(model_type) \n",
    "        if model_type == 'OLS':\n",
    "            \n",
    "            formula_expression = configs[config]['expression']\n",
    "            ratio_train = configs[config]['ratio_train']\n",
    "            alpha_filter = configs[config]['alpha_filter']\n",
    "            n_lags = 1\n",
    "\n",
    "            ## Model Fit\n",
    "            dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, tuple_features_combined, ratio_train, alpha_filter, device_name)\n",
    "            model = fit_model_OLS(formula_expression, dataTrain, False)\n",
    "            \n",
    "            ## Predict the model results\n",
    "            referenceTrain, predictionTrain = predict_OLS(model, dataTrain, True, False, 'train')\n",
    "            referenceTest, predictionTest = predict_OLS(model, dataTest, True, False, 'test')\n",
    "\n",
    "            predictionTrain = predictionTrain.values\n",
    "            indexTrain = dataTrain['index']\n",
    "            indexTest = dataTest['index']\n",
    "\n",
    "        elif model_type == 'LSTM':\n",
    "            \n",
    "            epochs = configs[config]['epochs']\n",
    "            batch_size = configs[config]['batch_size']\n",
    "            verbose = configs[config]['verbose']\n",
    "            n_lags = configs[config]['n_lags']\n",
    "            loss = configs[config][\"loss\"]\n",
    "            optimizer = configs[config][\"optimizer\"]\n",
    "            layers = configs[config]['layers']\n",
    "            ratio_train = configs[config]['ratio_train']\n",
    "\n",
    "            ## Prep Dataframe\n",
    "            index, train_X, train_y, test_X, test_y, scalerX, scalery, n_train_periods = prep_dataframe_ML(dataframeModel, min_date, max_date, list_features, n_lags, ratio_train, alpha_filter, reference_name, False)\n",
    "            ## Model Fit\n",
    "            model = fit_model_ML(train_X, train_y, \n",
    "                                 test_X, test_y, \n",
    "                                 epochs = epochs, \n",
    "                                 batch_size = batch_size, \n",
    "                                 verbose = True, \n",
    "                                 plotResult = False, \n",
    "                                 loss = loss, \n",
    "                                 optimizer = optimizer,\n",
    "                                 layers = layers)\n",
    "            \n",
    "            # Get model prediction\n",
    "            \n",
    "            # Get model prediction\n",
    "            referenceTrain = get_inverse_transform_ML(train_y, n_lags, scalery)\n",
    "            predictionTrain = predict_ML(model, train_X, n_lags, scalery)\n",
    "\n",
    "            referenceTest = get_inverse_transform_ML(test_y, n_lags, scalery)\n",
    "            predictionTest = predict_ML(model, test_X, n_lags, scalery)\n",
    "            \n",
    "            indexTrain = index[:n_train_periods]\n",
    "            indexTest = index[n_train_periods+n_lags:]\n",
    "            formula_expression = '-'\n",
    "        \n",
    "    \n",
    "        dataFrameTrain = pd.DataFrame(data = {'reference': referenceTrain, 'prediction': predictionTrain}, \n",
    "                                      index = indexTrain)\n",
    "        dataFrameTest = pd.DataFrame(data = {'reference': referenceTest, 'prediction': predictionTest}, \n",
    "                                      index = indexTest)\n",
    "        \n",
    "        dataFrameExport = dataFrameTrain.copy()\n",
    "        dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "        # Get model metrics\n",
    "        metrics_model_train = metrics(referenceTrain, predictionTrain)\n",
    "        metrics_model_test = metrics(referenceTest, predictionTest)\n",
    "        \n",
    "        ## Put everything in the dict\n",
    "        dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "        \n",
    "        # From https://hackmd.io/Y62wiJw0RaiBfU4Xhv8dQQ#\n",
    "        dictModel[model_name] = dict()\n",
    "        dictModel[model_name]['metrics'] = dict()\n",
    "        dictModel[model_name]['metrics']['train'] = metrics_model_train\n",
    "        dictModel[model_name]['metrics']['test'] = metrics_model_test\n",
    "        \n",
    "        # Model Parameters\n",
    "        dictModel[model_name]['parameters'] = dict()\n",
    "        dictModel[model_name]['parameters']['features'] = dict()\n",
    "        dictModel[model_name]['parameters']['features']['ref'] = tuple_features[0]\n",
    "        dictModel[model_name]['parameters']['features']['items'] = tuple_features[1:]        \n",
    "        dictModel[model_name]['parameters']['ratio_train'] = ratio_train\n",
    "        dictModel[model_name]['parameters']['alpha_filter'] = alpha_filter\n",
    "        dictModel[model_name]['model'] = model\n",
    "        dictModel[model_name]['modelType'] = model_type\n",
    "        \n",
    "        # Put it back in the readings dataframe\n",
    "        readings[test_model]['devices'][name_combined_data]['model'][model_name] = dictModel[model_name]\n",
    "        readings[test_model]['devices'][model_name] = dict()\n",
    "        readings[test_model]['devices'][model_name]['data'] = dataFrameExport\n",
    "        print '\\t Model Calculated'\n",
    "        \n",
    "    else:\n",
    "        print '\\t Model already present, skipping'\n",
    "\n",
    "print 'All models calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Checks\n",
    "\n",
    "\n",
    "\n",
    "#### Data stationarity\n",
    "\n",
    "We will use the Dicker-fuller test (ADF) test to verify the data is stationary. We need to check data stationarity for certain type of models. \n",
    "\n",
    "If the process is stationary means it doesn’t change its statistical properties over time: mean and variance do not change over time (constancy of variance is also called homoscedasticity), also covariance function does not depend on the time (should only depend on the distance between observations) Source [here](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3). Visually:\n",
    "\n",
    "- Growing mean --> Non stationary\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/0*qrYiVksz8g3drl5Z.png)\n",
    "\n",
    "- Growing spread --> Non stationary\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/0*fEqQDq_TaEqa511n.png)\n",
    "\n",
    "- Varying time covariance --> Non stationary\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qJs3g2f77flIXr6mFsbPmw.png)\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary). Hence:\n",
    "\n",
    "- p-value > 0.05: fail to reject the null hypothesis (H0), the data has an unit root and is non-stationary.\n",
    "- p-value <= 0.05: reject the null hypothesis (H0), the data does not have an unit root and is stationary.\n",
    "\n",
    "#### Autocorrelation\n",
    "\n",
    "High levels of autocorrelation can indicate our time series is shows seasonality. We will use the ACF plot to identify possible autocorrelation and potentially include differentiation.\n",
    "\n",
    "#### Granger Casuality Test (use with caution)\n",
    "\n",
    "This test is useful to determine the casuality of variables and determining whether one time series is useful in forecasting another. \n",
    "\n",
    "The Null hypothesis for granger causality tests is that the time series in the second column, x2, does NOT Granger cause the time series in the first column, x1. Grange causality means that past values of x2 have a statistically significant effect on the current value of x1, taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test. Hence:\n",
    "\n",
    "- p-value < size: allows to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "- p-value > size: we fail to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "\n",
    "The null hypothesis for all four test is that the coefficients corresponding to past values of the second time series are zero.\n",
    "\n",
    "Reference [here](https://en.wikipedia.org/wiki/Granger_causality), [here](https://stats.stackexchange.com/questions/24753/interpreting-granger-causality-tests-results#24796) and [here](http://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.grangercausalitytests.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from src.models.linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS, plot_OLS_coeffs\n",
    "from src.models.linear_regression_utils import tfuller_plot\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "test_model = '2019-02_EXT_DUBLIN_URBAN_BACKGROUND'\n",
    "tuple_features = (['REF', 'NO2_REF', 'CITY_COUNCIL'],\n",
    "                 ['A', 'GB_2W', '5262'],\n",
    "                 ['B', 'GB_2A', '5262'])\n",
    "\n",
    "min_date = '2018-12-01 00:00:00'\n",
    "max_date = '2019-01-20 00:00:00'\n",
    "\n",
    "model_name = 'TEST_MODEL'\n",
    "model_target = 'ALPHASENSE' # ALPHASENSE, PMS, MICS...\n",
    "model_type = 'OLS'\n",
    "formula_expression = 'REF ~ A + B'\n",
    "alpha_filter = None #No filtering\n",
    "ratio_train = 3./4 # Important that this is a float, don't forget the .\n",
    "model_full_name = '_'.join([model_target, model_type, model_name])\n",
    "print ('Model Name', model_full_name)\n",
    "\n",
    "print ('Preparing devices from test {}'.format(test_model))\n",
    "records.prepare_dataframe_model(tuple_features, test_model, None, None, \n",
    "                                      model_full_name, clean_na = True, clean_na_method = 'fill' , \n",
    "                                      target_raster = '5Min')\n",
    "\n",
    "dataframeModel = records.readings[test_model]['models'][model_full_name]['data']\n",
    "reference_name = records.readings[test_model]['models'][model_full_name]['reference']\n",
    "labels = dataframeModel[reference_name]\n",
    "features = dataframeModel.drop(reference_name, axis = 1)\n",
    "\n",
    "# List of features for later use\n",
    "feature_list = list(features.columns)\n",
    "n_features = len(tuple_features)-1\n",
    "\n",
    "## Preliminary Check\n",
    "# Fuller Plot for all\n",
    "# for column in dataframeModel:\n",
    "#     x = dataframeModel.loc[:,column]\n",
    "#     tfuller_plot(x, name = x.name, lags=60, lags_diff = 5)\n",
    "# Granger Causality\n",
    "# print ('--------------------------------------')\n",
    "# print ('Granger Causality Test')\n",
    "## Granger Causality Test (WIP)\n",
    "# for item in tuple_features:\n",
    "#     if item[0] != 'REF':\n",
    "#         print '\\nCausality for x1 = {} and x2 = {}'.format(reference_name, item[1])\n",
    "#         x = dataframeModel.loc[:,[reference_name, item[1]]].dropna()\n",
    "#         x = x.values\n",
    "#         granger_causality_tests = grangercausalitytests(x, 1)\n",
    "#         # print granger_causality_tests\n",
    "\n",
    "## Prepare Dataframe\n",
    "dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, \n",
    "                                                     tuple_features, \n",
    "                                                     ratio_train)\n",
    "\n",
    "## Model Fit\n",
    "model = fit_model_OLS(formula_expression, dataTrain, printSummary = True)\n",
    "plot_OLS_coeffs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from src.models.linear_regression_utils import predict_OLS\n",
    "from src.data.signal_utils import metrics\n",
    "import pandas as pd\n",
    "\n",
    "## Predict the model results\n",
    "dataFrameTrain = predict_OLS(model, dataTrain, True, False, 'train')\n",
    "dataFrameTest = predict_OLS(model, dataTest, True, False, 'test')\n",
    "\n",
    "## Combine them for export\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get Metrics\n",
    "metrics_model = dict()\n",
    "metrics_model['train'] = metrics(dataFrameTrain['reference'], dataFrameTrain['prediction'])\n",
    "metrics_model['test'] = metrics(dataFrameTest['reference'], dataFrameTest['prediction'])\n",
    "\n",
    "records.archive_model(test_model, model_full_name, \n",
    "                      metrics_model, \n",
    "                      dataFrameExport, model, model_type, \n",
    "                      model_target, ratio_train, formula_expression, alpha_filter = alpha_filter)\n",
    "\n",
    "print ('Metrics Summary:')\n",
    "print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "for metric in metrics_model['train'].keys():\n",
    "    print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models.linear_regression_utils import model_R_plots\n",
    "%matplotlib inline\n",
    "\n",
    "model_R_plots(model, dataTrain, dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models.ml_utils import prep_dataframe_ML, fit_model_ML, predict_ML, get_inverse_transform_ML\n",
    "from src.data.signal_utils import metrics\n",
    "\n",
    "test_model = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "tuple_features = (['REF', 'GB_1W', 'STATION_CASE'],\n",
    "                 ['A', 'CO_MICS_RAW', 'STATION_CASE'],\n",
    "                 ['B', 'TEMP', 'STATION_CASE'],\n",
    "                 ['C', 'HUM', 'STATION_CASE'],\n",
    "                 ['D', 'PM_25', 'STATION_CASE'])\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "model_name = 'TEST_MODEL'\n",
    "model_target = 'MICS' # ALPHASENSE, PMS, MICS...\n",
    "model_type = 'LSTM' # \n",
    "ratio_train = 3./4 # Important that this is a float, don't forget the .\n",
    "n_lags = 2\n",
    "model_full_name = '_'.join([model_target, model_type, model_name, 'n_lags-{}'.format(n_lags)])\n",
    "print ('Model Name', model_full_name)\n",
    "\n",
    "# Number of lags for the model\n",
    "n_lags = 4\n",
    "\n",
    "print ('Preparing devices from test {}'.format(test_model))\n",
    "records.prepare_dataframe_model(tuple_features, test_model, min_date, max_date, \n",
    "                                      model_full_name, clean_na = True, clean_na_method = 'drop' , \n",
    "                                      target_raster = '10Min')\n",
    "\n",
    "dataframeModel = records.readings[test_model]['models'][model_full_name]['data']\n",
    "reference_name = records.readings[test_model]['models'][model_full_name]['reference']\n",
    "\n",
    "labels = dataframeModel[reference_name]\n",
    "features = dataframeModel.drop(reference_name, axis = 1)\n",
    "\n",
    "# List of features for later use\n",
    "feature_list = list(features.columns)\n",
    "n_features = len(tuple_features)-1\n",
    "\n",
    "# Data Split\n",
    "train_X, train_y, test_X, test_y, scalerX, scalery = prep_dataframe_ML(dataframeModel, \n",
    "                                                                       n_features, \n",
    "                                                                       n_lags, ratio_train)\n",
    "\n",
    "index = dataframeModel.index\n",
    "n_train_periods = train_X.shape[0]\n",
    "\n",
    "# Model Fit\n",
    "print ('Model training...')\n",
    "model = fit_model_ML('LSTM', train_X, train_y, \n",
    "                       test_X, test_y, \n",
    "                       epochs = 50, batch_size = 72, \n",
    "                       verbose = 2, plotResult = True, \n",
    "                       loss = 'mse', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.models.ml_utils import predict_ML, get_inverse_transform_ML\n",
    "from src.data.signal_utils import metrics\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Get model prediction\n",
    "inv_train_y = get_inverse_transform_ML(train_y, scalery)\n",
    "dataFrameTrain = predict_ML(model, train_X, \n",
    "                            inv_train_y, index[:n_train_periods],\n",
    "                            scalery)\n",
    "\n",
    "inv_test_y = get_inverse_transform_ML(test_y, scalery)\n",
    "dataFrameTest = predict_ML(model, test_X, \n",
    "                           inv_test_y, index[n_train_periods+n_lags:], \n",
    "                           scalery)\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get Metrics\n",
    "print ('Calculating Metrics...')\n",
    "metrics_model = dict()\n",
    "metrics_model['train'] = metrics(dataFrameTrain['reference'], dataFrameTrain['prediction'])\n",
    "metrics_model['test'] = metrics(dataFrameTest['reference'], dataFrameTest['prediction'])\n",
    "\n",
    "print ('Archiving Model...')\n",
    "records.archive_model(test_model, model_full_name, \n",
    "                      metrics_model, \n",
    "                      dataFrameExport, model, model_type, \n",
    "                      model_target, ratio_train, n_lags = n_lags, scalerX = scalerX, scalery = scalery)\n",
    "\n",
    "print ('Metrics Summary:')\n",
    "print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "for metric in metrics_model['train'].keys():\n",
    "    print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Random Forest Regressor or SVR\n",
    "\n",
    "No feature scaling implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from src.data.signal_utils import metrics\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_model = '2019-02_EXT_DUBLIN_URBAN_BACKGROUND'\n",
    "tuple_features = (['REF', 'NO2_REF', 'CITY_COUNCIL'],\n",
    "                 ['A', 'GB_2W', '5262'],\n",
    "                 ['B', 'GB_2A', '5262'],\n",
    "                 ['C', 'PM_DALLAS_TEMP', '5262'])\n",
    "\n",
    "min_date = '2018-12-01 00:00:00'\n",
    "max_date = '2019-01-20 00:00:00'\n",
    "\n",
    "model_name = 'NO2_Dublin_W-A-T-Shuffle'\n",
    "model_target = 'ALPHASENSE' # ALPHASENSE, PMS, MICS...\n",
    "shuffle_split = True\n",
    "model_type = 'RF' # RF or SVR\n",
    "ratio_train = 3./4 # Important that this is a float, don't forget the .\n",
    "model_full_name = '_'.join([model_target, model_type, model_name])\n",
    "print ('Model Name', model_full_name)\n",
    "\n",
    "print ('Preparing devices from test {}'.format(test_model))\n",
    "records.prepare_dataframe_model(tuple_features, test_model, min_date, max_date, \n",
    "                                      model_full_name, clean_na = True, clean_na_method = 'drop' , \n",
    "                                      target_raster = '1Min')\n",
    "\n",
    "dataframeModel = records.readings[test_model]['models'][model_full_name]['data']\n",
    "reference_name = records.readings[test_model]['models'][model_full_name]['reference']\n",
    "\n",
    "labels = dataframeModel[reference_name]\n",
    "features = dataframeModel.drop(reference_name, axis = 1)\n",
    "\n",
    "# List of features for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "# Training and Testing Sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, random_state = 42, \n",
    "                                                    test_size = 1-ratio_train, shuffle = shuffle_split)\n",
    "\n",
    "n_train_periods = train_X.shape[0]\n",
    "print('Training X Shape:', train_X.shape)\n",
    "print('Training y Shape:', train_y.shape)\n",
    "print('Testing X Shape:', test_X.shape)\n",
    "print('Testing y Shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Single Model Fit - Prediction and Archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ml_utils import predict_ML\n",
    "\n",
    "# Instantiate model \n",
    "if model_type == 'RF':\n",
    "    model = RandomForestRegressor(n_estimators= 1000, random_state = 42)\n",
    "elif model_type == 'SVR':\n",
    "    model = SVR(kernel='rbf')\n",
    "    \n",
    "## Train the model on training data\n",
    "print ('Training Model {}...'.format(model_name))\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "## Get model prediction\n",
    "dataFrameTrain = predict_ML(model, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest = predict_ML(model, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])\n",
    "\n",
    "# Get model metrics\n",
    "print ('Calculating Metrics...')\n",
    "metrics_model = dict()\n",
    "metrics_model['train'] = metrics(dataFrameTrain['reference'], dataFrameTrain['prediction'])\n",
    "metrics_model['test'] = metrics(dataFrameTest['reference'], dataFrameTest['prediction'])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "print ('Archiving Model...')\n",
    "records.archive_model(test_model, model_full_name, \n",
    "                      metrics_model, \n",
    "                      dataFrameExport, model, model_type, \n",
    "                      model_target, ratio_train, shuffle_split)\n",
    "\n",
    "print ('Metrics Summary:')\n",
    "print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "for metric in metrics_model['train'].keys():\n",
    "    print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search with Cross Validation \n",
    "\n",
    "We perform here cross validated random search of the model hyperparameters, to later on retrieve the best parameters with a grid search around the best found results of the CV.\n",
    "\n",
    "Using **k-fold cross validation** below:\n",
    "\n",
    "![](https://i.imgur.com/HLbgMSS.png)\n",
    "\n",
    "Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from src.models.ml_utils import predict_ML\n",
    "\n",
    "if model_type == 'RF':\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    ## Evaluate the default model\n",
    "    base_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    random_model = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "elif model_type == 'SVR':\n",
    "    \n",
    "    \n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    random_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "                   \"gamma\": np.logspace(-2, 2, 5),\n",
    "                   \"kernel\": ['rbf', 'sigmoid'],\n",
    "                  \"shrinking\": [True, False]}\n",
    "    \n",
    "    ## Create the default model\n",
    "    base_model = SVR(kernel='rbf', gamma=0.1)\n",
    "\n",
    "    ## Create randomized Search\n",
    "    random_model = RandomizedSearchCV(estimator = SVR(), cv=5, \n",
    "                             n_iter = 100, scoring = 'neg_mean_absolute_error',\n",
    "                             param_distributions=random_grid,  verbose=2, random_state=42, n_jobs=-1)\n",
    "    \n",
    "# Fit the base model\n",
    "base_model.fit(train_X, train_y)\n",
    "## Get base model prediction\n",
    "dataFrameTrain_base = predict_ML(base_model, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_base = predict_ML(base_model, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])\n",
    "\n",
    "# Fit the random search model\n",
    "random_model.fit(train_X, train_y)\n",
    "random_model.best_params_\n",
    "best_random = random_model.best_estimator_\n",
    "## Evaluate the best model\n",
    "dataFrameTrain_best = predict_ML(best_random, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_best = predict_ML(best_random, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if model_type == 'RF':\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'n_estimators': [200, 300, 400, 1000]\n",
    "    }\n",
    "        \n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, \n",
    "                               scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                               n_jobs = -1, verbose = 2)\n",
    "elif model_type == 'SVR':\n",
    "\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "                   \"gamma\": np.logspace(-2, 2, 5),\n",
    "                  \"shrinking\": [True, False]\n",
    "    }\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator = SVR(), param_grid = param_grid, \n",
    "                               scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                               n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best grid estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "print (best_grid)\n",
    "dataFrameTrain_best_grid = predict_ML(best_grid, features[:n_train_periods], labels[:n_train_periods], dataframeModel.index[:n_train_periods])\n",
    "dataFrameTest_best_grid = predict_ML(best_grid, features[n_train_periods:], labels[n_train_periods:], dataframeModel.index[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the best predictions of the grid search, put them in the dataframe for plotting and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameExport = dataFrameTrain_best_grid.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest_best_grid)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model = dict()\n",
    "metrics_model['train'] = metrics(dataFrameTrain_best_grid['reference'], dataFrameTrain_best_grid['prediction'])\n",
    "metrics_model['test'] = metrics(dataFrameTest_best_grid['reference'], dataFrameTest_best_grid['prediction'])\n",
    "\n",
    "records.archive_model(test_model, model_full_name + '_best_grid_search', \n",
    "                      metrics_model, \n",
    "                      dataFrameExport, best_grid, model_type, \n",
    "                      model_target, ratio_train)\n",
    "\n",
    "print ('Metrics Summary:')\n",
    "print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "for metric in metrics_model['train'].keys():\n",
    "    print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ml_utils import plot_model_ML\n",
    "%matplotlib inline\n",
    "\n",
    "plot_model_ML(model, dataFrameTrain, dataFrameTest, feature_list, model_type, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model_export = widgets.Output()\n",
    "\n",
    "selectedModel = tuple()\n",
    "def selectModel(Models):\n",
    "    global selectedModel\n",
    "    selectedModel = list(Models)\n",
    "    # selectedTestBases = list()\n",
    "    # selectedTestBases.append('')\n",
    "    # for test in selectedTest:\n",
    "    #     selectedTestBases.append(basename(normpath(test)))\n",
    "    # name_drop_api.options = selectedTestBases\n",
    "    \n",
    "def exportModel(b):\n",
    "    with out_model_export:\n",
    "        clear_output()\n",
    "        for model_selected in selectedModel:\n",
    "            print ('Exporting model', model_selected, '...')\n",
    "            records.export_model(test_model, model_selected, modelDirectory)\n",
    "            print ('---')\n",
    "        \n",
    "button_model_export = widgets.Button(description='Export Selected')\n",
    "button_model_export.on_click(exportModel)\n",
    "\n",
    "interact(selectModel,\n",
    "    Models = widgets.SelectMultiple(options=records.readings[test_model]['models'].keys(), \n",
    "                            selected_labels = selectedModel,\n",
    "                            layout=widgets.Layout(width='700px')))\n",
    "\n",
    "# records.export_model(test_model, model_full_name, modelDirectory)\n",
    "display(button_model_export)\n",
    "display(out_model_export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeries Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "referencePlotted = False\n",
    "        \n",
    "for model_name in records.readings[test_model]['models']:\n",
    "    try:\n",
    "        ratio_train = records.readings[test_model]['models'][model_name]['parameters']['ratio_train']\n",
    "        data = records.readings[test_model]['devices'][model_name]['data']\n",
    "                \n",
    "        total_len = len(data.index)\n",
    "        n_train_periods = int(round(total_len*ratio_train))\n",
    "    \n",
    "        dataframeTrain = data.iloc[:n_train_periods,:]\n",
    "        dataframeTest = data.iloc[n_train_periods:,:]\n",
    "                        \n",
    "        if (not referencePlotted):\n",
    "            plt.plot(dataframeTrain.index, dataframeTrain['reference'], 'b.', label = 'Reference Train', alpha = 0.3)\n",
    "            plt.plot(dataframeTest.index, dataframeTest['reference'], 'b.', label = 'Reference Test', alpha = 0.3)\n",
    "            referencePlotted = True\n",
    "            \n",
    "        plt.plot(dataframeTrain.index, dataframeTrain['prediction'], linewidth = 0.6, label = 'Prediction Train ' + model_name)\n",
    "        plt.plot(dataframeTest.index, dataframeTest['prediction'], linewidth = 0.6, label = 'Prediction Test ' + model_name)\n",
    "    except:\n",
    "        pass\n",
    "plt.legend(loc = 'best')\n",
    "# plt.ylabel(str(readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['features'][\"ref\"][1]))\n",
    "plt.xlabel('Date (-)')\n",
    "plt.grid(True)\n",
    "# plt.title('Model Comparison for ' + str(readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['features'][\"ref\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "number_of_subplots = len(records.readings[test_model]['models'].keys()) \n",
    "if number_of_subplots % 2 == 0: cols = 2\n",
    "else: cols = 3\n",
    "rows = int(math.ceil(number_of_subplots / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "n = 0\n",
    "\n",
    "for model_name in records.readings[test_model]['models']:\n",
    "    try:\n",
    "        ratio_train = records.readings[test_model]['models'][model_name]['parameters']['ratio_train']\n",
    "    \n",
    "        data = records.readings[test_model]['devices'][model_name]['data']\n",
    "        dataVal = data.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)    \n",
    "        total_len = len(dataVal.index)\n",
    "        n_train_periods = int(round(total_len*ratio_train))\n",
    "        \n",
    "        dataframeTrain = dataVal.iloc[:n_train_periods,:]\n",
    "        dataframeTest = dataVal.iloc[n_train_periods:,:]\n",
    "    \n",
    "        ax = fig.add_subplot(gs[n])\n",
    "        n += 1          \n",
    "        plot.plot(dataframeTrain['reference'], dataframeTrain['prediction'], 'go', label = 'Train ' + model_name, alpha = 0.3)\n",
    "        plot.plot(dataframeTest['reference'], dataframeTest['prediction'], 'bo', label = 'Test ' + model_name, alpha = 0.3)\n",
    "        plot.plot(dataframeTrain['reference'], dataframeTrain['reference'], 'k', label = '1:1 Line', linewidth = 0.4, alpha = 0.3)\n",
    "     \n",
    "        plot.legend(loc = 'best', prop={'size': 8})\n",
    "        plot.ylabel('Prediction (-)')\n",
    "        plot.xlabel('Reference (-)')\n",
    "        plot.show()\n",
    "    except:\n",
    "        print (\"Model {} was not archived properly\".format(model_name))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "from src.data.signal_utils import metrics\n",
    "\n",
    "number_of_subplots = 3\n",
    "if number_of_subplots % 2 == 0: cols = 2\n",
    "else: cols = 3\n",
    "rows = int(math.ceil(number_of_subplots / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "fig = plot.figure(figsize=(20,6))\n",
    "\n",
    "fig.tight_layout()\n",
    "n = 0\n",
    "test = '2018-09_EXT_BOLOGNA_TEST_WALL_MO'\n",
    "\n",
    "pollutant_list = ['CO_AD_BASE1-60', 'NO2_AD_BASE1-60', 'O3_AD_BASE1-60']\n",
    "ref_list = ['CO_REF', 'NO2_REF', 'O3_REF']\n",
    "min_date = '2018-08-21 14:45:00'\n",
    "max_date = '2018-08-28 23:45:00'\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    data_source_1 = records.readings[test]['devices']['SCK2']['data']\n",
    "    # data_source_2 = records.readings[test]['devices']['5528']['data']\n",
    "    data_ref = records.readings[test]['devices']['ARPAE_MO']['data']\n",
    "    \n",
    "    data_source_1_val = data_source_1.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)\n",
    "    data_ref_val = data_ref.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)\n",
    "    \n",
    "    data_source_1_val = data_source_1_val[data_source_1_val.index > min_date]\n",
    "    data_source_1_val = data_source_1_val[data_source_1_val.index < max_date]\n",
    "\n",
    "    data_ref_val = data_ref_val[data_ref_val.index > min_date]\n",
    "    data_ref_val = data_ref_val[data_ref_val.index < max_date]\n",
    "    \n",
    "    ax = fig.add_subplot(gs[n])\n",
    "    n += 1          \n",
    "    plot.plot(data_ref_val[ref_list[i]], data_source_1_val[pollutant_list[i]], 'go', label = 'SC Station #5527', alpha = 0.5)\n",
    "    # plot.plot(data_ref[ref_list[i]], data_source_2[pollutant_list[i]], 'bo', label = 'SC Station #5528', alpha = 0.3)\n",
    "    plot.plot(data_ref_val[ref_list[i]], data_ref_val[ref_list[i]], 'k', label = '1:1 Line', linewidth = 0.4, alpha = 0.3)\n",
    " \n",
    "    plot.legend(loc = 'best')\n",
    "    plot.ylabel('Pollutant {}'.format(ref_list[i]))\n",
    "    plot.xlabel('Pollutant {}'.format(ref_list[i]))\n",
    "    plot.title('Hola')\n",
    "    \n",
    "    metrics_poll = metrics(data_ref_val[ref_list[i]], data_source_1_val[pollutant_list[i]])\n",
    "    print (metrics_poll)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     12,
     18
    ]
   },
   "outputs": [],
   "source": [
    "from src.visualization.visualization import targetDiagram\n",
    "%matplotlib inline\n",
    "\n",
    "for model in records.readings[test_model]['models']:\n",
    "    try:\n",
    "        print ('-----------------------------------------------------')\n",
    "        print ('\\nModel Name: {}'.format(model))\n",
    "        print (\"{:<23} {:<7} {:<5}\".format('Metric','Train','Test'))\n",
    "        metrics_model = records.readings[test_model]['models'][model]['metrics']\n",
    "        for metric in metrics_model['train'].keys():\n",
    "            print (\"{:<20}\".format(metric) +\"\\t\" +\"{:0.3f}\".format(metrics_model['train'][metric]) +\"\\t\"+ \"{:0.3f}\".format(metrics_model['test'][metric]))\n",
    "    except:\n",
    "        print ('Cannot use model {}'.format(model))\n",
    "targetDiagram(records.readings[test_model]['models'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Objectives (TO CHECK)\n",
    "Explained here http://dx.doi.org/10.1016/j.envint.2016.12.007\n",
    "\n",
    "Sensor values Y, reference values x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import linregress\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def fUEREL(ux, values_x, values_Y):\n",
    "    def RSS(values_x, values_Y, intercept, slope):\n",
    "        pre_sum_1 = np.power(values_Y - intercept - np.multiply(slope, values_x), 2)\n",
    "        # pre_sum_2 = np.power(values_Y / (intercept + np.multiply(slope, values_x)) - 1, 2)\n",
    "        \n",
    "        # fig, axes = plot.subplots(1, 2, figsize=(15,10))\n",
    "        # axes[0].plot(pre_sum_1)\n",
    "        # axes[1].plot(pre_sum_2)\n",
    "\n",
    "        RSS = np.sum(np.power(pre_sum_1,2))\n",
    "        \n",
    "        return RSS\n",
    "    \n",
    "    slope, intercept, _, _, _ = linregress(values_x, values_Y)\n",
    "    # fig = plot.figure(figsize=(15,10))\n",
    "    # plot.plot(slope*values_x + intercept, label='Sensor')\n",
    "    # plot.plot(values_Y)\n",
    "    \n",
    "    RSS = RSS(values_x, values_Y, intercept, slope)\n",
    "    n = len(values_x)\n",
    "    if len(values_Y) != n: return\n",
    "    A = RSS/(n-2)-np.power(ux,2)\n",
    "    B = np.power(intercept + (slope-1)*values_x, 2)\n",
    "    C = np.power(A + B, 0.5)\n",
    "    UEREL = np.divide(2*C, values_Y)\n",
    "    \n",
    "    return UEREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "\n",
    "dqo_table = (['PM', 50],\n",
    "            ['O3', 30],\n",
    "            ['CO',25],\n",
    "            ['NO',25],\n",
    "            ['NO2',25],\n",
    "            ['NOX',25],\n",
    "            ['SO2',25])\n",
    "\n",
    "ux = 0\n",
    "test_model = '2019-02_EXT_DUBLIN_URBAN_BACKGROUND'\n",
    "\n",
    "## ---\n",
    "number_of_subplots = len(records.readings[test_model]['models'].keys()) \n",
    "\n",
    "if number_of_subplots % 2 == 0: cols = 2\n",
    "else: cols = 3\n",
    "    \n",
    "rows = int(math.ceil(number_of_subplots / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "n = 0\n",
    "\n",
    "for model in records.readings[test_model]['models']:\n",
    "    try:\n",
    "        data = records.readings[test_model]['devices'][model]['data']\n",
    "        dataVal = data.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)\n",
    "        values_x = dataVal['reference'].values\n",
    "        values_Y = dataVal['prediction'].values\n",
    "        \n",
    "        total_len = len(data.index)\n",
    "        n_train_periods = int(round(total_len*ratio_train))\n",
    "    \n",
    "        ax = fig.add_subplot(gs[n])\n",
    "        n += 1      \n",
    "    \n",
    "        uerel = 100*fUEREL(ux, values_x, values_Y)\n",
    "        \n",
    "        plot.plot(values_x, uerel, 'ko')\n",
    "        plot.xlabel('Ref. conc [ppb]')\n",
    "        plot.ylabel('Rel. Exp. Unc (%)')\n",
    "        plot.ylim([0, 100])\n",
    "        plot.title(model)\n",
    "        plot.axhline(y=25, color='r', linestyle='-')\n",
    "    except:\n",
    "        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "out_model_load = widgets.Output()\n",
    "\n",
    "dict_models = dict()\n",
    "with open(join(modelDirectory, 'summary.json'), 'r') as summary_file:\n",
    "    dict_models = json.load(summary_file)\n",
    "    \n",
    "selectedModels = tuple()\n",
    "def load_selectModels(selected_model):\n",
    "    with out_model_load:\n",
    "        global selectedModels\n",
    "        selectedModels = list(selected_model)\n",
    "        print (selectedModels)\n",
    "    \n",
    "def load_show_devices(Source):\n",
    "    load_device.options = [s for s in list(records.readings[Source]['devices'].keys())]\n",
    "    load_device.source = Source\n",
    "\n",
    "    load_min_date.value = records.readings[Source]['devices'][load_device.value]['data'].index.min()._short_repr\n",
    "    load_max_date.value = records.readings[Source]['devices'][load_device.value]['data'].index.max()._short_repr\n",
    "\n",
    "def loadModel(b):\n",
    "    with out_model_load:\n",
    "        clear_output()\n",
    "            \n",
    "        if len(selectedModels)>0:\n",
    "            global loaded_model\n",
    "            global loaded_params\n",
    "            global loaded_metrics\n",
    "            global loaded_features\n",
    "            global loaded_model_name\n",
    "            \n",
    "            if 'ARCHIVE' in selectedModels[0]:\n",
    "                model_name = selectedModels[0][8:]\n",
    "                print ('Loading model {} from disk'.format(model_name))\n",
    "                filename = join(modelDirectory, load_target_drop.value, model_name)\n",
    "                if load_type_drop.value == \"LSTM\":\n",
    "                    # ML Model\n",
    "                    # Load Model and weights\n",
    "                    json_file = open(filename + \"_model.json\", \"r\")\n",
    "                    loaded_model_json = json_file.read()\n",
    "                    json_file.close()\n",
    "                \n",
    "                    loaded_model = model_from_json(loaded_model_json)\n",
    "                    loaded_model.load_weights(filename + \"_model.h5\")\n",
    "                elif load_type_drop.value == \"OLS\" or load_type_drop.value == 'RF' or load_type_drop.value == 'SVR':\n",
    "                    # OLS, RF, or SVR Model\n",
    "                    loaded_model = joblib.load(filename + '_model.sav')\n",
    "                    \n",
    "                # Load params and metrics\n",
    "                loaded_params = joblib.load(filename + '_parameters.sav')\n",
    "                loaded_metrics = joblib.load(filename + '_metrics.sav')\n",
    "                loaded_features = joblib.load(filename + '_features.sav')\n",
    "                print ('Model loaded from disk')\n",
    "            elif 'SESSION' in selectedModels[0]:\n",
    "                model_name = selectedModels[0][8:]\n",
    "\n",
    "                test_source = list_tests[list_model_session.index(selectedModels[0])]\n",
    "                print ('Using model {} from current session'.format(model_name))\n",
    "\n",
    "                loaded_model = records.readings[test_source]['models'][model_name]['model']\n",
    "                loaded_params = records.readings[test_source]['models'][model_name]['parameters']\n",
    "                loaded_metrics = records.readings[test_source]['models'][model_name]['metrics']\n",
    "                loaded_features = records.readings[test_source]['models'][model_name]['features']\n",
    "                loaded_ref = records.readings[test_source]['models'][model_name]['reference']\n",
    "                print ('Model loaded from session')\n",
    "            display(Markdown('## Model Load'))\n",
    "            display(Markdown(\"Loaded \" + model_name))\n",
    "            display(Markdown('**Model Type** (*loaded_model*):' ))\n",
    "            display(loaded_model)\n",
    "            display(Markdown('**Model Parameters** (*loaded_params*)'))\n",
    "            display(loaded_params)\n",
    "            display(Markdown('**Model Metrics** (*loaded_metrics*)'))\n",
    "            display(loaded_metrics)\n",
    "            display(Markdown('**Model Features** (*loaded_features*)'))\n",
    "            display(loaded_features)\n",
    "            loaded_model_name = model_name\n",
    "        else:\n",
    "            print ('Select one model to load')\n",
    "    \n",
    "def load_show_models(target, mtype):\n",
    "    with out_model_load:\n",
    "        clear_output()\n",
    "        global list_tests\n",
    "        global list_model_session\n",
    "        list_models = list()\n",
    "        for item in dict_models[target]:\n",
    "            if dict_models[target][item] == mtype:\n",
    "                list_models.append('ARCHIVE_' + item)\n",
    "        list_tests = list()\n",
    "        list_model_session = list()\n",
    "        for reading in records.readings:\n",
    "            if 'models' in records.readings[reading]:\n",
    "                for model_name in records.readings[reading]['models']:\n",
    "                    try:\n",
    "                        if records.readings[reading]['models'][model_name]['model_type'] == mtype:\n",
    "                            list_models.append('SESSION_' + model_name)\n",
    "                            list_tests.append(reading)\n",
    "                            list_model_session.append('SESSION_' + model_name)\n",
    "                    except:\n",
    "                        print ('Could not use model {} from current session. Model is not archived'.format(model_name))\n",
    "        load_models.options = list(list_models)\n",
    "        \n",
    "def load_calculateModel(b):\n",
    "    with out_model_load:\n",
    "        \n",
    "        load_test_name = load_device.source\n",
    "        load_device_name = load_device.value\n",
    "        load_prediction_name = load_result_text.value\n",
    "        \n",
    "        clear_output()\n",
    "        # Predict based on choices\n",
    "        records.predict_channels(load_test_name, load_device_name, loaded_model, loaded_features, loaded_params, \n",
    "                         load_type_drop.value, loaded_model_name, load_result_text.value, plot_result.value, load_min_date.value, load_max_date.value, \n",
    "                         clean_na = True, clean_na_method = 'fill', target_raster = '1Min')\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Import Local Models</h4>'))\n",
    "\n",
    "# Test dropdown\n",
    "load_test = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=widgets.Layout(width='400px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "load_test_drop = widgets.interactive(load_show_devices, \n",
    "                                Source=load_test, \n",
    "                                layout=widgets.Layout(width='600px'))\n",
    "\n",
    "load_type_drop = widgets.Dropdown(options = ['LSTM', 'RF', 'OLS', 'SVR'],\n",
    "                                  value = 'LSTM',\n",
    "                                  description = 'Model Type',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "load_target_drop = widgets.Dropdown(options = ['ALPHASENSE', 'MICS', 'PMS'],\n",
    "                                  value = 'MICS',\n",
    "                                  description = 'Model Target',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "load_model_type_drop = widgets.interactive(load_show_models, \n",
    "                                target = load_target_drop,\n",
    "                                mtype = load_type_drop, \n",
    "                                layout = widgets.Layout(width='700px'))\n",
    "\n",
    "load_models = widgets.SelectMultiple(selected_labels = selectedModels, \n",
    "                           layout = widgets.Layout(width='700px'))\n",
    "\n",
    "load_models_interact = widgets.interactive(load_selectModels,\n",
    "                                     selected_model = load_models,\n",
    "                                     model_source= load_test_dd,\n",
    "                                     layout = widgets.Layout(width='700px'))\n",
    "\n",
    "load_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "load_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Test dropdown\n",
    "load_test_dd = widgets.Dropdown(options=[k for k in records.readings.keys()], \n",
    "                        layout=widgets.Layout(width='400px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "load_test_drop = widgets.interactive(load_show_devices, \n",
    "                                Source=load_test_dd, \n",
    "                                layout=widgets.Layout(width='400px'))\n",
    "\n",
    "# Device dropdown\n",
    "load_device = widgets.Dropdown(layout=widgets.Layout(width='200px'),\n",
    "                        description = 'Device')\n",
    "\n",
    "# Sensor dropdown\n",
    "load_result_text = widgets.Text(layout = widgets.Layout(width='300px'),\n",
    "                               description = 'Result name')\n",
    "\n",
    "load_calculateButton = widgets.Button(description='Predict channel')\n",
    "load_calculateButton.on_click(load_calculateModel)\n",
    "load_device_box = widgets.HBox([load_test_drop, load_device])\n",
    "plot_result = widgets.Checkbox(value=True, \n",
    "                                     description='Plot Result', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "calculate_channel_box = widgets.HBox([load_result_text, load_calculateButton, plot_result])\n",
    "display(load_model_type_drop)\n",
    "display(load_models)\n",
    "\n",
    "load_B = widgets.Button(description='Load Model')\n",
    "load_B.on_click(loadModel)\n",
    "\n",
    "buttonBox = widgets.VBox([load_B, load_device_box, load_min_date, load_max_date, calculate_channel_box])\n",
    "display(buttonBox)\n",
    "display(out_model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "48px",
    "left": "552px",
    "top": "705.497px",
    "width": "315px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
