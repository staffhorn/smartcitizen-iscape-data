{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Initialisation\n",
    "\n",
    "Run this cell to initialise the neccesary variables in your notebook. This will also allow some extensions and permit the autoreload function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     26
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded updated sensor names and dumped into /Users/macoscar/Documents/04_Projects/02_FabLab/01_SmartCitizen/01_Repositories/DataAnalysis/smartcitizen-iscape-data/notebooks/sensorData/sensorNames.sav\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "        code_show=true; \n",
       "        function code_toggle() {\n",
       "            if (code_show){\n",
       "                $('div.input').show();\n",
       "            } else {\n",
       "                $('div.input').hide();\n",
       "            }\n",
       "            code_show = !code_show\n",
       "        } \n",
       "        $( document ).ready(code_toggle);\n",
       "        </script>\n",
       "        \n",
       "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook initialisation -> OK\n"
     ]
    }
   ],
   "source": [
    "from os import getcwd, pardir\n",
    "from os.path import join, abspath, normpath, basename\n",
    "# Create recordings\n",
    "from recording_class import recordings\n",
    "\n",
    "try:\n",
    "    # init_notebook_mode(connected=True)\n",
    "    \n",
    "    ## Get Root Directory\n",
    "    rootDirectory = abspath(join(abspath(join(getcwd(), pardir)), pardir))\n",
    "    dataDirectory = join(rootDirectory, 'smartcitizen-iscape-data')\n",
    "    \n",
    "    # `do not disturb` mode\n",
    "    import warnings                                  \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    ## Create a button that hides cells\n",
    "    from IPython.display import HTML, display, clear_output, Markdown\n",
    "    from ipywidgets import interact\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    display(HTML(\n",
    "        '''\n",
    "        <script>\n",
    "        code_show=true; \n",
    "        function code_toggle() {\n",
    "            if (code_show){\n",
    "                $('div.input').show();\n",
    "            } else {\n",
    "                $('div.input').hide();\n",
    "            }\n",
    "            code_show = !code_show\n",
    "        } \n",
    "        $( document ).ready(code_toggle);\n",
    "        </script>\n",
    "        \n",
    "    The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "    To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.'''))\n",
    "    \n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    records = recordings()\n",
    "\n",
    "    print ('Notebook initialisation -> OK')\n",
    "    \n",
    "except:\n",
    "    print ('Failed Initialisation')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import/Export\n",
    "\n",
    "## Import Local Test\n",
    "\n",
    "Import test from local test database or smartcitizen API:\n",
    "\n",
    "- Load all the kits within the test\n",
    "- Check if there were alphasense sensors and retrieve their calibration data and order of slots\n",
    "- Check if there was a reference and convert it's units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a6cadd66c24671b23abbf3adf053f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Import CSV Tests</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98152f52ffc418c93a873aaeb238ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='x', layout=Layout(width='700px'), options={'2017-06_INT_FIRM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c8a77f1a7c4611a4a83035aa22e0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='1', description='Target Raster', layout=Layout(width='300px')), Drop…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8df6cf61684b2a877cd93767ca2253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Import API Tests</h3>'), VBox(children=(Text(value='', description='Kit List'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b39e49624384811bff6d51d6a4126a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<hr>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f2756d600a454c9cfbcb426474a7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Load', style=ButtonStyle()), Button(description='Clear All', style=ButtonSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b1be9ef60840d786c7ee36dc094d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from test_utils import getTests\n",
    "from api_utils import getKitID\n",
    "import re\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "selectedTest = tuple()\n",
    "def selectTests(x):\n",
    "    global selectedTest\n",
    "    selectedTest = list(x)\n",
    "    selectedTestBases = list()\n",
    "    selectedTestBases.append('')\n",
    "    for test in selectedTest:\n",
    "        selectedTestBases.append(basename(normpath(test)))\n",
    "    name_drop_api.options = selectedTestBases\n",
    "\n",
    "def loadButtonCSV(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        target_raster_csv = raster_text_csv.value + raster_drop_csv.value\n",
    "        if na_drop_csv.value != 'None':\n",
    "            na_drop_action = na_drop_csv.value\n",
    "            na_dropage = True\n",
    "        else:\n",
    "            na_dropage = False\n",
    "            na_drop_action = ''\n",
    "        for testCSV in selectedTest:\n",
    "            testName = basename(normpath(testCSV))\n",
    "            records.add_recording_CSV(testName, testCSV, target_raster_csv, na_dropage, na_drop_action)\n",
    "        \n",
    "def loadButtonAPI(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        target_raster_api = raster_text_api.value + raster_drop_api.value\n",
    "        devices = kitList_api.value.strip('').split(',')\n",
    "        devicesCorrected = list()\n",
    "        for device in devices: \n",
    "            device = re.sub(' ', '', device)\n",
    "            devicesCorrected.append(device)\n",
    "        testName = testName_api.value\n",
    "        if testName != '':\n",
    "            if na_drop_api.value != 'None':\n",
    "                na_drop_action = na_drop_api.value\n",
    "                na_dropage = True\n",
    "            else:\n",
    "                na_dropage = False\n",
    "                na_drop_action = ''\n",
    "            records.add_recording_API(testName, devicesCorrected, start_date_widget_api.value, end_date_widget_api.value, \\\n",
    "                                      target_raster_api)\n",
    "        else:\n",
    "            print ('Input test name')\n",
    "            \n",
    "def loadButton(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        target_raster_csv = raster_text_csv.value + raster_drop_csv.value\n",
    "        if na_drop_csv.value != 'None':\n",
    "            na_drop_action = na_drop_csv.value\n",
    "            na_dropage = True\n",
    "        else:\n",
    "            na_dropage = False\n",
    "            na_drop_action = ''\n",
    "        for testCSV in selectedTest:\n",
    "            testName = basename(normpath(testCSV))\n",
    "            records.add_recording_CSV(testName, testCSV, target_raster_csv, na_dropage, na_drop_action)\n",
    "        \n",
    "        \n",
    "        if kitList_api.value != '':\n",
    "            target_raster_api = raster_text_api.value + raster_drop_api.value\n",
    "            devices = kitList_api.value.strip('').split(',')\n",
    "            devicesCorrected = list()\n",
    "            \n",
    "            for device in devices: \n",
    "                device = re.sub(' ', '', device)\n",
    "                devicesCorrected.append(device)\n",
    "            \n",
    "            if name_drop_api.value == '':\n",
    "                testName = testName_api.value\n",
    "                print (testName)\n",
    "\n",
    "            else:\n",
    "                testName = name_drop_api.value\n",
    "            \n",
    "            if testName != '':\n",
    "                if na_drop_api.value != 'None':\n",
    "                    na_drop_action = na_drop_api.value\n",
    "                    na_dropage = True\n",
    "                else:\n",
    "                    na_dropage = False\n",
    "                    na_drop_action = ''\n",
    "                records.add_recording_API(testName, devicesCorrected, start_date_widget_api.value, end_date_widget_api.value, \\\n",
    "                                          target_raster_api)\n",
    "\n",
    "\n",
    "def getKitIDInt(b):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        devices = kitList_api.value.strip('').split(',')\n",
    "        kitIDs = list()\n",
    "        for device in devices:\n",
    "            kitID = getKitID(device, False)\n",
    "            if kitID != 'None':\n",
    "                kitIDs.append(kitID)\n",
    "                print ('Device {} has kit ID {}'.format(device, kitID))\n",
    "            else:\n",
    "                print ('Device {} does not exist'.format(device))\n",
    "    \n",
    "def clearButton(b):\n",
    "    with out:\n",
    "        records.clear_recordings()\n",
    "        clear_output()\n",
    "\n",
    "## API\n",
    "banner_api = widgets.HTML('<h3>Import API Tests</h3>')\n",
    "kitList_api = widgets.Text(description = 'Kit List')\n",
    "testName_api = widgets.Text(description = 'Test Name')\n",
    "\n",
    "name_drop_api = widgets.Dropdown(options = selectedTest,\n",
    "                                  description = 'Merge with CSV',\n",
    "                                  layout = widgets.Layout(width='400px'))\n",
    "\n",
    "getKitIDb_api = widgets.Button(description='Get Kit ID')\n",
    "getKitIDb_api.on_click(getKitIDInt)\n",
    "\n",
    "loadB_api = widgets.Button(description='Load API Kit')\n",
    "loadB_api.on_click(loadButtonAPI)\n",
    "\n",
    "raster_text_api = widgets.Text(description = 'Target Raster',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "\n",
    "raster_drop_api = widgets.Dropdown(options = ['H', 'Min', 'S'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "na_drop_api = widgets.Dropdown(options = ['None', 'fill', 'drop'],\n",
    "                                  value = 'fill',\n",
    "                                  description = 'Process na',\n",
    "                                  layout = widgets.Layout(width='200px'))\n",
    "\n",
    "start_date_widget_api = widgets.DatePicker(description='Start Date')\n",
    "end_date_widget_api = widgets.DatePicker(description='End Date')\n",
    "dateBox_api = widgets.VBox([start_date_widget_api, end_date_widget_api])\n",
    "\n",
    "raster_box_api = widgets.HBox([raster_text_api, raster_drop_api, na_drop_api])\n",
    "namebox_api = widgets.HBox([testName_api, name_drop_api])\n",
    "Hbox_api = widgets.VBox([kitList_api, namebox_api])\n",
    "# ButtonBox_api = widgets.HBox([getKitIDb_api, loadB_api])\n",
    "apiBox = widgets.VBox([banner_api, Hbox_api, raster_box_api, dateBox_api])#, ButtonBox_api]) \n",
    "        \n",
    "## CSV\n",
    "display(widgets.HTML('<h3>Import CSV Tests</h3>'))\n",
    "\n",
    "tests = getTests(dataDirectory)\n",
    "interact(selectTests,\n",
    "         x = widgets.SelectMultiple(options=tests, \n",
    "                           selected_labels = selectedTest, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "# loadB_csv = widgets.Button(description='Load Local Tests')\n",
    "# loadB_csv.on_click(loadButtonCSV)\n",
    "\n",
    "raster_text_csv = widgets.Text(description = 'Target Raster',\n",
    "                              value = '1',\n",
    "                              layout = widgets.Layout(width='300px'))\n",
    "\n",
    "raster_drop_csv = widgets.Dropdown(options = ['H', 'Min', 'S', 'ms'],\n",
    "                                  value = 'Min',\n",
    "                                  description = '',\n",
    "                                  layout = widgets.Layout(width='100px'))\n",
    "\n",
    "na_drop_csv = widgets.Dropdown(options = ['None', 'fill', 'drop'],\n",
    "                                  value = 'fill',\n",
    "                                  description = 'Process na',\n",
    "                                  layout = widgets.Layout(width='200px'))\n",
    "\n",
    "raster_box_csv = widgets.HBox([raster_text_csv, raster_drop_csv, na_drop_csv])\n",
    "\n",
    "# buttonBox_csv = widgets.HBox([loadB_csv, resetB_csv])\n",
    "csvBox = widgets.VBox([raster_box_csv])#, buttonBox_csv])\n",
    "\n",
    "loadB = widgets.Button(description='Load')\n",
    "loadB.on_click(loadButton)\n",
    "\n",
    "resetB = widgets.Button(description='Clear All')\n",
    "resetB.on_click(clearButton)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB, resetB])\n",
    "# Display everything\n",
    "display(csvBox)\n",
    "display(apiBox)\n",
    "display(widgets.HTML('<hr>'))\n",
    "display(buttonBox)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe cleaning and preparation\n",
    "\n",
    "Inspired by the code of Dmitriy Sergeev at https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-302da1fdfbfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeseries_train_test_split\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0mplotModelResults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepareDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2018-12_INT_CCS811'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/04_Projects/02_FabLab/01_SmartCitizen/01_Repositories/DataAnalysis/smartcitizen-iscape-data/notebooks/signal_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from signal_utils import mean_absolute_percentage_error, timeseries_train_test_split, \\\n",
    "                         plotModelResults, prepareDataFrame\n",
    "\n",
    "test_name = '2018-12_INT_CCS811'\n",
    "device_name = 'CCS811_OUT'\n",
    "\n",
    "## Ignore columns\n",
    "irrelevantColumns = ['BATT', 'BATT_CHG_RATE', 'LIGHT', 'CO_MICS_THEAT', 'NO2_MICS_THEAT']\n",
    "frequency = '1Min'\n",
    "\n",
    "# Resample data\n",
    "data = prepareDataFrame(records.readings[test_name]['devices'][device_name]['data'], \n",
    "                 frequency, irrelevantColumns, _plotModelAnom = True, \n",
    "                 _scaleAnom = 1.9, _methodAnom = 'before-after-avg')\n",
    "\n",
    "# Make a copy to a 'CLEAN' keyword\n",
    "records.readings[test_name]['devices'][device_name + '_CLEAN'] = dict()\n",
    "\n",
    "# Put everything except data inside\n",
    "for key in records.readings[test_name]['devices'][device_name].keys():\n",
    "    if 'data' not in key:\n",
    "        records.readings[test_name]['devices'][device_name + '_CLEAN'][key] = readings[test_name]['devices'][device_name][key]\n",
    "\n",
    "# Put data inside\n",
    "records.readings[test_name]['devices'][device_name + '_CLEAN']['data'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters\n",
    "\n",
    "Not recommended though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holt_winters import *\n",
    "\n",
    "test_name = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "device_name = 'STATION_CASE'\n",
    "channel = 'HUM'\n",
    "data = readings[test_name]['devices'][device_name]['data'].loc[:,channel].copy()\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "display(data.head(4))\n",
    "display(data.describe())\n",
    "\n",
    "slen = seasonality_day(test_name)\n",
    "\n",
    "# initializing model parameters alpha, beta and gamma\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Minimizing the loss function \n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_absolute_error, 24), \n",
    "               method=\"TNC\", bounds = ((0, 1), (0, 1), (0, 1)))\n",
    "\n",
    "# ...and train the model with them\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "\n",
    "# alpha_final, beta_final, gamma_final = (0.005890895737085178, 0.004673709964305939, 0.01299124317761835)\n",
    "print 'alpha, beta, gamma'\n",
    "print(alpha_final, beta_final, gamma_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = HoltWinters(data, slen = slen, \n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 0, scaling_factor = 5)\n",
    "\n",
    "_model.triple_exponential_smoothing()\n",
    "\n",
    "plotHoltWinters(data, _model, plot_intervals=True, plot_anomalies=True)\n",
    "\n",
    "dataframe = pd.DataFrame(data)\n",
    "column_data = dataframe.columns\n",
    "dataframe['Anomalies'] = False\n",
    "dataframe['Target'] = _model.series\n",
    "dataframe['model'] = _model.result\n",
    "dataframe['UpperBond'] = _model.UpperBond\n",
    "dataframe['LowerBond'] = _model.LowerBond\n",
    "\n",
    "def tag_anomalies(row):\n",
    "    if (row['UpperBond'] < row ['Target']) or (row['LowerBond'] > row ['Target']):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "dataframe['Anomalies'] = dataframe.apply(lambda row: tag_anomalies (row), axis = 1)\n",
    "\n",
    "readings[test_name]['devices'][device_name]['data']['Anomalies'] = dataframe.Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CSV and API test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     7,
     12
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_A_devices(Source):\n",
    "    B_test.options = [k for k in readings.keys() if k != A_test.value]\n",
    "    A_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    A_device.source = Source\n",
    "\n",
    "def show_B_devices(Source):\n",
    "    A_test.options = [k for k in readings.keys() if k != B_test.value]\n",
    "    B_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    B_device.source = Source\n",
    "\n",
    "def mergeCSVAPI(b):\n",
    "    clear_output()\n",
    "    try: \n",
    "        A_data = readings[A_test.value]['devices'][A_device.value]['data']\n",
    "        B_data = readings[B_test.value]['devices'][B_device.value]['data']\n",
    "    \n",
    "        data = A_data.combine_first(B_data)\n",
    "        # display(data)\n",
    "        if testName.value not in readings: \n",
    "            readings[testName.value] = dict()\n",
    "            readings[testName.value]['devices'] = dict()\n",
    "        if deviceName.value not in readings[testName.value]['devices']: readings[testName.value]['devices'][deviceName.value] = dict()\n",
    "        readings[testName.value]['devices'][deviceName.value]['data'] = data\n",
    "    except:\n",
    "        print 'Data could not be combined, review data'\n",
    "    else:\n",
    "        print 'Data Combined'\n",
    "\n",
    "    if 'alphasense' in readings[A_test.value]['devices'][A_device.value]:\n",
    "        if 'alphasense' in readings[B_test.value]['devices'][B_device.value]:\n",
    "            if readings[A_test.value]['devices'][A_device.value]['alphasense'] == readings[B_test.value]['devices'][B_device.value]:\n",
    "                print 'Both devices have matching alphasense data, using any of them'\n",
    "                readings[testName.value]['devices'][deviceName.value]['alphasense'] = dict()\n",
    "                readings[testName.value]['devices'][deviceName.value]['alphasense'] = readings[A_test.value]['devices'][A_device.value]['alphasense']\n",
    "            else:\n",
    "                print 'Alphasense data differs between both devices. Do it manually'\n",
    "    elif 'alphasense' in readings[B_test.value]['devices'][B_device.value]:\n",
    "        print 'A device does not have alphasense data, using B'\n",
    "        readings[testName.value]['devices'][deviceName.value]['alphasense'] = readings[B_test.value]['devices'][B_device.value]['alphasense']\n",
    "    else:\n",
    "        'No alphasense information found. If any, input manually'\n",
    "    \n",
    "# Test dropdown\n",
    "A_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout = widgets.Layout(width='500px'),\n",
    "                        description = 'Test A')\n",
    "\n",
    "A_test_drop = widgets.interactive(show_A_devices, \n",
    "                                Source=A_test, \n",
    "                                layout = widgets.Layout(width='500px'))\n",
    "\n",
    "# Test dropdown\n",
    "B_test = widgets.Dropdown(options=[k for k in readings.keys() if k != A_test.value], \n",
    "                        layout = widgets.Layout(width='500px'),\n",
    "                        description = 'Test B')\n",
    "\n",
    "B_test_drop = widgets.interactive(show_B_devices, \n",
    "                                Source=B_test, \n",
    "                                layout = widgets.Layout(width='500px'))\n",
    "\n",
    "# Device dropdown\n",
    "A_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device A')\n",
    "\n",
    "# Device dropdown\n",
    "B_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device B')\n",
    "\n",
    "# New Test and device Name\n",
    "testName = widgets.Text(description = 'New Test Name')\n",
    "deviceName = widgets.Text(description = 'New Device Name ')\n",
    "createTest = widgets.Button(description='Combine Tests')\n",
    "createTest.on_click(mergeCSVAPI)\n",
    "\n",
    "test_box = widgets.HBox([A_test_drop, B_test_drop], layout = widgets.Layout(width='500px'))\n",
    "device_box = widgets.HBox([A_device, B_device], layout=widgets.Layout(justify_content='space-between'))\n",
    "name_box = widgets.HBox([testName, deviceName, createTest])\n",
    "root_box = widgets.VBox([test_box, device_box, name_box])\n",
    "display(root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     8,
     12,
     17
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "import os\n",
    "global selected\n",
    "selected = []\n",
    "\n",
    "def selectedFilesChannels(x):\n",
    "    selected = list(x)\n",
    "    \n",
    "selected_export=tuple()\n",
    "def selectedDevices_export(Source):\n",
    "    global selected_export\n",
    "    selected_export = list(Source)\n",
    "    \n",
    "def show_device_export(Source):\n",
    "    _devices_select_export.options = [s for s in list(readings[_test_export.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "def exportFile(b):\n",
    "    for i in range(len(selected_export)):\n",
    "        b.f = selected_export[i]\n",
    "        exportDir = exportPath.value\n",
    "        if not os.path.exists(exportDir): os.mkdir(exportDir)\n",
    "        savePath = os.path.join(exportDir, b.f)\n",
    "        if not os.path.exists(savePath):\n",
    "            readings[_test_export.value]['devices'][b.f]['data'].to_csv(savePath + '.csv', sep=\",\")\n",
    "            display(FileLink(savePath))\n",
    "        else:\n",
    "            display(widgets.HTML('File Already exists!'))\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test_export = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_export_drop = widgets.interactive(show_device_export, \n",
    "                                Source=_test_export, \n",
    "                                layout=layout)\n",
    "\n",
    "_devices_select_export = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_export_drop = interact(selectedDevices_export,\n",
    "                                 Source = _devices_select_export)\n",
    "\n",
    "display(widgets.HTML('<h3>Export Files</h3>'))\n",
    "exportPath = widgets.Text(description = 'Type in export path  ', layout=widgets.Layout(width='700px'))\n",
    "eb = widgets.Button(description='Export file', layout=widgets.Layout(width='150px'))\n",
    "eb.on_click(exportFile)\n",
    "\n",
    "selectBox = widgets.VBox([_test_export_drop, _devices_select_export])\n",
    "exportBox = widgets.HBox([exportPath,eb])\n",
    "_BOX=widgets.VBox([selectBox, exportBox])\n",
    "display(_BOX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculator\n",
    "Input your formulas into this cell for analysis in the plots below\n",
    "\n",
    "There are formulas for calculating:\n",
    "- *MICS* = Poly(R, H, T) - **MICS_FORMULA**\n",
    "- *Alphasense's correction proposal* = f(Curr, Sens, Zero) - **AD_FORMULA**\n",
    "- *Smoothing* = f(Signal, Window) - **SMOOTH**\n",
    "- *Absolute humidity* = f(Temperature, Humidity, Pressure) - **ABS_HUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     28,
     40,
     55
    ]
   },
   "outputs": [],
   "source": [
    "from formula_utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def show_device_list(Source):\n",
    "    _devices_select.options = [s for s in list(readings[_test.value]['devices'].keys())]\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def commonChannels(selected):\n",
    "    global commonChannelsList\n",
    "    commonChannelsList = []\n",
    "    if (len(selected) == 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "    if (len(selected) > 1):\n",
    "        commonChannelsList = readings[_test.value]['devices'][selected[0]]['data'].columns\n",
    "        for s in list(selected):\n",
    "            commonChannelsList = list(set(commonChannelsList) & set(readings[_test.value]['devices'][s]['data'].columns))\n",
    "    _Aterm.options = list(commonChannelsList)\n",
    "    _Aterm.source = selected\n",
    "    _Bterm.options = list(commonChannelsList)\n",
    "    _Bterm.source = selected\n",
    "    _Cterm.options = list(commonChannelsList)\n",
    "    _Cterm.source = selected\n",
    "    _Dterm.options = list(commonChannelsList)\n",
    "    _Dterm.source = selected\n",
    "    \n",
    "def calculateFormula(b):\n",
    "    clear_output()\n",
    "    A = _Aterm.value\n",
    "    B = _Bterm.value\n",
    "    C = _Cterm.value\n",
    "    D = _Dterm.value\n",
    "    Name = _formulaName.value\n",
    "    for s in list(selected):\n",
    "        result = functionFormula(s,A,B,C,D,readings)\n",
    "        readings[_test.value]['devices'][s]['data'][Name] = result\n",
    "    print \"Formula {} Added in test {}\".format(Name, _test.value)\n",
    "    \n",
    "def functionFormula(s, Aname, Bname, Cname, Dname, _readings): \n",
    "    calcData = pd.DataFrame()\n",
    "    mergeData = pd.merge(pd.merge(pd.merge(_readings[_test.value]['devices'][s]['data'].loc[:,(Aname,)],_readings[_test.value]['devices'][s]['data'].loc[:,(Bname,)],left_index=True, right_index=True), _readings[_test.value]['devices'][s]['data'].loc[:,(Cname,)], left_index=True, right_index=True),_readings[_test.value]['devices'][s]['data'].loc[:,(Dname,)],left_index=True, right_index=True)\n",
    "    calcData[Aname] = mergeData.iloc[:,0] #A\n",
    "    calcData[Bname] = mergeData.iloc[:,1] #B\n",
    "    calcData[Cname] = mergeData.iloc[:,2] #C\n",
    "    calcData[Dname] = mergeData.iloc[:,3] #D\n",
    "    A = calcData[Aname]\n",
    "    B = calcData[Bname]\n",
    "    C = calcData[Cname]\n",
    "    D = calcData[Dname]\n",
    "    result = eval(_formula.value)\n",
    "    return result\n",
    "        \n",
    "selected=tuple()\n",
    "def selectedDevices(Source):\n",
    "    global selected\n",
    "    selected = list(Source)\n",
    "    commonChannels(selected)\n",
    "\n",
    "# Test dropdown\n",
    "layout = widgets.Layout(width='400px')\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_device_list, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "_Aterm = widgets.Dropdown(description = 'A', layout=layout)\n",
    "_Bterm = widgets.Dropdown(description = 'B', layout=layout)\n",
    "_Cterm = widgets.Dropdown(description = 'C', layout=layout)\n",
    "_Dterm = widgets.Dropdown(description = 'D', layout=layout)\n",
    "\n",
    "_devices_select = widgets.SelectMultiple(layout=widgets.Layout(width='700px'))\n",
    "_devices_select_drop = interact(selectedDevices,\n",
    "                                 Source = _devices_select)\n",
    "\n",
    "_selectBox = widgets.VBox([_test_drop, _devices_select])\n",
    "\n",
    "_formulaName = widgets.Text(description = 'Name: ')\n",
    "_formula = widgets.Text(description = '=')\n",
    "_ABtermsBox = widgets.HBox([_Aterm, _Bterm])\n",
    "_CDtermsBox = widgets.HBox([_Cterm, _Dterm])\n",
    "_termsBox = widgets.VBox([_selectBox, _ABtermsBox, _CDtermsBox])\n",
    "_calculate = widgets.Button(description='Calculate')\n",
    "_calculateBox = widgets.HBox([_formulaName,_formula, _calculate])\n",
    "_calculate.on_click(calculateFormula)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Select the Files for your formulas to apply</h4>'))\n",
    "display(_termsBox)\n",
    "display(widgets.HTML('<h4>Input your formula Below</h4>'))\n",
    "display(_calculateBox)\n",
    "\n",
    "## Vapour equilibrium: B is temperature in degC, assumed 1013mbar\n",
    "# (1.0007 + 3.46*1e-6*1013)*6.1121*np.exp(17.502*B/(240.97+B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     28,
     34,
     40,
     107,
     166,
     201,
     209,
     214,
     221,
     223
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, FileLink, FileLinks, clear_output, HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "# --\n",
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Plot Y limits\n",
    "setLimits = False\n",
    "maxY = 15000\n",
    "minY = 0\n",
    "\n",
    "toshow = []\n",
    "axisshow = []\n",
    "# meanTable = []\n",
    "\n",
    "def show_devices(Source):\n",
    "    _device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    _device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "\n",
    "def show_sensors(Source):\n",
    "    _sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    _sensor_drop.source = Source\n",
    "    _min_date.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    _max_date.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "\n",
    "def clear_all(b):\n",
    "    clear_output()\n",
    "    del toshow[:]\n",
    "    del axisshow[:]\n",
    "\n",
    "def add_sensor(b):\n",
    "    clear_output()\n",
    "    d = [_device.source, _sensor_drop.source, _sensor_drop.value]\n",
    "    \n",
    "    if d not in toshow: \n",
    "        toshow.append(d)\n",
    "        axisshow.append(_axis_drop.value)\n",
    "        \n",
    "    plot_data = readings[toshow[0][0]]['devices'][toshow[0][1]]['data'].loc[:,(toshow[0][2],)]\n",
    "    list_data_primary = []\n",
    "    list_data_secondary = []\n",
    "    list_data_terciary = []\n",
    "    \n",
    "    if b.slice_time:\n",
    "        plot_data = plot_data[plot_data.index > _min_date.value]\n",
    "        plot_data = plot_data[plot_data.index < _max_date.value]\n",
    "    \n",
    "    if len(toshow) > 1:\n",
    "        for i in range(1, len(toshow)):\n",
    "            plot_data = pd.merge(plot_data, readings[toshow[i][0]]['devices'][toshow[i][1]]['data'].loc[:,(toshow[i][2],)], left_index=True, right_index=True)\n",
    "    # plot_data = plot_data.groupby(pd.Grouper(freq='10Min')).aggregate(np.mean)    \n",
    "\n",
    "    print ('-------------------------------------')\n",
    "    print (' Medias:\\n')\n",
    "    meanTable = []\n",
    "    for d in toshow:\n",
    "        myMean = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        meanTable.append(myMean)   \n",
    "    res = plot_data.mean()\n",
    "    for i in range(len(meanTable)): print (meanTable[i] + '%.2f' % (res[i]))\n",
    "    print ('-------------------------------------')\n",
    "    \n",
    "    print ('-------------------------------------')\n",
    "    print (' Std Deviation:\\n')\n",
    "    stdTable = []\n",
    "    for d in toshow:\n",
    "        myStd = ' ' + d[1]  + \"\\t\" + d[2] + \"\\t\"\n",
    "        stdTable.append(myStd)   \n",
    "    std = plot_data.std()\n",
    "    for i in range(len(stdTable)): print stdTable[i] + '%.2f' % (std[i])\n",
    "    print ('-------------------------------------')\n",
    "\n",
    "    # Change columns naming\n",
    "    changed = []\n",
    "    for i in range(len(plot_data.columns)):\n",
    "        changed.append(toshow[i][0] + ' - '+ toshow[i][1] + ' - '+ plot_data.columns[i])\n",
    "    plot_data.columns = changed\n",
    "    \n",
    "    subplot_rows = 0\n",
    "    if len(toshow) > 0:\n",
    "        for i in range(len(toshow)):\n",
    "            if axisshow[i]=='1': \n",
    "                list_data_primary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,1)\n",
    "            if axisshow[i]=='2': \n",
    "                list_data_secondary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,2)\n",
    "            if axisshow[i]=='3': \n",
    "                list_data_terciary.append(str(changed[i]))\n",
    "                subplot_rows = max(subplot_rows,3)\n",
    "          \n",
    "    \n",
    "    if _matplotly.value == 'Plotly':\n",
    "        fig1 = tls.make_subplots(rows=subplot_rows, cols=1, shared_xaxes=_synchroniseXaxis.value)\n",
    "    \n",
    "        #if len(list_data_primary)>0:\n",
    "            #fig1 = plot_data.iplot(kind='scatter', y = list_data_primary, asFigure=True, layout = layout)\n",
    "        #ply.offline.iplot(fig1)\n",
    "        \n",
    "        for i in range(len(list_data_primary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_primary[i]], 'type': 'scatter', 'name': list_data_primary[i]}, 1, 1)\n",
    "    \n",
    "        for i in range(len(list_data_secondary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_secondary[i]], 'type': 'scatter', 'name': list_data_secondary[i]}, 2, 1)\n",
    "        \n",
    "        for i in range(len(list_data_terciary)):\n",
    "            fig1.append_trace({'x': plot_data.index, 'y': plot_data[list_data_terciary[i]], 'type': 'scatter', 'name': list_data_terciary[i]}, 3, 1)\n",
    "    \n",
    "        if setLimits: \n",
    "            fig1['layout'].update(height = 800,\n",
    "                                legend=dict(x=-.1, y=1.2) ,\n",
    "                               xaxis=dict(title='Time'))\n",
    "                              \n",
    "        else:\n",
    "            fig1['layout'].update(height = 800,\n",
    "                                  legend=dict(x=-.1, y=1.2) ,\n",
    "                               xaxis=dict(title='Time'))\n",
    "                               \n",
    "        ply.offline.plot(fig1)\n",
    "        \n",
    "    elif _matplotly.value == 'Matplotlib':\n",
    "        \n",
    "        fig, axes = plot.subplots(subplot_rows, 1, figsize=(15,10))\n",
    "        # Four axes, returned as a 2-d array\n",
    "        \n",
    "        if subplot_rows == 1:\n",
    "            for i in range(len(list_data_primary)):\n",
    "                axes.plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                axes.legend(loc='best')\n",
    "\n",
    "        else:\n",
    "            for i in range(len(list_data_primary)):\n",
    "                axes[0].plot(plot_data.index, plot_data[list_data_primary[i]], label =  list_data_primary[i])\n",
    "                axes[0].legend(loc='best')\n",
    "                axes[0].grid(visible = True)\n",
    "\n",
    "            for i in range(len(list_data_secondary)):\n",
    "                axes[1].plot(plot_data.index, plot_data[list_data_secondary[i]], label =  list_data_secondary[i])\n",
    "                axes[1].legend(loc='best')\n",
    "                axes[1].grid(visible = True)\n",
    "\n",
    "            for i in range(len(list_data_terciary)):\n",
    "                axes[2].plot(plot_data.index, plot_data[list_data_terciary[i]], label =  list_data_terciary[i])\n",
    "                axes[2].legend(loc='best')\n",
    "                axes[2].grid(visible = True)\n",
    "\n",
    "        plot.xlabel('Date') \n",
    "        plot.grid(visible = True)\n",
    "        plot.show()\n",
    "        \n",
    "    \n",
    "def reset_time(b):\n",
    "    _min_date.value = readings[b.src.value].index.min()._short_repr\n",
    "    _max_date.value = readings[b.src.value].index.max()._short_repr\n",
    "\n",
    "layout=widgets.Layout(width='330px')\n",
    "\n",
    "# Test dropdown\n",
    "_test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=layout,\n",
    "                        description = 'Test')\n",
    "\n",
    "_test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout=layout)\n",
    "\n",
    "# Device dropdown\n",
    "_device = widgets.Dropdown(layout=layout,\n",
    "                        description = 'Device')\n",
    "\n",
    "_device_drop = widgets.interactive(show_sensors, \n",
    "                                Source=_device, \n",
    "                                layout=layout)\n",
    "\n",
    "# Sensor dropdown\n",
    "_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "\n",
    "# Buttons\n",
    "_b_add = widgets.Button(description='Add to Plot', layout=widgets.Layout(width='120px'))\n",
    "_b_add.on_click(add_sensor)\n",
    "_b_add.slice_time = False\n",
    "_b_reset_all = widgets.Button(description='Clear all', layout=widgets.Layout(width='120px'))\n",
    "_b_reset_all.on_click(clear_all)\n",
    "\n",
    "# Axis dropdown\n",
    "_axis_drop = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='1',\n",
    "    description='Subplot:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Synchronise Checkbox\n",
    "_synchroniseXaxis = widgets.Checkbox(value=False, \n",
    "                                     description='Synchronise X axis', \n",
    "                                     disabled=False, \n",
    "                                     layout=widgets.Layout(width='300px'))\n",
    "\n",
    "_matplotly = widgets.RadioButtons(\n",
    "    options=['Matplotlib', 'Plotly'], value='Matplotlib',\n",
    "    description='Plot Type',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Date fields\n",
    "_min_date = widgets.Text(description='Start date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "_max_date = widgets.Text(description='End date:', \n",
    "                         layout=widgets.Layout(width='330px'))\n",
    "\n",
    "# Date buttons\n",
    "_b_apply_time = _b_reset = widgets.Button(description='Apply dates', layout=widgets.Layout(width='100px'))\n",
    "_b_apply_time.on_click(add_sensor)\n",
    "_b_apply_time.slice_time = True\n",
    "_b_reset_time = _b_reset = widgets.Button(description='Reset dates', layout=widgets.Layout(width='100px'))\n",
    "_b_reset_time.on_click(reset_time)\n",
    "#_b_reset_time.src = _kit\n",
    "\n",
    "\n",
    "_device_box = widgets.HBox([_test_drop, _device_drop])\n",
    "_sensor_box = widgets.HBox([_sensor_drop, _axis_drop, _synchroniseXaxis])\n",
    "_plot_type_box = widgets.VBox([_matplotly])\n",
    "\n",
    "_plot_box = widgets.HBox([_b_add , _b_reset_all])\n",
    "_time_box = widgets.HBox([_min_date,_max_date, _b_reset_time, _b_apply_time])\n",
    "_root_box = widgets.VBox([_matplotly, _time_box, _device_box, _sensor_box, _plot_box])\n",
    "display(_root_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back2Back Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     32,
     113
    ]
   },
   "outputs": [],
   "source": [
    "cropTime = False\n",
    "min_date = \"2001-01-01 00:00:01\"\n",
    "max_date = \"2001-01-01 00:00:01\"\n",
    "doubleAxis = True\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "\n",
    "def show_devices(Source):\n",
    "    A_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    A_device.source = Source\n",
    "    B_device.options = [s for s in list(readings[Source]['devices'].keys())]\n",
    "    B_device.source = Source\n",
    "    #_min_date.value = readings[Source].index.min()._short_repr\n",
    "    #_max_date.value = readings[Source].index.max()._short_repr\n",
    "    \n",
    "\n",
    "def show_sensors_A(Source):\n",
    "    A_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    A_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr\n",
    "    \n",
    "def show_sensors_B(Source):\n",
    "    B_sensor_drop.options = [s for s in list(readings[_test.value]['devices'][Source]['data'].columns)]\n",
    "    B_sensor_drop.source = Source\n",
    "    minCropDate.value = readings[_test.value]['devices'][Source]['data'].index.min()._short_repr\n",
    "    maxCropDate.value = readings[_test.value]['devices'][Source]['data'].index.max()._short_repr    \n",
    "\n",
    "    \n",
    "def redraw(b):\n",
    "    cropTime = cropTimeCheck.value\n",
    "    doubleAxis = doubleAxisCheck.value\n",
    "    min_date = minCropDate.value\n",
    "    max_date = maxCropDate.value\n",
    "    mergedData = pd.merge(readings[_test.value]['devices'][A_device.value]['data'].loc[:,(A_sensor_drop.value,)], \n",
    "                          readings[_test.value]['devices'][B_device.value]['data'].loc[:,(B_sensor_drop.value,)], \n",
    "                          left_index=True, right_index=True, suffixes=('_'+A_sensor_drop.value, '_'+B_sensor_drop.value))\n",
    "    clear_output()\n",
    "    \n",
    "    if cropTime:\n",
    "        mergedData = mergedData[mergedData.index > min_date]\n",
    "        mergedData = mergedData[mergedData.index < max_date]\n",
    "        \n",
    "    #jointplot\n",
    "    df = pd.DataFrame()\n",
    "    A = A_sensor_drop.value + '-' + A_device.value\n",
    "    B = B_sensor_drop.value + '-' + B_device.value\n",
    "    df[A] = mergedData.iloc[:,0]\n",
    "    df[B] = mergedData.iloc[:,1]\n",
    "    \n",
    "    \n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.jointplot(A, B, data=df, kind=\"reg\", color=\"b\", size=12, scatter_kws={\"s\": 80});\n",
    "    print \"data from \" + str(df.index.min()) + \" to \" + str(df.index.max())                      \n",
    "    pearsonCorr = list(df.corr('pearson')[list(df.columns)[0]])[-1]\n",
    "    rmse = sqrt(mean_squared_error(df[A].fillna(0), df[B].fillna(0)))\n",
    "    print 'Pearson correlation coefficient: ' + str(pearsonCorr)\n",
    "    print 'Coefficient of determination R²: ' + str(pearsonCorr*pearsonCorr)\n",
    "    print 'RMSE: ' + str(rmse)\n",
    "\n",
    "    if cropTime: \n",
    "        \n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "                yaxis2=dict(title=B,titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "                legend=dict(x=-.1, y=1.2), \n",
    "                xaxis=dict(range=[min_date, max_date],title='Time'), \n",
    "                yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "            \n",
    "    else:\n",
    "        if (doubleAxis):\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            yaxis2=dict(title=B, titlefont=dict(color='rgb(255,165,0)'), tickfont=dict(color='rgb(255,165,0)'), overlaying='y', side='right')\n",
    "            )\n",
    "        else:\n",
    "            layout = go.Layout(\n",
    "            legend=dict(x=-.1, y=1.2), \n",
    "            xaxis=dict(title='Time'), \n",
    "            yaxis=dict(zeroline=True, title=A, titlefont=dict(color='rgb(0,97,255)'), tickfont=dict(color='rgb(0,97,255)')),\n",
    "            )\n",
    "        \n",
    "    trace0 = go.Scatter(x=df[A].index, y=df[A], name = A,line = dict(color='rgb(0,97,255)'))\n",
    "    \n",
    "    if (doubleAxis):\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, yaxis='y2', line = dict(color='rgb(255,165,0)'))\n",
    "    else:\n",
    "        trace1 = go.Scatter(x=df[B].index,y=df[B],name=B, line = dict(color='rgb(255,165,0)'))\n",
    "    data = [trace0, trace1]\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    ply.offline.iplot(figure)\n",
    "    \n",
    "if len(readings) < 1: print (\"Please load some data first...\")\n",
    "else:\n",
    "    \n",
    "    layout=widgets.Layout(width='350px')\n",
    "    b_redraw = widgets.Button(description='Draw')\n",
    "    b_redraw.on_click(redraw)\n",
    "    doubleAxisCheck = widgets.Checkbox(value=False, description='Secondary y axis', disabled=False)\n",
    "    \n",
    "    cropTimeCheck = widgets.Checkbox(value=False,description='Crop Data in X axis', disabled=False)\n",
    "    minCropDate = widgets.Text(description='Start date:', layout=layout)\n",
    "    maxCropDate = widgets.Text(description='End date:', layout=layout)\n",
    "    \n",
    "    # Test dropdown\n",
    "    _test = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout = widgets.Layout(width='500px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "    _test_drop = widgets.interactive(show_devices, \n",
    "                                Source=_test, \n",
    "                                layout = widgets.Layout(width='500px'))\n",
    "\n",
    "    # Device dropdown\n",
    "    A_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    A_device_drop = widgets.interactive(show_sensors_A, \n",
    "                                    Source=A_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    B_device = widgets.Dropdown(layout=layout,\n",
    "                            description = 'Device')\n",
    "    \n",
    "    B_device_drop = widgets.interactive(show_sensors_B, \n",
    "                                    Source=B_device, \n",
    "                                    layout=layout)\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    A_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    # Sensor dropdown\n",
    "    B_sensor_drop = widgets.Dropdown(layout=layout,\n",
    "                               description = 'Channel')\n",
    "    \n",
    "    draw_box = widgets.HBox([b_redraw, doubleAxisCheck], layout=widgets.Layout(justify_content='space-between'))\n",
    "    test_box = widgets.HBox([_test_drop], layout = widgets.Layout(width='500px'))\n",
    "    device_box = widgets.HBox([A_device, widgets.HTML('<h4><< Data source >></h4>') , B_device], layout=widgets.Layout(justify_content='space-between'))\n",
    "    sensor_box = widgets.HBox([A_sensor_drop, widgets.HTML('<h4><< Sensor selection >></h4>') , B_sensor_drop], layout=widgets.Layout(justify_content='space-between'))\n",
    "    crop_box = widgets.HBox([cropTimeCheck, minCropDate, maxCropDate], layout=widgets.Layout(justify_content='space-between'))\n",
    "    root_box = widgets.VBox([draw_box, test_box, device_box, sensor_box, crop_box])\n",
    "    \n",
    "    display(root_box)\n",
    "    \n",
    "    #redraw(b_redraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Seaborn Correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def paint(Source):\n",
    "    clear_output()\n",
    "    sns.set(font_scale=1.4)\n",
    "    g = sns.PairGrid(readings.values()[0])\n",
    "    g = g.map(plt.scatter)\n",
    "\n",
    "_kit = widgets.Dropdown(options=[k for k in readings.keys()], layout=layout)\n",
    "_kit_drop = widgets.interactive(paint, Source=_kit, layout=layout)\n",
    "display(_kit_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = 'STATION_CASE'\n",
    "test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "\n",
    "min_date = '2018-08-03 00:00:00'\n",
    "max_date = '2018-09-17 00:00:00'\n",
    "\n",
    "# Assign by time-frames\n",
    "freq_time = 2\n",
    "\n",
    "# Select labels\n",
    "list_channels = ['EXT_PM_1', 'EXT_PM_25', 'EXT_PM_10', 'TEMP']\n",
    "\n",
    "## -----------\n",
    "dataframePlot = pd.DataFrame()\n",
    "dataframePlot = readings[test]['devices'][device]['data'].copy()\n",
    "dataframePlot = dataframePlot[dataframePlot.index>min_date]\n",
    "dataframePlot = dataframePlot[dataframePlot.index<max_date]\n",
    "\n",
    "if freq_time == 6:\n",
    "    labels = ['Morning','Afternoon','Evening', 'Night']\n",
    "    len_labels = 4\n",
    "elif freq_time == 12:\n",
    "    labels = ['Morning', 'Evening']\n",
    "    len_labels = 2\n",
    "else:\n",
    "    labels = [str(i) for i in np.arange(0, 24, freq_time)]\n",
    "    len_labels = freq_time * len(labels)\n",
    "    \n",
    "vector_time = np.arange(0, 25, freq_time)\n",
    "\n",
    "dataframePlot = dataframePlot.assign(session=pd.cut(dataframePlot.index.hour,\n",
    "                                            vector_time,\n",
    "                                            labels=labels))\n",
    "# Group them by session\n",
    "df_se = dataframePlot.groupby(['session']).mean()\n",
    "df_se = df_se[list_channels]\n",
    "\n",
    "# Calculate average\n",
    "df_se_avg = df_se.mean(axis = 0)\n",
    "\n",
    "display(df_se)\n",
    "\n",
    "# Add additional columns\n",
    "append_rel = '_AVG_REL'\n",
    "\n",
    "list_all = []\n",
    "list_all.append('session')\n",
    "for column in list_channels:\n",
    "    dataframePlot[column + append_rel] = dataframePlot[column]/df_se_avg[column]\n",
    "    list_all.append(column)\n",
    "    list_all.append(column + append_rel)\n",
    "\n",
    "## Full dataframe\n",
    "dataframePlot = dataframePlot[list_all]\n",
    "dataframePlot.dropna()\n",
    "\n",
    "## Dataframe by session\n",
    "dataframePlot_avg = dataframePlot.groupby(['session']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16
    ]
   },
   "outputs": [],
   "source": [
    "# Plotly\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly.widgets import GraphWidget\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "import plotly.tools as tls\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "channel =  'EXT_PM_25'\n",
    "colorscale = [[0, '#edf8fb'], [.3, '#b3cde3'],  [.6, '#8856a7'],  [1, '#810f7c']]\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=dataframePlot[channel],\n",
    "        x=dataframePlot.index.date,\n",
    "        y=dataframePlot['session'],\n",
    "        colorscale=colorscale,\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Daily Pollutant',\n",
    "    xaxis = dict(ticks=''),\n",
    "    yaxis = dict(ticks='' , categoryarray=labels, autorange = 'reversed')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "\n",
    "channel_1 =  'EXT_PM_1' # Goes on the left side (x1)\n",
    "channel_2 =  'TEMP' # Goes on the right side (x2)\n",
    "relative = False\n",
    "\n",
    "if relative:\n",
    "    channel_1 = channel_1 + append_rel\n",
    "    channel_2 = channel_2 + append_rel\n",
    "    limits = ([0.5, 1.5], [0.5, 1.5])\n",
    "    marks = ()\n",
    "else:\n",
    "    limits = ([0, 20], [15, 40]) # x1, x2\n",
    "    marks = ([10, 'x'], [20, 'x'], [300, 'x2'], [400, 'x2'])\n",
    "\n",
    "dict_shapes = [{'type': 'line', \n",
    "                      'x0': mark[0],\n",
    "                      'y0': -1,\n",
    "                      'x1': mark[0],\n",
    "                      'y1': len_labels+1,\n",
    "                      'xref': mark[1],\n",
    "                      'line': {\n",
    "                          'color': 'rgba(5, 0, 0, 0.8)',\n",
    "                          'width': 1,\n",
    "                          'dash': 'dot'\n",
    "                      }}\n",
    "               for mark in marks]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_1],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x',\n",
    "            yaxis = 'y',\n",
    "            name = channel_1,\n",
    "            marker = dict(color = 'rgba(58, 78, 255, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 78, 255, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "trace2 = go.Bar(\n",
    "            x=dataframePlot_avg[channel_2],\n",
    "            y=labels,\n",
    "            orientation = 'h',\n",
    "            xaxis = 'x2',\n",
    "            yaxis='y',\n",
    "            name = channel_2,\n",
    "            marker = dict(color = 'rgba(58, 71, 80, 0.6)',\n",
    "                          line = dict(color = 'rgba(58, 71, 80, 1.0)', width = 2))\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(title='Daily average measurement (freq = {}h)'.format(freq_time),\n",
    "                   xaxis = dict(domain=[0, 0.5], autorange='reversed', title = channel_1), # range=(limits[0][0], limits[0][1])),\n",
    "                   xaxis2 = dict(domain=[0.5, 1], title = channel_2), # range=(limits[1][0], limits[1][1])),\n",
    "                   yaxis = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   yaxis2 = dict(range=(-1, len_labels+1), autorange='reversed'),\n",
    "                   bargap = 0)#,\n",
    "                   #shapes = dict_shapes)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "ply.offline.plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaSense Baseline Calibration\n",
    "\n",
    "These functions are used to create the alphasense pollutant correction based on Working, Auxiliary and calibration data provided by alphasense. Run the 1.1.1.1 AlphaSense Sensors calibration data cell to load in the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     60
    ]
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "%matplotlib inline\n",
    "min_delta = 1\n",
    "max_delta = 60\n",
    "ad_append = 'AD_BASE' + str(min_delta) + '-' + str(max_delta)\n",
    "deltas_co = np.arange(10,10,1)\n",
    "deltas_no2 = np.arange(min_delta,min_delta+max_delta,1)\n",
    "deltas_ox = np.arange(min_delta,min_delta+max_delta,1)\n",
    "\n",
    "selectedTestsAD = tuple()\n",
    "def selectTestAD(x):\n",
    "    global selectedTestsAD\n",
    "    selectedTestsAD = list(x)\n",
    "    \n",
    "def calculateCorrectionAD(b):\n",
    "    clear_output()\n",
    "    for testAD in selectedTestsAD:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testAD]['devices']:\n",
    "            if 'is_reference' in readings[testAD]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testAD]['devices'][reading]['data']\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in readings[testAD]['devices']:\n",
    "            if 'alphasense' in readings[testAD]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testAD]['devices'][kit]['alphasense']\n",
    "                sensorID_CO = readings[testAD]['devices'][kit]['alphasense']['CO']\n",
    "                sensorID_NO2 = readings[testAD]['devices'][kit]['alphasense']['NO2']\n",
    "                sensorID_OX = readings[testAD]['devices'][kit]['alphasense']['O3']\n",
    "                sensorSlots = readings[testAD]['devices'][kit]['alphasense']['SLOTS']\n",
    "                              \n",
    "                sensorID = (['CO', sensorID_CO, 'classic', 'single_aux', sensorSlots.index('CO')+1, deltas_co], \n",
    "                            ['NO2', sensorID_NO2, 'baseline', 'single_aux', sensorSlots.index('NO2')+1, deltas_no2], \n",
    "                            ['O3', sensorID_OX, 'baseline', 'single_aux', sensorSlots.index('O3')+1, deltas_ox])\n",
    "                \n",
    "                # Calculate correction\n",
    "                readings[testAD]['devices'][kit]['alphasense']['model_stats'] = dict()\n",
    "                readings[testAD]['devices'][kit]['data'], readings[testAD]['devices'][kit]['alphasense']['model_stats'][ad_append] = calculatePollutantsAlpha(\n",
    "                        _dataframe = readings[testAD]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = ad_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = checkBoxDecomp.value,\n",
    "                        _plotsInter = checkBoxPlotsIn.value, \n",
    "                        _plotResult = checkBoxPlotsResult.value,\n",
    "                        _verbose = checkBoxVerb.value, \n",
    "                        _printStats = checkBoxStats.value)\n",
    "\n",
    "# Find out which tests have alphasense values\n",
    "testAlphaSense = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        if 'alphasense' in readings[test]['devices'][kit] and test not in testAlphaSense:\n",
    "            testAlphaSense.append(test)\n",
    "\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing alphasense to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestAD,\n",
    "         x = widgets.SelectMultiple(options=testAlphaSense, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsAD, \n",
    "                           layout=widgets.Layout(width='700px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionAD)\n",
    "\n",
    "# Synchronise Checkbox\n",
    "\n",
    "checkBoxDecomp = widgets.Checkbox(value=False, \n",
    "                                  description='Decomp')\n",
    "checkBoxPlotsIn = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Inter')     \n",
    "checkBoxVerb = widgets.Checkbox(value=False, \n",
    "                                  description='Verbose') \n",
    "checkBoxPlotsResult = widgets.Checkbox(value=False, \n",
    "                                  description='Plots Results') \n",
    "checkBoxStats = widgets.Checkbox(value=True, \n",
    "                                  description='Print Stats') \n",
    "\n",
    "Box = widgets.HBox([calculateCorrection, checkBoxDecomp, checkBoxPlotsIn, checkBoxVerb, checkBoxPlotsResult, checkBoxStats])\n",
    "display(Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print readings[test]['devices']['STATION_CASE']['data'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore results\n",
    "# test = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "test = 'STATION'\n",
    "# device = 'STATION CHIMNEY'\n",
    "device = '4748'\n",
    "dataframe = readings[test]['devices'][device]['alphasense']['model_stats'][ad_append]\n",
    "dataframe['CO'] = dataframe['CO'].iloc[1:,:]\n",
    "dataframe['NO2'] = dataframe['NO2'].iloc[1:,:]\n",
    "\n",
    "\n",
    "fig, (ax, ax2)= plt.subplots(nrows = 2, figsize= (15,10))\n",
    "ax.bar(dataframe['CO'].index, dataframe['CO']['pollutant_avg'], label='CO')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=70 )\n",
    "ax2.bar(dataframe['NO2'].index,dataframe['NO2']['pollutant_avg'], label='NO2')\n",
    "#ax.plot(dataframe['O3'].index,dataframe['O3']['pollutant_avg'],'ro', label='O3')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=70 )\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.title('Average pollutant concentration')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Correction Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample For stats checks\n",
    "pollutant = 'NO2'\n",
    "display(CorrParams[pollutant])\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    fig1, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax1.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_temp'], label = 'Temp', linestyle='-', linewidth=0, marker='o')\n",
    "    ax2.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['stderr_hum'] , label = 'Hum', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax1.legend(loc='best')\n",
    "    ax1.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax1.set_ylabel('Avg Temp-Hum / day')\n",
    "    ax1.grid(True)\n",
    "    ax2.legend(loc='best')\n",
    "    ax2.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax2.set_ylabel('Avg Temp / day')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    fig2, (ax3, ax4) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "            \n",
    "    ax3.plot(CorrParams[pollutant]['r_valueRef'], CorrParams[pollutant]['avg_pollutant'], label = 'Avg Pollutant', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['deltaAuxBas_avg'], label = 'Delta', linestyle='-', linewidth=0, marker='o')\n",
    "    ax4.plot(CorrParams[pollutant]['avg_pollutant'], CorrParams[pollutant]['ratioAuxBas_avg'] , label = 'Ratio', linestyle='-', linewidth=0, marker='o')\n",
    "    \n",
    "    ax3.legend(loc='best')\n",
    "    ax3.set_xlabel('R^2 {} vs Ref'.format(pollutant))\n",
    "    ax3.set_ylabel('Avg {} / day'.format(pollutant))\n",
    "    ax3.grid(True)\n",
    "    ax4.legend(loc='best')\n",
    "    ax4.set_xlabel('{} Average'.format(pollutant))\n",
    "    ax4.set_ylabel('Offset / Ratio Baseline vs Auxiliary')\n",
    "    ax4.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MICS Baseline Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "from pollutant_cal_utils import *\n",
    "from test_utils import ref_append\n",
    "%matplotlib inline\n",
    "mics_append = 'MICS_BASE_CALC'\n",
    "\n",
    "selectedTestsMICS = tuple()\n",
    "def selectTestMICS(x):\n",
    "    global selectedTestsMICS\n",
    "    selectedTestsMICS = list(x)\n",
    "    \n",
    "def calculateCorrectionMICS(b):\n",
    "    clear_output()\n",
    "    for testMICS in selectedTestsMICS:\n",
    "        # Look for a reference\n",
    "        for reading in readings[testMICS]['devices']:\n",
    "            # If there is reference, use it\n",
    "            if 'is_reference' in readings[testMICS]['devices'][reading]:\n",
    "                print ('Reference found')\n",
    "                refAvail = True\n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data']\n",
    "                break\n",
    "            # If not, at least use alphasense data\n",
    "            elif 'alphasense' in readings[testMICS]['devices'][reading]:\n",
    "                refAvail = True\n",
    "                \n",
    "                dataframeRef = readings[testMICS]['devices'][reading]['data'].loc[:,['CO_' + ad_append, 'NO2_' + ad_append, 'O3_' + ad_append ]]\n",
    "                # Rename to be a reference\n",
    "                for name in dataframeRef.columns:\n",
    "                    namesub = re.sub(ad_append, ref_append, name)\n",
    "                    dataframeRef.rename(columns={name: namesub}, inplace=True)\n",
    "                break\n",
    "            else:\n",
    "                refAvail = False\n",
    "                dataframeRef = ''\n",
    "\n",
    "        for kit in deviceMICS:\n",
    "            if 'mics' in readings[testMICS]['devices'][kit]:\n",
    "                \n",
    "                sensorID = readings[testMICS]['devices'][kit]['mics']               \n",
    "                sensorID = (['CO', sensorID, 'baseline', 'single_temp'], \n",
    "                            ['NO2', sensorID, 'baseline', 'single_temp'])\n",
    "            \n",
    "            # Temporary until better understanding\n",
    "            else:\n",
    "                sensorID = (['CO', 1, 'baseline', 'single_temp'], \n",
    "                            ['NO2', 1, 'baseline', 'single_temp'])\n",
    "                \n",
    "            # Calculate correction\n",
    "            readings[testMICS]['devices'][kit]['data'], CorrParams = calculatePollutantsMICS(\n",
    "                        _dataframe = readings[testMICS]['devices'][kit]['data'], \n",
    "                        _pollutantTuples = sensorID,\n",
    "                        _append = mics_append,\n",
    "                        _refAvail = refAvail, \n",
    "                        _dataframeRef = dataframeRef, \n",
    "                        _deltas = deltasMICS,\n",
    "                        _overlapHours = overlapHours, \n",
    "                        _type_regress = 'best', \n",
    "                        _filterExpSmoothing = filterExpSmoothing, \n",
    "                        _trydecomp = False,\n",
    "                        _plotsInter = False, \n",
    "                        _plotResult = True,\n",
    "                        _verbose = False, \n",
    "                        _printStats = True)\n",
    "\n",
    "# Find out which tests have measured the mics\n",
    "testMICS = list()\n",
    "deviceMICS = list()\n",
    "for test in readings:\n",
    "    for kit in readings[test]['devices']:\n",
    "        columnsTest = readings[test]['devices'][kit]['data'].columns\n",
    "        if ('CO_MICS_RAW' in columnsTest or 'NO2_MICS_RAW' in columnsTest):\n",
    "            if test not in testMICS:\n",
    "                testMICS.append(test)\n",
    "            if kit not in deviceMICS:\n",
    "                deviceMICS.append(kit)\n",
    "            \n",
    "display(widgets.HTML('<h4>Select the tests containing MICS to calculate correction</h4>'))\n",
    "            \n",
    "interact(selectTestMICS,\n",
    "         x = widgets.SelectMultiple(options=testMICS, \n",
    "                           description='Select tests below', \n",
    "                           selected_labels = selectedTestsMICS, \n",
    "                           layout=widgets.Layout(width='1000px')))\n",
    "\n",
    "calculateCorrection = widgets.Button(description='Calculate Baseline')\n",
    "calculateCorrection.on_click(calculateCorrectionMICS)\n",
    "deltasMICS = np.arange(1,100,1)\n",
    "display(calculateCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from test_utils import combine_data\n",
    "\n",
    "name_combined_data = 'COMBINED_DEVICES'\n",
    "\n",
    "def prepareDataframe(b):\n",
    "    clear_output()\n",
    "    global test_model\n",
    "    test_model = test_selection.value\n",
    "    print 'Combining devices from test {} into one'.format(test_model)\n",
    "    try: \n",
    "        for reading in readings:\n",
    "            ## Since we don't know if there are more or less channels than last time\n",
    "            ## (and tbh, I don't feel like checking), remove the key\n",
    "            readings[reading]['devices'].pop('COMBINED_DEVICES', None)\n",
    "            ## And then add it again\n",
    "            dataframe = combine_data(readings[reading]['devices'], True)\n",
    "            readings[reading]['devices'][name_combined_data] = dict()\n",
    "            readings[reading]['devices'][name_combined_data]['data'] = dict()\n",
    "            readings[reading]['devices'][name_combined_data]['data'] = dataframe\n",
    "            \n",
    "            ## Create dict for model comparison\n",
    "            readings[test_model]['devices'][name_combined_data]['model'] = dict()\n",
    "    except:\n",
    "        print('\\tError ocurred. Review data')\n",
    "    else:\n",
    "        print('\\n\\tData combined successfully')\n",
    "    \n",
    "test_selection = widgets.Dropdown(options=[k for k in readings.keys()], \n",
    "                        layout=widgets.Layout(width='600px'),\n",
    "                        description = 'Test')\n",
    "\n",
    "prepare_button  = widgets.Button(description='Prepare Dataframe')\n",
    "prepare_button.on_click(prepareDataframe)\n",
    "\n",
    "box = widgets.HBox([test_selection, prepare_button])\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Model Process\n",
    "\n",
    "Inspired by the example of \"Jakob Aungiers, Altum Intelligence ltd\" at https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Utils\n",
    "from linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from numpy import concatenate\n",
    "\n",
    "# ML Utils\n",
    "from ml_utils import prep_dataframe_ML, fit_model_ML, predict_ML, get_inverse_transform_ML\n",
    "\n",
    "# Metrics \n",
    "from signal_utils import metrics\n",
    "import json\n",
    "\n",
    "## Input\n",
    "##--------------\n",
    "configs = json.load(open('./spotcheck_models/models.json', 'r'))\n",
    "test_model = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "device_name = 'STATION_CASE'\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "##--------------\n",
    "\n",
    "for config in configs.keys():\n",
    "    \n",
    "    # Retrieve models\n",
    "    model_name = config\n",
    "    print model_name\n",
    "\n",
    "    if model_name not in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "        model_type = configs[config]['type']\n",
    "        features = configs[config]['features']\n",
    "        \n",
    "        tuple_features = [(k, v) for k, v in features.iteritems()]\n",
    "        print tuple_features\n",
    "        \n",
    "        \n",
    "        list_features = list()\n",
    "        for item in tuple_features: \n",
    "            if item[0] == 'REF':\n",
    "                a = tuple_features.index(item)\n",
    "                list_features.insert(0,item[1] + '_' + device_name)\n",
    "                reference_name = item[1] + '_' + device_name\n",
    "            else:\n",
    "                list_features.append(item[1] + '_' + device_name)\n",
    "                \n",
    "        tuple_features[0], tuple_features[a] = tuple_features[a], tuple_features[0]\n",
    "        \n",
    "        tuple_features_combined = [(item[0], item[1]  + '_' + device_name) for item in tuple_features]\n",
    "        \n",
    "        dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].loc[:,list_features]\n",
    "        dataframeModel = dataframeModel.dropna()\n",
    "        \n",
    "        dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "        dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "        \n",
    "        print '\\t{}'. format(model_type) \n",
    "        if model_type == 'OLS':\n",
    "            \n",
    "            formula_expression = configs[config]['expression']\n",
    "            ratio_train = configs[config]['ratio_train']\n",
    "            alpha_filter = configs[config]['alpha_filter']\n",
    "            n_lags = 1\n",
    "\n",
    "            ## Model Fit\n",
    "            dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, tuple_features_combined, ratio_train, alpha_filter, device_name)\n",
    "            model = fit_model_OLS(formula_expression, dataTrain, False)\n",
    "            \n",
    "            ## Predict the model results\n",
    "            referenceTrain, predictionTrain = predict_OLS(model, dataTrain, True, False, 'train')\n",
    "            referenceTest, predictionTest = predict_OLS(model, dataTest, True, False, 'test')\n",
    "\n",
    "            predictionTrain = predictionTrain.values\n",
    "            indexTrain = dataTrain['index']\n",
    "            indexTest = dataTest['index']\n",
    "\n",
    "        elif model_type == 'LSTM':\n",
    "            \n",
    "            epochs = configs[config]['epochs']\n",
    "            batch_size = configs[config]['batch_size']\n",
    "            verbose = configs[config]['verbose']\n",
    "            n_lags = configs[config]['n_lags']\n",
    "            loss = configs[config][\"loss\"]\n",
    "            optimizer = configs[config][\"optimizer\"]\n",
    "            layers = configs[config]['layers']\n",
    "            ratio_train = configs[config]['ratio_train']\n",
    "\n",
    "            ## Prep Dataframe\n",
    "            index, train_X, train_y, test_X, test_y, scalerX, scalery, n_train_periods = prep_dataframe_ML(dataframeModel, min_date, max_date, list_features, n_lags, ratio_train, alpha_filter, reference_name, False)\n",
    "            ## Model Fit\n",
    "            model = fit_model_ML(train_X, train_y, \n",
    "                                 test_X, test_y, \n",
    "                                 epochs = epochs, \n",
    "                                 batch_size = batch_size, \n",
    "                                 verbose = True, \n",
    "                                 plotResult = False, \n",
    "                                 loss = loss, \n",
    "                                 optimizer = optimizer,\n",
    "                                 layers = layers)\n",
    "            \n",
    "            # Get model prediction\n",
    "            \n",
    "            # Get model prediction\n",
    "            referenceTrain = get_inverse_transform_ML(train_y, n_lags, scalery)\n",
    "            predictionTrain = predict_ML(model, train_X, n_lags, scalery)\n",
    "\n",
    "            referenceTest = get_inverse_transform_ML(test_y, n_lags, scalery)\n",
    "            predictionTest = predict_ML(model, test_X, n_lags, scalery)\n",
    "            \n",
    "            indexTrain = index[:n_train_periods]\n",
    "            indexTest = index[n_train_periods+n_lags:]\n",
    "            formula_expression = '-'\n",
    "        \n",
    "    \n",
    "        dataFrameTrain = pd.DataFrame(data = {'reference': referenceTrain, 'prediction': predictionTrain}, \n",
    "                                      index = indexTrain)\n",
    "        dataFrameTest = pd.DataFrame(data = {'reference': referenceTest, 'prediction': predictionTest}, \n",
    "                                      index = indexTest)\n",
    "        \n",
    "        dataFrameExport = dataFrameTrain.copy()\n",
    "        dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "        # Get model metrics\n",
    "        metrics_model_train = metrics(referenceTrain, predictionTrain)\n",
    "        metrics_model_test = metrics(referenceTest, predictionTest)\n",
    "        \n",
    "        ## Put everything in the dict\n",
    "        dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "        \n",
    "        # From https://hackmd.io/Y62wiJw0RaiBfU4Xhv8dQQ#\n",
    "        dictModel[model_name] = dict()\n",
    "        dictModel[model_name]['metrics'] = dict()\n",
    "        dictModel[model_name]['metrics']['train'] = metrics_model_train\n",
    "        dictModel[model_name]['metrics']['test'] = metrics_model_test\n",
    "        \n",
    "        # Model Parameters\n",
    "        dictModel[model_name]['parameters'] = dict()\n",
    "        dictModel[model_name]['parameters']['features'] = dict()\n",
    "        dictModel[model_name]['parameters']['features']['ref'] = tuple_features[0]\n",
    "        dictModel[model_name]['parameters']['features']['items'] = tuple_features[1:]        \n",
    "        dictModel[model_name]['parameters']['ratio_train'] = ratio_train\n",
    "        dictModel[model_name]['parameters']['alpha_filter'] = alpha_filter\n",
    "        dictModel[model_name]['model'] = model\n",
    "        dictModel[model_name]['modelType'] = model_type\n",
    "        \n",
    "        # Put it back in the readings dataframe\n",
    "        readings[test_model]['devices'][name_combined_data]['model'][model_name] = dictModel[model_name]\n",
    "        readings[test_model]['devices'][model_name] = dict()\n",
    "        readings[test_model]['devices'][model_name]['data'] = dataFrameExport\n",
    "        print '\\t Model Calculated'\n",
    "        \n",
    "    else:\n",
    "        print '\\t Model already present, skipping'\n",
    "\n",
    "print 'All models calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Checks\n",
    "\n",
    "\n",
    "\n",
    "#### Data stationarity\n",
    "\n",
    "We will use the Dicker-fuller test (ADF) test to verify the data is stationary. We need to check data stationarity for certain type of models. \n",
    "\n",
    "If the process is stationary means it doesn’t change its statistical properties over time: mean and variance do not change over time (constancy of variance is also called homoscedasticity), also covariance function does not depend on the time (should only depend on the distance between observations) Source [here](https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-9-time-series-analysis-in-python-a270cb05e0b3). Visually:\n",
    "\n",
    "- Growing mean --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/0*qrYiVksz8g3drl5Z.png)\n",
    "\n",
    "- Growing spread --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/0*fEqQDq_TaEqa511n.png)\n",
    "\n",
    "- Varying time covariance --> Non stationary\n",
    "![](https://cdn-images-1.medium.com/max/800/1*qJs3g2f77flIXr6mFsbPmw.png)\n",
    "\n",
    "- Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "- Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary). Hence:\n",
    "\n",
    "- p-value > 0.05: fail to reject the null hypothesis (H0), the data has an unit root and is non-stationary.\n",
    "- p-value <= 0.05: reject the null hypothesis (H0), the data does not have an unit root and is stationary.\n",
    "\n",
    "#### Autocorrelation\n",
    "\n",
    "High levels of autocorrelation can indicate our time series is shows seasonality. We will use the ACF plot to identify possible autocorrelation and potentially include differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "    print column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import tfullerplot\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'GB_1W'],\n",
    "                 ['A', 'CO_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'],\n",
    "                 ['D', 'PM_25'])\n",
    "\n",
    "device_name = 'STATION_CASE'\n",
    "\n",
    "# --------------------\n",
    "list_features = list()\n",
    "for item in tuple_features:\n",
    "    item_name = item[1] + '_' + device_name\n",
    "    list_features.append(item_name)\n",
    "    if item_name not in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "        print '{} not in {}'.format(item_name, test_model)\n",
    "        break\n",
    "\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].loc[:,list_features]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "\n",
    "min_date = '2018-08-04 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "dataframeModel = dataframeModel.groupby(pd.Grouper(freq='1Min')).aggregate(np.mean)\n",
    "\n",
    "# Fuller Plot for all\n",
    "for item in tuple_features:\n",
    "    if item[0] == 'REF': \n",
    "        reference = dataframeModel.loc[:,item[1] + '_' + device_name]\n",
    "        reference_name = reference.name\n",
    "    x = dataframeModel.loc[:,item[1] + '_' + device_name]\n",
    "    tfullerplot(x, name = item[1], lags=60, lags_diff = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granger Casuality Test (use with caution)\n",
    "\n",
    "This test is useful to determine the casuality of variables and determining whether one time series is useful in forecasting another. \n",
    "\n",
    "The Null hypothesis for granger causality tests is that the time series in the second column, x2, does NOT Granger cause the time series in the first column, x1. Grange causality means that past values of x2 have a statistically significant effect on the current value of x1, taking past values of x1 into account as regressors. We reject the null hypothesis that x2 does not Granger cause x1 if the pvalues are below a desired size of the test. Hence:\n",
    "\n",
    "- p-value < size: allows to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "- p-value > size: we fail to reject the null hypothesis (H0) for x1 = f(x2)\n",
    "\n",
    "The null hypothesis for all four test is that the coefficients corresponding to past values of the second time series are zero.\n",
    "\n",
    "Reference [here](https://en.wikipedia.org/wiki/Granger_causality), [here](https://stats.stackexchange.com/questions/24753/interpreting-granger-causality-tests-results#24796) and [here](http://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.grangercausalitytests.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the channels first\n",
    "for channel in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "    print channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import prep_data_OLS, fit_model_OLS, predict_OLS, plotOLSCoeffs\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from numpy import concatenate\n",
    "\n",
    "## Select data\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "\n",
    "formula_expression = 'REF ~ np.log10(A)+B+D'\n",
    "model_name_OLS = 'log(CO_MICS) + TEMP + PM'\n",
    "\n",
    "ratio_train = 3./4 # Important that this is a float, don't forget the .\n",
    "alpha_filter = 0.3\n",
    "\n",
    "# print ('--------------------------------------')\n",
    "# print ('Granger Causality Test')\n",
    "## Granger Causality Test (WIP)\n",
    "# for item in tuple_features:\n",
    "#     if item[0] != 'REF':\n",
    "#         print '\\nCausality for x1 = {} and x2 = {}'.format(reference_name, item[1])\n",
    "#         x = dataframeModel.loc[:,[reference_name, item[1]]].dropna()\n",
    "#         x = x.values\n",
    "#         granger_causality_tests = grangercausalitytests(x, 1)\n",
    "#         # print granger_causality_tests\n",
    "\n",
    "tuple_features_combined = [(item[0], item[1]  + '_' + device_name) for item in tuple_features]\n",
    "    \n",
    "## Model Fit\n",
    "dataTrain, dataTest, n_train_periods = prep_data_OLS(dataframeModel, tuple_features_combined, ratio_train, alpha_filter, device_name)\n",
    "model = fit_model_OLS(formula_expression, dataTrain, True)\n",
    "\n",
    "plotOLSCoeffs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import predict_OLS\n",
    "from signal_utils import metrics\n",
    "\n",
    "## Predict the model results\n",
    "referenceTrain, predictionTrain = predict_OLS(model, dataTrain, True, False, 'train')\n",
    "referenceTest, predictionTest = predict_OLS(model, dataTest, True, False, 'test')\n",
    "\n",
    "# Put train into pd dataframe\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': referenceTrain, 'prediction': predictionTrain.values}, \n",
    "                              index = dataTrain['index'])\n",
    "\n",
    "# Put test into pd dataframe\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': referenceTest, 'prediction': predictionTest}, \n",
    "                              index = dataTest['index'])\n",
    "\n",
    "## Combine them for export\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "metrics_model_train = metrics(referenceTrain, predictionTrain)\n",
    "metrics_model_test = metrics(referenceTest, predictionTest)\n",
    "\n",
    "## Metrics Train\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))\n",
    "\n",
    "dictModel = dict()\n",
    "dictModel[model_name_OLS] = dict()\n",
    "dictModel[model_name_OLS]['metrics'] = dict()\n",
    "dictModel[model_name_OLS]['metrics']['train'] = metrics_model_train\n",
    "dictModel[model_name_OLS]['metrics']['test'] = metrics_model_test\n",
    "\n",
    "# Model Parameters\n",
    "dictModel[model_name_OLS]['parameters'] = dict()\n",
    "dictModel[model_name_OLS]['parameters']['formula'] = formula_expression\n",
    "dictModel[model_name_OLS]['parameters']['features'] = dict()\n",
    "dictModel[model_name_OLS]['parameters']['features']['ref'] = tuple_features[0]\n",
    "dictModel[model_name_OLS]['parameters']['features']['items'] = tuple_features[1:]\n",
    "\n",
    "dictModel[model_name_OLS]['parameters']['ratio_train'] = ratio_train\n",
    "dictModel[model_name_OLS]['parameters']['alpha_filter'] = alpha_filter\n",
    "\n",
    "dictModel[model_name_OLS]['model'] = model\n",
    "dictModel[model_name_OLS]['modelType'] = 'OLS'\n",
    "\n",
    "# Put it back in the readings dict\n",
    "readings[test_model]['devices'][name_combined_data]['model'][model_name_OLS] = dictModel[model_name_OLS]\n",
    "readings[test_model]['devices'][model_name_OLS] = dict()\n",
    "readings[test_model]['devices'][model_name_OLS]['data'] = dataFrameExport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from linear_regression_utils import modelRplots\n",
    "%matplotlib inline\n",
    "\n",
    "modelRplots(model, dataTrain, dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "modelDirectory = join(rootDirectory, 'smartcitizen-iscape-models/')\n",
    "modelTarget = 'MICS' # MICS, ALPHASENSE OR PMS\n",
    "\n",
    "modelDirOLS = join(modelDirectory, 'Models/', modelTarget)\n",
    "summaryDir = join(modelDirectory, 'Models/summary.json')\n",
    "filenameOLS = join(modelDirOLS, model_name_OLS)\n",
    "\n",
    "# Save everything\n",
    "joblib.dump(dictModel[model_name_OLS]['metrics'], filenameOLS + '_metrics.sav')\n",
    "joblib.dump(dictModel[model_name_OLS]['parameters'], filenameOLS + '_parameters.sav')\n",
    "joblib.dump(dictModel[model_name_OLS]['model'], filenameOLS + '_model.sav')\n",
    "\n",
    "print(\"Model: \\n\\t\" + model_name_OLS + \"\\nSaved in:\\n\\t\" + modelDirOLS)\n",
    "\n",
    "summary = json.load(open(summaryDir, 'r'))\n",
    "summary[modelTarget][model_name_OLS] = dict()\n",
    "summary[modelTarget][model_name_OLS] = dictModel[model_name_OLS]['modelType']\n",
    "\n",
    "with open(summaryDir, 'w') as json_file:\n",
    "    json_file.write(json.dumps(summary))\n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for channel in readings[test_model]['devices'][name_combined_data]['data'].columns:\n",
    "    print channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine all data in one dataframe\n",
    "#from ml_utils import prep_dataframe_ML\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'CO_AD_BASE_1-60_FILTER'],\n",
    "                 ['A', 'CO_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'],\n",
    "                 ['D', 'PM_25'])\n",
    "\n",
    "device_name = 'STATION_CASE'\n",
    "model_name_ML = 'LSTM CO - epochs = 100 - filter_alpha = 0.3 -  traintest = 0.8 - 3 lags'\n",
    "modeltype = 'LSTM'\n",
    "\n",
    "ratio_train = 4./5 # Important that this is a float, don't forget the .\n",
    "alpha_filter = 0.3 # 1 means no filtering\n",
    "\n",
    "# Number of lags for the model\n",
    "n_lags = 1\n",
    "\n",
    "## Prep Dataframe\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data']\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "dataframeModel = dataframeModel.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_features: \n",
    "    if item[0] == 'REF':\n",
    "        list_features.insert(0,item[1] + '_' + device_name)\n",
    "        reference_name = item[1] + '_' + device_name\n",
    "    else:\n",
    "        list_features.append(item[1] + '_' + device_name)\n",
    "\n",
    "# Data Split\n",
    "index, train_X, train_y, test_X, test_y, scalerX, scalery, n_train_periods = prep_dataframe_MLA(dataframeModel, min_date, max_date, list_features, n_lags, ratio_train, alpha_filter, reference_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ml_utils import fit_model_ML\n",
    "from ml_utils import predict_ML, get_inverse_transform_ML\n",
    "from signal_utils import metrics\n",
    "\n",
    "# Model Fit\n",
    "model = fit_model_ML('LSTM', train_X, train_y, \n",
    "                       test_X, test_y, \n",
    "                       epochs = 100, batch_size = 72, \n",
    "                       verbose = 2, plotResult = True, \n",
    "                       loss = 'mse', optimizer = 'adam')\n",
    " \n",
    "# Get model prediction\n",
    "inv_y_train = get_inverse_transform_ML(train_y, n_lags, scalery)\n",
    "inv_yhat_train = predict_ML(model, train_X, n_lags, scalery)\n",
    "\n",
    "inv_y_test = get_inverse_transform_ML(test_y, n_lags, scalery)\n",
    "inv_yhat_test = predict_ML(model, test_X, n_lags, scalery)\n",
    "\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': inv_y_train, 'prediction': inv_yhat_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': inv_y_test, 'prediction': inv_yhat_test}, \n",
    "                              index = index[n_train_periods+n_lags:])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(inv_y_train, inv_yhat_train)\n",
    "metrics_model_test = metrics(inv_y_test, inv_yhat_test)\n",
    "\n",
    "## Print Metrics\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "# Actual data\n",
    "plot.plot(index[:n_train_periods], inv_y_train,'r', label = 'Reference Train', alpha = 0.3)\n",
    "plot.plot(index[n_train_periods+n_lags:], inv_y_test, 'b', label = 'Reference Test', alpha = 0.3)\n",
    "# Fitted Values for Training\n",
    "plot.plot(index[:n_train_periods], inv_yhat_train, 'r', label = 'Prediction Train')\n",
    "\n",
    "# Fitted Values for Test\n",
    "plot.plot(index[n_train_periods+n_lags:], inv_yhat_test, 'b', label = 'Prediction Test')\n",
    "\n",
    "plot.title('{} Regression Results'.format(modeltype) + model_name_ML)\n",
    "plot.ylabel('Reference/Prediction (-)')\n",
    "plot.xlabel('Date (-)')\n",
    "plot.legend(loc='best')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "\n",
    "No feature scaling implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from signal_utils import metrics\n",
    "from ml_utils import evaluate\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'NO2_AD_BASE1-60_FILTER'],\n",
    "                 ['A', 'NO2_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'])\n",
    "\n",
    "device_name = 'STATION_CASE'\n",
    "ratio_train = 0.8\n",
    "\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data']\n",
    "\n",
    "modeltype = 'RandomForest'\n",
    "model_name_ML = modeltype + 'NO2- n_estimators = 1000 - random_state = 42 -  traintest = {} - no lags'.format(ratio_train)\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "### \n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_features: \n",
    "    if item[0] == 'REF':\n",
    "        list_features.insert(0,item[1] + '_' + device_name)\n",
    "        reference_name = item[1] + '_' + device_name\n",
    "    else:\n",
    "        list_features.append(item[1] + '_' + device_name)\n",
    "\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data']\n",
    "#dataframeModel = dataframeModel.groupby(pd.Grouper(freq='10Min')).aggregate(np.mean)\n",
    "dataframeModel = dataframeModel.loc[:, list_features]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "index = dataframeModel.index\n",
    "\n",
    "labels = dataframeModel[reference_name]\n",
    "features = dataframeModel.drop(reference_name, axis = 1)\n",
    "\n",
    "# List of features for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Training and Testing Sets\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, random_state = 42, \n",
    "                                                    test_size = 1-ratio_train, shuffle = False)\n",
    "\n",
    "n_train_periods = train_X.shape[0]\n",
    "print('Training X Shape:', train_X.shape)\n",
    "print('Training y Shape:', train_y.shape)\n",
    "print('Testing X Shape:', test_X.shape)\n",
    "print('Testing y Shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "model = RandomForestRegressor(n_estimators= 1000, random_state = 42)\n",
    "\n",
    "## Train the model on training data\n",
    "model.fit(train_X, train_y);\n",
    "\n",
    "## Get model prediction\n",
    "inv_yhat_train = evaluate(model, features[:n_train_periods], labels[:n_train_periods])\n",
    "inv_yhat_test = evaluate(model, features[n_train_periods:], labels[n_train_periods:])\n",
    "\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': labels[:n_train_periods], 'prediction': inv_yhat_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': labels[n_train_periods:], 'prediction': inv_yhat_test}, \n",
    "                             index = index[n_train_periods:])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(labels[:n_train_periods], inv_yhat_train)\n",
    "metrics_model_test = metrics(labels[n_train_periods:], inv_yhat_test)\n",
    "\n",
    "## Print Metrics\n",
    "print '---'\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search with Cross Validation \n",
    "\n",
    "We perform here cross validated random search of the model hyperparameters, to later on retrieve the best parameters with a grid search around the best found results of the CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **k-fold cross validation** below:\n",
    "\n",
    "![](https://i.imgur.com/HLbgMSS.png)\n",
    "\n",
    "Source: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_X, train_y)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the evaluation of the best and the baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the default model\n",
    "base_model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "base_model.fit(train_X, train_y)\n",
    "## Get base model prediction\n",
    "predictions_base_train = evaluate(base_model, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_base_test = evaluate(base_model, features[n_train_periods:], labels[n_train_periods:])\n",
    "\n",
    "## Evaluate the best model\n",
    "best_random = rf_random.best_estimator_\n",
    "predictions_best_train = evaluate(best_random, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_best_test = evaluate(best_random, features[n_train_periods:], labels[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'n_estimators': [200, 300, 400, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                           n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best grid estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n",
    "print best_grid\n",
    "predictions_best_grid_train = evaluate(best_grid, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_best_grid_test = evaluate(best_grid, features[n_train_periods:], labels[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the best predictions of the grid search, put them in the dataframe for plotting and archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTrain_best_grid = pd.DataFrame(data = {'reference': labels[:n_train_periods], 'prediction': predictions_best_grid_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest_best_grid = pd.DataFrame(data = {'reference': labels[n_train_periods:], 'prediction': predictions_best_grid_test}, \n",
    "                             index = index[n_train_periods:])\n",
    "\n",
    "model_name_ML = model_name_ML + '_best_grid_search'\n",
    "\n",
    "dataFrameExport = dataFrameTrain_best_grid.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest_best_grid)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(labels[:n_train_periods], predictions_best_train)\n",
    "metrics_model_test = metrics(labels[n_train_periods:], predictions_best_test)\n",
    "\n",
    "## Print Metrics\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))\n",
    "    \n",
    "model = best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "# Actual data\n",
    "plot.plot(dataFrameTrain.index, dataFrameTrain['reference'],'r', linewidth = 1, label = 'Reference Train', alpha = 0.3)\n",
    "plot.plot(dataFrameTest.index, dataFrameTest['reference'], 'b', linewidth = 1, label = 'Reference Test', alpha = 0.3)\n",
    "\n",
    "# Fitted Values for Training\n",
    "plot.plot(dataFrameTrain.index, dataFrameTrain['prediction'], 'r', linewidth = 1, label = 'Prediction Train')\n",
    "\n",
    "# Fitted Values for Test\n",
    "plot.plot(dataFrameTest.index, dataFrameTest['prediction'], 'b', linewidth = 1, label = 'Prediction Test')\n",
    "\n",
    "plot.title('{} Regression Results'.format(modeltype) + model_name_ML)\n",
    "plot.ylabel('Reference/Prediction (-)')\n",
    "plot.xlabel('Date (-)')\n",
    "plot.legend(loc='best')\n",
    "plot.show()\n",
    "\n",
    "## Model feature importances\n",
    "importances = list(model.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list_features[1:], importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "for pair in feature_importances:\n",
    "    print ('Variable: {} Importance: {}'.format(pair[0], pair[1]))\n",
    "\n",
    "# Reset style \n",
    "plot.style.use('seaborn')\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "fig= plot.figure(figsize = (12,6))\n",
    "plot.subplot(1,2,1)\n",
    "# Make a bar chart\n",
    "plot.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "\n",
    "# Tick labels for x axis\n",
    "plot.xticks(x_values, list_features[1:], rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "\n",
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "plot.subplot(1,2,2)\n",
    "# Make a line graph\n",
    "plot.plot(x_values, cumulative_importances, 'g-')\n",
    "\n",
    "# Draw line at 95% of importance retained\n",
    "plot.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "\n",
    "# Format x ticks and labels\n",
    "plot.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plot.xlabel('Variable'); plot.ylabel('Cumulative Importance'); plot.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.svm import SVR\n",
    "from signal_utils import metrics\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Always have an item called 'REF', the rest can be anything\n",
    "tuple_features = (['REF', 'NO2_AD_BASE1-60_FILTER'],\n",
    "                 ['A', 'NO2_MICS_RAW'],\n",
    "                 ['B', 'TEMP'],\n",
    "                 ['C', 'HUM'])\n",
    "\n",
    "device_name = 'STATION_CASE'\n",
    "ratio_train = 0.8\n",
    "\n",
    "dataframeModel = readings[test_model]['devices'][name_combined_data]['data'].copy()\n",
    "\n",
    "modeltype = 'SVR'\n",
    "model_name_ML = modeltype + 'NO2 - n_estimators = 1000 - random_state = 42 -  traintest = {} - no lags'.format(ratio_train)\n",
    "\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "### \n",
    "\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_features: \n",
    "    if item[0] == 'REF':\n",
    "        list_features.insert(0,item[1] + '_' + device_name)\n",
    "        reference_name = item[1] + '_' + device_name\n",
    "    else:\n",
    "        list_features.append(item[1] + '_' + device_name)\n",
    "\n",
    "dataframeModel = dataframeModel.loc[:, list_features]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "index = dataframeModel.index\n",
    "\n",
    "labels = dataframeModel[reference_name]\n",
    "features = dataframeModel.drop(reference_name, axis = 1)\n",
    "\n",
    "# List of features for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Training and Testing Sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, random_state = 42, \n",
    "                                                    test_size = 1-ratio_train, shuffle = True)\n",
    "\n",
    "n_train_periods = train_X.shape[0]\n",
    "print('Training X Shape:', train_X.shape)\n",
    "print('Training y Shape:', train_y.shape)\n",
    "print('Testing X Shape:', test_X.shape)\n",
    "print('Testing y Shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate model \n",
    "model = SVR(kernel='rbf')\n",
    "\n",
    "## Train the model on training data\n",
    "model.fit(train_X,train_y)\n",
    "\n",
    "## Get model prediction\n",
    "inv_yhat_train = evaluate(model, features[:n_train_periods], labels[:n_train_periods])\n",
    "inv_yhat_test = evaluate(model, features[n_train_periods:], labels[n_train_periods:])\n",
    "\n",
    "dataFrameTrain = pd.DataFrame(data = {'reference': labels[:n_train_periods], 'prediction': inv_yhat_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest = pd.DataFrame(data = {'reference': labels[n_train_periods:], 'prediction': inv_yhat_test}, \n",
    "                             index = index[n_train_periods:])\n",
    "\n",
    "dataFrameExport = dataFrameTrain.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(labels[:n_train_periods], inv_yhat_train)\n",
    "metrics_model_test = metrics(labels[n_train_periods:], inv_yhat_test)\n",
    "\n",
    "## Print Metrics\n",
    "print '---'\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearch with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "model = SVR(kernel='rbf', gamma=0.1)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "random_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "               \"gamma\": np.logspace(-2, 2, 5),\n",
    "               \"kernel\": ['rbf', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "\n",
    "svr_random = RandomizedSearchCV(estimator = model, cv=5, \n",
    "                         n_iter = 100, scoring = 'neg_mean_absolute_error',\n",
    "                         param_distributions=random_grid,  verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "svr_random.fit(train_X, train_y)\n",
    "svr_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Base model and best randomized search fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the default model\n",
    "base_model = SVR(kernel='rbf', gamma=0.1)\n",
    "base_model.fit(train_X, train_y)\n",
    "## Get base model prediction\n",
    "predictions_base_train = evaluate(base_model, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_base_test = evaluate(base_model, features[n_train_periods:], labels[n_train_periods:])\n",
    "\n",
    "## Evaluate the best model\n",
    "best_random = svr_random.best_estimator_\n",
    "predictions_best_train = evaluate(best_random, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_best_test = evaluate(best_random, features[n_train_periods:], labels[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform now a gridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\"C\": [1e0, 1e1, 1e2, 1e3], \n",
    "               \"gamma\": np.logspace(-2, 2, 5),\n",
    "              \"shrinking\": [True, False]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = SVR(kernel='', gamma=?)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                           scoring = 'neg_mean_absolute_error', cv = 3, \n",
    "                           n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n",
    "print best_grid\n",
    "predictions_best_grid_train = evaluate(best_grid, features[:n_train_periods], labels[:n_train_periods])\n",
    "predictions_best_grid_test = evaluate(best_grid, features[n_train_periods:], labels[n_train_periods:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy, save it in the export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameTrain_best_grid = pd.DataFrame(data = {'reference': labels[:n_train_periods], 'prediction': predictions_best_grid_train}, \n",
    "                              index = index[:n_train_periods])\n",
    "dataFrameTest_best_grid = pd.DataFrame(data = {'reference': labels[n_train_periods:], 'prediction': predictions_best_grid_test}, \n",
    "                             index = index[n_train_periods:])\n",
    "\n",
    "model_name_ML = model_name_ML + '_best_grid_search'\n",
    "\n",
    "dataFrameExport = dataFrameTrain_best_grid.copy()\n",
    "dataFrameExport = dataFrameExport.combine_first(dataFrameTest_best_grid)\n",
    "\n",
    "# Get model metrics\n",
    "metrics_model_train = metrics(labels[:n_train_periods], predictions_best_train)\n",
    "metrics_model_test = metrics(labels[n_train_periods:], predictions_best_test)\n",
    "\n",
    "## Print Metrics\n",
    "print('\\t\\t Train \\t\\t Test')\n",
    "for item in metrics_model_train.keys():\n",
    "    print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model_train[item], metrics_model_test[item]))\n",
    "    \n",
    "model = best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Put everything in the dict\n",
    "dictModel = readings[test_model]['devices'][name_combined_data]\n",
    "\n",
    "# From https://hackmd.io/Y62wiJw0RaiBfU4Xhv8dQQ#\n",
    "dictModel[model_name_ML] = dict()\n",
    "dictModel[model_name_ML]['metrics'] = dict()\n",
    "dictModel[model_name_ML]['metrics']['train'] = metrics_model_train\n",
    "dictModel[model_name_ML]['metrics']['test'] = metrics_model_test\n",
    "\n",
    "# Model Parameters\n",
    "dictModel[model_name_ML]['parameters'] = dict()\n",
    "dictModel[model_name_ML]['parameters']['features'] = tuple_features\n",
    "dictModel[model_name_ML]['parameters']['ratio_train'] = n_train_periods\n",
    "\n",
    "if modeltype == 'LSTM':\n",
    "    dictModel[model_name_ML]['parameters']['scalerX'] = scalerX\n",
    "    dictModel[model_name_ML]['parameters']['scalery'] = scalery\n",
    "    dictModel[model_name_ML]['parameters']['n_lags'] = n_lags\n",
    "    dictModel[model_name_ML]['parameters']['alpha_filter'] = alpha_filter\n",
    "\n",
    "dictModel[model_name_ML]['data'] = dict()\n",
    "dictModel[model_name_ML]['data']['train'] = dataFrameTrain\n",
    "dictModel[model_name_ML]['data']['test'] = dataFrameTest\n",
    "dictModel[model_name_ML]['model'] = model\n",
    "dictModel[model_name_ML]['modelType'] = modeltype\n",
    "\n",
    "# Put it back in the readings dataframe\n",
    "readings[test_model]['devices'][name_combined_data]['model'][model_name_ML] = dictModel[model_name_ML]\n",
    "readings[test_model]['devices'][model_name_ML] = dict()\n",
    "readings[test_model]['devices'][model_name_ML]['data'] = dataFrameExport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "\n",
    "modelDirectory = join(rootDirectory, 'smartcitizen-iscape-models/')\n",
    "modelTarget = 'MICS' # MICS, ALPHASENSE OR PMS\n",
    "modelDirML = join(modelDirectory, 'Models/', modelTarget)\n",
    "summaryDir = join(modelDirectory, 'Models/summary.json')\n",
    "filenameML = join(modelDirML, model_name_ML)\n",
    "\n",
    "# Save everything\n",
    "joblib.dump(dictModel[model_name_ML]['metrics'], filenameML + '_metrics.sav')\n",
    "joblib.dump(dictModel[model_name_ML]['parameters'], filenameML + '_parameters.sav')\n",
    "\n",
    "if modeltype == 'LSTM':\n",
    "    model_json = model.to_json()\n",
    "    with open(filenameML + \"_model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(filenameML + \"_model.h5\")\n",
    "\n",
    "elif modeltype == 'RandomForest':\n",
    "    joblib.dump(model, filenameML + '_model.sav', compress=3)\n",
    "    \n",
    "print(\"Model: \\n\\t\" + model_name_ML + \"\\nSaved in:\\n\\t\" + modelDirML)\n",
    "\n",
    "summary = json.load(open(summaryDir, 'r'))\n",
    "summary[modelTarget][model_name_ML] = dict()\n",
    "summary[modelTarget][model_name_ML] = modeltype\n",
    "\n",
    "with open(summaryDir, 'w') as json_file:\n",
    "    json_file.write(json.dumps(summary))\n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeries Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "referencePlotted = False\n",
    "        \n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "\n",
    "            ratio_train = readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['ratio_train']\n",
    "\n",
    "            data = readings[test_model]['devices'][model_name]['data']\n",
    "                \n",
    "            total_len = len(data.index)\n",
    "            n_train_periods = int(round(total_len*ratio_train))\n",
    "    \n",
    "            dataframeTrain = data.iloc[:n_train_periods,:]\n",
    "            dataframeTest = data.iloc[n_train_periods:,:]\n",
    "                        \n",
    "            if (not referencePlotted):\n",
    "                plt.plot(dataframeTrain.index, dataframeTrain['reference'], 'b.', label = 'Reference Train', alpha = 0.3)\n",
    "                plt.plot(dataframeTest.index, dataframeTest['reference'], 'b.', label = 'Reference Test', alpha = 0.3)\n",
    "                referencePlotted = True\n",
    "            \n",
    "            plt.plot(dataframeTrain.index, dataframeTrain['prediction'], linewidth = 0.6, label = 'Prediction Train ' + model_name)\n",
    "            plt.plot(dataframeTest.index, dataframeTest['prediction'], linewidth = 0.6, label = 'Prediction Test ' + model_name)\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "# plt.ylabel(str(readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['features'][\"ref\"][1]))\n",
    "plt.xlabel('Date (-)')\n",
    "plt.grid()\n",
    "# plt.title('Model Comparison for ' + str(readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['features'][\"ref\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "\n",
    "number_of_subplots = 0\n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "            number_of_subplots += 1\n",
    "            \n",
    "\n",
    "if number_of_subplots % 2 == 0: cols = 2\n",
    "else: cols = 3\n",
    "rows = int(math.ceil(number_of_subplots / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "n = 0\n",
    "\n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "            \n",
    "            ratio_train = readings[test_model]['devices'][name_combined_data]['model'][model_name]['parameters']['ratio_train']\n",
    "\n",
    "            data = readings[test_model]['devices'][model_name]['data']\n",
    "            dataVal = data.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)    \n",
    "            total_len = len(dataVal.index)\n",
    "            n_train_periods = int(round(total_len*ratio_train))\n",
    "    \n",
    "            dataframeTrain = dataVal.iloc[:n_train_periods,:]\n",
    "            dataframeTest = dataVal.iloc[n_train_periods:,:]\n",
    "\n",
    "            ax = fig.add_subplot(gs[n])\n",
    "            n += 1          \n",
    "            plot.plot(dataframeTrain['reference'], dataframeTrain['prediction'], 'go', label = 'Train ' + model_name, alpha = 0.3)\n",
    "            plot.plot(dataframeTest['reference'], dataframeTest['prediction'], 'bo', label = 'Test ' + model_name, alpha = 0.3)\n",
    "\n",
    "            plot.plot(dataframeTrain['reference'], dataframeTrain['reference'], 'k', linewidth = 0.4, alpha = 0.3)\n",
    "\n",
    "            plot.legend(loc = 'best')\n",
    "            plot.ylabel('Prediction (-)')\n",
    "            plot.xlabel('Reference (-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     12,
     18
    ]
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "for model in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "    print '-----------------------------------------------------'\n",
    "    print '\\nModel Name: {}'.format(model)\n",
    "    print '\\n\\t\\t Train \\t\\t Test'\n",
    "    metrics_model = readings[test_model]['devices'][name_combined_data]['model'][model]['metrics']\n",
    "    for item in metrics_model['train']:\n",
    "        print ('% s: \\t %.5f \\t %.5f ' % (item, metrics_model['train'][item], metrics_model['test'][item]))\n",
    "\n",
    "def minRtarget(targetR):\n",
    "    return sqrt(1+ np.power(targetR,2)-2*np.power(targetR,2))\n",
    "\n",
    "_plot_train = True\n",
    "_dataframe = readings[test_model]['devices'][name_combined_data]\n",
    "\n",
    "def targetDiagram(dataframe, plot_train):\n",
    "\n",
    "    targetR20 = 0.5\n",
    "    targetR0 = sqrt(targetR20)\n",
    "    MR0 = minRtarget(targetR0)\n",
    "    targetR21 = 0.7\n",
    "    targetR1 = sqrt(targetR21)\n",
    "    MR1 = minRtarget(targetR1)\n",
    "    targetR22 = 0.9\n",
    "    targetR2 = sqrt(targetR22)\n",
    "    MR2 = minRtarget(targetR2)\n",
    "\n",
    "\n",
    "    fig  = plot.figure(figsize=(13,13))\n",
    "    for model in readings[test_model]['devices'][name_combined_data]['model']:\n",
    "        metrics_model = dataframe[model]['metrics']\n",
    "    \n",
    "        if plot_train == True:\n",
    "            plot.scatter(metrics_model['train']['sign_sigma']*metrics_model['train']['RMSD_norm_unb'], metrics_model['train']['normalised_bias'], label = 'Train ' + model)\n",
    "        plot.scatter(metrics_model['test']['sign_sigma']*metrics_model['test']['RMSD_norm_unb'], metrics_model['test']['normalised_bias'], label = 'Test ' + model)\n",
    "    \n",
    "    ## Add circles\n",
    "    ax = plot.gca()\n",
    "    circle1 = plot.Circle((0, 0), 1, linewidth = 0.8, color='k', fill =False)\n",
    "    circleMR0 = plot.Circle((0, 0), MR0, linewidth = 0.8, color='r', fill=False)\n",
    "    circleMR1 = plot.Circle((0, 0), MR1, linewidth = 0.8, color='y', fill=False)\n",
    "    circleMR2 = plot.Circle((0, 0), MR2, linewidth = 0.8, color='g', fill=False)\n",
    "    \n",
    "    circle3 = plot.Circle((0, 0), 0.01, color='k', fill=True)\n",
    "    \n",
    "    ## Add annotations\n",
    "    ax.add_artist(circle1)\n",
    "    ax.annotate('R2 < 0',\n",
    "                xy=(1, 0), xycoords='data',\n",
    "                xytext=(-35, 10), textcoords='offset points')\n",
    "    \n",
    "    ax.add_artist(circleMR0)\n",
    "    ax.annotate('R2 < ' + str(targetR20),\n",
    "                xy=(MR0, 0), xycoords='data',\n",
    "                xytext=(-35, 10), textcoords='offset points', color = 'r')\n",
    "    \n",
    "    ax.add_artist(circleMR1)\n",
    "    ax.annotate('R2 < ' + str(targetR21),\n",
    "                xy=(MR1, 0), xycoords='data',\n",
    "                xytext=(-45, 10), textcoords='offset points', color = 'y')\n",
    "    \n",
    "    \n",
    "    ax.add_artist(circleMR2)\n",
    "    ax.annotate('R2 < ' + str(targetR22),\n",
    "                xy=(MR2, 0), xycoords='data',\n",
    "                xytext=(-45, 10), textcoords='offset points', color = 'g')\n",
    "    ax.add_artist(circle3)\n",
    "    \n",
    "    ## Display and others\n",
    "    plt.axhline(0, color='black', linewidth = 0.5)\n",
    "    plt.axvline(0, color='black', linewidth = 0.5)\n",
    "    plot.legend(loc='best')\n",
    "    plot.xlim([-1.1,1.1])\n",
    "    plot.ylim([-1.1,1.1])\n",
    "    plot.title('Target Diagram')\n",
    "    plot.ylabel('Normalised Bias (-)')\n",
    "    plot.xlabel(\"RMSD*'\")\n",
    "    plot.show()\n",
    "    \n",
    "\n",
    "targetDiagram(_dataframe, _plot_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Objectives\n",
    "Explained here http://dx.doi.org/10.1016/j.envint.2016.12.007\n",
    "\n",
    "Sensor values Y, reference values x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "\n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "            print '\\t' + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import linregress\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def fUEREL(ux, values_x, values_Y):\n",
    "    def RSS(values_x, values_Y, intercept, slope):\n",
    "        pre_sum_1 = np.power(values_Y - intercept - np.multiply(slope, values_x), 2)\n",
    "        # pre_sum_2 = np.power(values_Y / (intercept + np.multiply(slope, values_x)) - 1, 2)\n",
    "        \n",
    "        # fig, axes = plot.subplots(1, 2, figsize=(15,10))\n",
    "        # axes[0].plot(pre_sum_1)\n",
    "        # axes[1].plot(pre_sum_2)\n",
    "\n",
    "        RSS = np.sum(np.power(pre_sum_1,2))\n",
    "        \n",
    "        return RSS\n",
    "    \n",
    "    slope, intercept, _, _, _ = linregress(values_x, values_Y)\n",
    "    # fig = plot.figure(figsize=(15,10))\n",
    "    # plot.plot(slope*values_x + intercept, label='Sensor')\n",
    "    # plot.plot(values_Y)\n",
    "    \n",
    "    RSS = RSS(values_x, values_Y, intercept, slope)\n",
    "    n = len(values_x)\n",
    "    if len(values_Y) != n: return\n",
    "    A = RSS/(n-2)-np.power(ux,2)\n",
    "    B = np.power(intercept + (slope-1)*values_x, 2)\n",
    "    C = np.power(A + B, 0.5)\n",
    "    UEREL = np.divide(2*C, values_Y)\n",
    "    \n",
    "    return UEREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "\n",
    "dqo_table = (['PM', 50],\n",
    "            ['O3', 30],\n",
    "            ['CO',25],\n",
    "            ['NO',25],\n",
    "            ['NO2',25],\n",
    "            ['NOX',25],\n",
    "            ['SO2',25])\n",
    "\n",
    "ux = 0\n",
    "test_model = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "\n",
    "## ---\n",
    "\n",
    "number_of_subplots = 0\n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "            number_of_subplots += 1\n",
    "\n",
    "if number_of_subplots % 2 == 0: cols = 2\n",
    "else: cols = 3\n",
    "    \n",
    "rows = int(math.ceil(number_of_subplots / cols))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "n = 0\n",
    "\n",
    "for device in readings[test_model]['devices']:\n",
    "    if 'model' in readings[test_model]['devices'][device]:\n",
    "        for model_name in readings[test_model]['devices'][device]['model']:\n",
    "                        \n",
    "            data = readings[test_model]['devices'][model_name]['data']\n",
    "            dataVal = data.groupby(pd.Grouper(freq='1H')).aggregate(np.mean)\n",
    "            values_x = dataVal['reference'].values\n",
    "            values_Y = dataVal['prediction'].values\n",
    "            \n",
    "            total_len = len(data.index)\n",
    "            n_train_periods = int(round(total_len*ratio_train))\n",
    "\n",
    "            ax = fig.add_subplot(gs[n])\n",
    "            n += 1      \n",
    "\n",
    "            uerel = 100*fUEREL(ux, values_x, values_Y)\n",
    "            \n",
    "            plot.plot(values_x, uerel, 'ko')\n",
    "            plot.xlabel('Ref. conc [ppb]')\n",
    "            plot.ylabel('Rel. Exp. Unc (%)')\n",
    "            plot.ylim([0, 100])\n",
    "            plot.title(model_name)\n",
    "            plot.axhline(y=25, color='r', linestyle='-')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "modelDir = join(rootDirectory, 'smartcitizen-iscape-models/Models/')\n",
    "\n",
    "dict_models = dict()\n",
    "with open(join(modelDir, 'summary.json'), 'r') as summary_file:\n",
    "    dict_models = json.load(summary_file)\n",
    "    \n",
    "selectedModels = tuple()\n",
    "def selectModels(Source):\n",
    "    global selectedModels\n",
    "    selectedModels = list(Source)\n",
    "    \n",
    "def loadModel(b):\n",
    "    clear_output()\n",
    "    if len(selectedModels)>0:\n",
    "        filename = join(modelDir, target_drop.value, selectedModels[0])\n",
    "        \n",
    "        global loaded_model\n",
    "        global loaded_params\n",
    "        global loaded_metrics\n",
    "\n",
    "        if type_drop.value == \"LSTM\":\n",
    "            # ML Model\n",
    "            # Load Model and weights\n",
    "            json_file = open(filename + \"_model.json\", \"r\")\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "        \n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "            loaded_model.load_weights(filename + \"_model.h5\")\n",
    "        elif type_drop.value == \"OLS\":\n",
    "            # OLS Model\n",
    "            loaded_model = joblib.load(filename + '_model.sav')\n",
    "        elif type_drop.value == 'RandomForest':\n",
    "            loaded_model = joblib.load(filename + '_model.sav')\n",
    "            \n",
    "        # Load params and metrics\n",
    "        loaded_params = joblib.load(filename + '_parameters.sav')\n",
    "        loaded_metrics = joblib.load(filename + '_metrics.sav')\n",
    "        \n",
    "        display(Markdown('## Model Load'))\n",
    "        display(Markdown(\"Loaded \" + selectedModels[0] + \" from disk\"))\n",
    "        display(Markdown('**Model Type** (*loaded_model*):' ))\n",
    "        display(loaded_model)\n",
    "        display(Markdown('**Model Parameters** (*loaded_params*)'))\n",
    "        display(loaded_params)\n",
    "        display(Markdown('**Model Metrics** (*loaded_metrics*)'))\n",
    "        display(loaded_metrics)\n",
    "    else:\n",
    "        print 'Select one model to load'\n",
    "    \n",
    "def show_models(target, mtype):\n",
    "    list_models = list()\n",
    "    for item in dict_models[target]:\n",
    "        if dict_models[target][item] == mtype:\n",
    "            list_models.append(item)\n",
    "    models.options = list(list_models)\n",
    "\n",
    "display(widgets.HTML('<hr><h4>Import Local Models</h4>'))\n",
    "\n",
    "type_drop = widgets.Dropdown(options = ['LSTM', 'RandomForest', 'OLS'],\n",
    "                                  value = 'LSTM',\n",
    "                                  description = 'Model Type',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "target_drop = widgets.Dropdown(options = ['ALPHASENSE', 'MICS', 'PMS'],\n",
    "                                  value = 'MICS',\n",
    "                                  description = 'Model Target',\n",
    "                                  layout = widgets.Layout(width='300px'))\n",
    "\n",
    "model_type_drop = widgets.interactive(show_models, \n",
    "                                target=target_drop,\n",
    "                                mtype = type_drop, \n",
    "                                layout=widgets.Layout(width='700px'))\n",
    "\n",
    "models = widgets.SelectMultiple(selected_labels = selectedModels, \n",
    "                           layout=widgets.Layout(width='700px'))\n",
    "\n",
    "models_interact = widgets.interactive(selectModels,\n",
    "                                     Source = models,\n",
    "                                     layout = widgets.Layout(width='700px'))\n",
    "display(model_type_drop)\n",
    "display(models)\n",
    "\n",
    "loadB = widgets.Button(description='Load Model')\n",
    "loadB.on_click(loadModel)\n",
    "\n",
    "buttonBox = widgets.HBox([loadB])\n",
    "display(buttonBox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression_utils import predict_OLS, prep_data_OLS\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# ---\n",
    "\n",
    "# Input\n",
    "test_name = '2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS'\n",
    "device_name = 'STATION CHIMNEY'\n",
    "prediction_name = 'CO_MICS_ppm'\n",
    "min_date = '2018-08-01 00:00:00'\n",
    "max_date = '2018-09-20 00:00:00'\n",
    "\n",
    "# ---\n",
    "\n",
    "# Retrieve model\n",
    "model_predict = loaded_model\n",
    "tuple_feat_predict = loaded_params['features']['items']\n",
    "alpha_filter = loaded_params['alpha_filter']\n",
    "\n",
    "## Prep Dataframe\n",
    "\n",
    "list_features = list()\n",
    "for item in tuple_feat_predict:\n",
    "    list_features.append(item[1])\n",
    "    if item[1] not in readings[test_name]['devices'][device_name]['data'].columns:\n",
    "        print '{} not in {}'.format(item[1], test_name)\n",
    "        break\n",
    "\n",
    "dataframeModel = readings[test_name]['devices'][device_name]['data'].loc[:,list_features]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "dataframeModel.dropna()\n",
    "\n",
    "## Predict the model results\n",
    "datapredict = prep_data_OLS(dataframeModel, tuple_feat_predict, 1, alpha_filter, device_name)\n",
    "\n",
    "prediction = predict_OLS(model_predict, datapredict, False, False, 'test')\n",
    "\n",
    "dataframe = pd.DataFrame(prediction, columns = [prediction_name]).set_index(datapredict['index'])\n",
    "readings[test_name]['devices'][device_name]['data'][prediction_name] = dataframe.loc[:,prediction_name]\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "# Actual data\n",
    "try:\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['reference'],'r', label = 'Reference Train', alpha = 0.3)\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['reference'], 'b', label = 'Reference Test', alpha = 0.3)\n",
    "    # Fitted Values for Training\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['prediction'], 'r', label = 'Prediction Train')\n",
    "    # Fitted Values for Test\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['prediction'], 'b', label = 'Prediction Test')\n",
    "except:\n",
    "    print 'No data available from model training'\n",
    "    \n",
    "# Fitted values\n",
    "plot.plot(dataframe.index, dataframe.loc[:, prediction_name], 'g')\n",
    "# plot.ylim([0,1.5])\n",
    "plot.grid(True)\n",
    "plot.xlabel('Time')\n",
    "plot.ylabel('Model' + prediction_name)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "\n",
    "# Input\n",
    "test_name = '2018-11_EXT_SALEM_TEST'\n",
    "device_name = 'Miltown'\n",
    "prediction_name = 'NO2_MICS_ppm'\n",
    "min_date = '2018-01-01 00:00:00'\n",
    "max_date = '2018-12-20 00:00:00'\n",
    "\n",
    "# Temporary\n",
    "readings[test_name]['devices'][device_name]['data']['NO2_MICS_RAW'] = readings[test_name]['devices'][device_name]['data']['NO2_MICS_RAW']*readings['2018-08_INT_STATION_TEST_SUMMER_HOLIDAYS']['devices']['STATION_CASE']['data']['NO2_MICS_RAW'].mean()/readings[test_name]['devices'][device_name]['data']['NO2_MICS_RAW'].mean()\n",
    "\n",
    "# ---\n",
    "\n",
    "model_predict = loaded_model\n",
    "tuple_feat_predict = loaded_params['features'] \n",
    "print tuple_feat_predict\n",
    "\n",
    "if type_drop.value == 'LSTM':\n",
    "    scalerX_predict = loaded_params['scalerX']\n",
    "    scalery_predict = loaded_params['scalery']\n",
    "    n_lags = loaded_params['n_lags']\n",
    "    alpha_filter = loaded_params['alpha_filter']\n",
    "    \n",
    "n_train_periods = loaded_params['ratio_train']\n",
    "\n",
    "## Prep Dataframe\n",
    "dataframeModel = readings[test_name]['devices'][device_name]['data']\n",
    "dataframeModel = dataframeModel[dataframeModel.index > min_date]\n",
    "dataframeModel = dataframeModel[dataframeModel.index < max_date]\n",
    "\n",
    "list_features_predict = list()\n",
    "for item in tuple_feat_predict: \n",
    "    if item[0] != 'REF':\n",
    "        list_features_predict.append(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import prep_prediction_ML\n",
    "from ml_utils import predict_ML\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline\n",
    "\n",
    "test_X, index_pred, n_obs = prep_prediction_ML(dataframeModel, list_features_predict, n_lags, alpha_filter, scalerX_predict, verbose = True)\n",
    "prediction = predict_ML(model_predict, test_X, n_lags, scalery_predict)\n",
    "dataframe = pd.DataFrame(prediction, columns = [prediction_name]).set_index(index_pred)\n",
    "readings[test_name]['devices'][device_name]['data'][prediction_name] = dataframe.loc[:,prediction_name]\n",
    "\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "# Actual data\n",
    "try:\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['reference'],'r', label = 'Reference Train', alpha = 0.3)\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['reference'], 'b', label = 'Reference Test', alpha = 0.3)\n",
    "    # Fitted Values for Training\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['prediction'], 'r', label = 'Prediction Train')\n",
    "    # Fitted Values for Test\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['prediction'], 'b', label = 'Prediction Test')\n",
    "except:\n",
    "    print 'No data available from model training'\n",
    "    \n",
    "# Fitted values\n",
    "plot.plot(dataframe.index, dataframe.loc[:, prediction_name], 'g')\n",
    "plot.ylim([0,1.5])\n",
    "plot.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plot\n",
    "% matplotlib inline\n",
    "\n",
    "# dataframeModel = dataframeModel.groupby(pd.Grouper(freq='90S')).aggregate(np.mean)\n",
    "dataframeModel = dataframeModel.loc[:, list_features_predict]\n",
    "dataframeModel = dataframeModel.dropna()\n",
    "index_pred = dataframeModel.index\n",
    "\n",
    "features = np.array(dataframeModel)\n",
    "\n",
    "## Get model prediction\n",
    "prediction = model_predict.predict(features)\n",
    "\n",
    "dataframe = pd.DataFrame(prediction, columns = [prediction_name]).set_index(index_pred)\n",
    "# dataframe = dataframe.groupby(pd.Grouper(freq='10Min')).aggregate(np.mean)    \n",
    "\n",
    "readings[test_name]['devices'][device_name]['data'][prediction_name] = dataframe.loc[:,prediction_name]\n",
    "# print readings[test_name]['devices'][device_name]['data'][prediction_name]\n",
    "# Plot\n",
    "fig = plot.figure(figsize=(15,10))\n",
    "\n",
    "# Actual data\n",
    "try:\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['reference'],'r', label = 'Reference Train', alpha = 0.3)\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['reference'], 'b', label = 'Reference Test', alpha = 0.3)\n",
    "    # Fitted Values for Training\n",
    "    plot.plot(dataFrameTrain.index, dataFrameTrain['prediction'], 'r', label = 'Prediction Train')\n",
    "    # Fitted Values for Test\n",
    "    plot.plot(dataFrameTest.index, dataFrameTest['prediction'], 'b', label = 'Prediction Test')\n",
    "except:\n",
    "    print 'No data available from model training'\n",
    "    \n",
    "# Fitted values\n",
    "plot.plot(dataframe.index, dataframe.loc[:, prediction_name], 'g', label = 'Predicted value')\n",
    "# plot.xlim(['2018-11-01','2018-11-20'])\n",
    "plot.grid(True)\n",
    "plot.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {
    "height": "357px",
    "width": "307px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "48px",
    "left": "552px",
    "top": "705.497px",
    "width": "315px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
